Ti­tulo,Autores,Fecha,Tema,Abstract,URL
Text-Free Learning of a Natural Language Interface for Pretrained FaceGenerators,"XiaodanDu, Raymond A.Yeh, NicholasKolkin, EliShechtman, GregShakhnarovich",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We propose Fast text2StyleGAN, a natural language interface that adapts pre-trained GANs for text-guided human face synthesis. Leveraging the recent advances in Contrastive Language-Image Pre-training (CLIP), no text data is required during training. Fast text2StyleGAN is formulated as a conditional variational autoencoder (CVAE) that provides extra control and diversity to the generated images at test time. Our model does not require re-training or fine-tuning of the GANs or CLIP when encountering new text prompts. In contrast to prior work, we do not rely on optimization at test time, making our method orders of magnitude faster than prior work. Empirically, on FFHQ dataset, our method offers faster and more accurate generation of images from natural language descriptions with varying levels of detail compared to prior work.",https://arxiv.org/abs/2209.03954
TF-GridNet: Making Time-Frequency Domain Models Great Again forMonaural Speaker Separation,"Zhong-QiuWang, SamueleCornell, Shukjae Choi, Younglo Lee, Byeong-YeolKim, ShinjiWatanabe",08-sep-22,Sound (cs.SD)," We propose TF-GridNet, a novel multi-path deep neural network (DNN) operating in the time-frequency (T-F) domain, for monaural talker- independent speaker separation in anechoic conditions. The model stacks several multi-path blocks, each consisting of an intra-frame spectral module, a sub-band temporal module, and a full-band self-attention module, to leverage local and global spectro-temporal information for separation. The model is trained to perform complex spectral mapping, where the real and imaginary (RI) components of the input mixture are stacked as input features to predict the target RI components. Besides using the scale-invariant signal-to-distortion ratio (SI-SDR) loss for model training, we include a novel loss term to encourage the separated sources to add up to the input mixture. Without using dynamic mixing, we obtain 23.4 dB SI-SDR improvement (SI-SDRi) on the WSJ0-2mix dataset, outperforming the previous best by a large margin.",https://arxiv.org/abs/2209.03953
Opponent Indifference in Rating Systems: A Theoretical Case for Sonas,"GregBodwin, Forest Zhang",08-sep-22,Data Structures and Algorithms (cs.DS)," In competitive games, it is common to assign each player a real number rating signifying their skill level. A rating system is a procedure by which player ratings are adjusted upwards each time they win, or downwards each time they lose.   Many matchmaking systems give players some control over their opponent's rating; for example, a player might be able to selectively initiate matches against opponents whose ratings are publicly visible, or abort a match without penalty before it begins but after glimpsing their opponent's rating. It is natural to ask whether one can design a rating system that does not incentivize a rating-maximizing player to act strategically, seeking matches against opponents of one rating over another. We show the following:   \- The full version of this ""opponent indifference"" property is unfortunately too strong to be feasible. Although it is satisfied by some rating systems, these systems lack certain desirable expressiveness properties, suggesting that they are not suitable to capture most games of interest.   \- However, there is a natural relaxation, roughly requiring indifference between any two opponents who are ``reasonably evenly matched'' with the choosing player. We prove that this relaxed variant of opponent indifference, which we call $P$ opponent indifference, is viable. In fact, a certain strong version of $P$ opponent indifference precisely characterizes the rating system Sonas, which was originally proposed for its empirical predictive accuracy on the outcomes of high-level chess matches.",https://arxiv.org/abs/2209.03952
W-Transformers : A Wavelet-based Transformer Framework for UnivariateTime Series Forecasting,"LenaSasal, TanujitChakraborty, AbdenourHadid",08-sep-22,Machine Learning (cs.LG)," Deep learning utilizing transformers has recently achieved a lot of success in many vital areas such as natural language processing, computer vision, anomaly detection, and recommendation systems, among many others. Among several merits of transformers, the ability to capture long-range temporal dependencies and interactions is desirable for time series forecasting, leading to its progress in various time series applications. In this paper, we build a transformer model for non-stationary time series. The problem is challenging yet crucially important. We present a novel framework for univariate time series representation learning based on the wavelet- based transformer encoder architecture and call it W-Transformer. The proposed W-Transformers utilize a maximal overlap discrete wavelet transformation (MODWT) to the time series data and build local transformers on the decomposed datasets to vividly capture the nonstationarity and long- range nonlinear dependencies in the time series. Evaluating our framework on several publicly available benchmark time series datasets from various domains and with diverse characteristics, we demonstrate that it performs, on average, significantly better than the baseline forecasters for short- term and long-term forecasting, even for datasets that consist of only a few hundred training samples.",https://arxiv.org/abs/2209.03950
The Utility of Explainable AI in Ad Hoc Human-Machine Teaming,"RohanPaleja, Muyleng Ghuy, Nadun RanawakaArachchige, ReedJensen, MatthewGombolay",08-sep-22,Artificial Intelligence (cs.AI)," Recent advances in machine learning have led to growing interest in Explainable AI (xAI) to enable humans to gain insight into the decision- making of machine learning models. Despite this recent interest, the utility of xAI techniques has not yet been characterized in human-machine teaming. Importantly, xAI offers the promise of enhancing team situational awareness (SA) and shared mental model development, which are the key characteristics of effective human-machine teams. Rapidly developing such mental models is especially critical in ad hoc human-machine teaming, where agents do not have a priori knowledge of others' decision-making strategies. In this paper, we present two novel human-subject experiments quantifying the benefits of deploying xAI techniques within a human-machine teaming scenario. First, we show that xAI techniques can support SA ($p<0.05)$. Second, we examine how different SA levels induced via a collaborative AI policy abstraction affect ad hoc human-machine teaming performance. Importantly, we find that the benefits of xAI are not universal, as there is a strong dependence on the composition of the human-machine team. Novices benefit from xAI providing increased SA ($p<0.05$) but are susceptible to cognitive overhead ($p<0.05$). On the other hand, expert performance degrades with the addition of xAI-based support ($p<0.05$), indicating that the cost of paying attention to the xAI outweighs the benefits obtained from being provided additional information to enhance SA. Our results demonstrate that researchers must deliberately design and deploy the right xAI techniques in the right scenario by carefully considering human-machine team composition and how the xAI method augments SA.",https://arxiv.org/abs/2209.03945
Data Feedback Loops: Model-driven Amplification of Dataset Biases,"RohanTaori, Tatsunori B.Hashimoto",08-sep-22,Machine Learning (cs.LG)," Datasets scraped from the internet have been critical to the successes of large-scale machine learning. Yet, this very success puts the utility of future internet-derived datasets at potential risk, as model outputs begin to replace human annotations as a source of supervision.   In this work, we first formalize a system where interactions with one model are recorded as history and scraped as training data in the future. We then analyze its stability over time by tracking changes to a test-time bias statistic (e.g. gender bias of model predictions). We find that the degree of bias amplification is closely linked to whether the model's outputs behave like samples from the training distribution, a behavior which we characterize and define as consistent calibration. Experiments in three conditional prediction scenarios - image classification, visual role- labeling, and language generation \- demonstrate that models that exhibit a sampling-like behavior are more calibrated and thus more stable. Based on this insight, we propose an intervention to help calibrate and stabilize unstable feedback systems.   Code is available at [this https URL](https://github.com/rtaori/data_feedback).",https://arxiv.org/abs/2209.03943
The Users Aren't Alright: Dangerous Mental Illness Behaviors andRecommendations,"AshleeMilton, StevieChancellor",08-sep-22,Information Retrieval (cs.IR)," In this paper, we argue that recommendation systems are in a unique position to propagate dangerous and cruel behaviors to people with mental illnesses.",https://arxiv.org/abs/2209.03942
NeuralFMU: Presenting a workflow for integrating hybrid NeuralODEsinto real world applications,"TobiasThummerer, JohannesStoljar, LarsMikelsons",08-sep-22,Machine Learning (cs.LG)," The term NeuralODE describes the structural combination of an Artifical Neural Network (ANN) and a numerical solver for Ordinary Differential Equations (ODEs), the former acts as the right-hand side of the ODE to be solved. This concept was further extended by a black-box model in the form of a Functional Mock-up Unit (FMU) to obtain a subclass of NeuralODEs, named NeuralFMUs. The resulting structure features the advantages of first-principle and data-driven modeling approaches in one single simulation model: A higher prediction accuracy compared to conventional First Principle Models (FPMs), while also a lower training effort compared to purely data-driven models. We present an intuitive workflow to setup and use NeuralFMUs, enabling the encapsulation and reuse of existing conventional models exported from common modeling tools. Moreover, we exemplify this concept by deploying a NeuralFMU for a consumption simulation based on a Vehicle Longitudinal Dynamics Model (VLDM), which is a typical use case in automotive industry. Related challenges that are often neglected in scientific use cases, like real measurements (e.g. noise), an unknown system state or high-frequent discontinuities, are handled in this contribution. For the aim to build a hybrid model with a higher prediction quality than the original FPM, we briefly highlight two open-source libraries: FMI.jl for integrating FMUs into the Julia programming environment, as well as an extension to this library called FMIFlux.jl, that allows for the integration of FMUs into a neural network topology to finally obtain a NeuralFMU.",https://arxiv.org/abs/2209.03941
Lost in Translation: Reimagining the Machine Learning Life Cycle inEducation,"Lydia T.Liu, SerenaWang, TolaniBritton, Rediet Abebe",08-sep-22,Artificial Intelligence (cs.AI)," Machine learning (ML) techniques are increasingly prevalent in education, from their use in predicting student dropout, to assisting in university admissions, and facilitating the rise of MOOCs. Given the rapid growth of these novel uses, there is a pressing need to investigate how ML techniques support long-standing education principles and goals. In this work, we shed light on this complex landscape drawing on qualitative insights from interviews with education experts. These interviews comprise in-depth evaluations of ML for education (ML4Ed) papers published in preeminent applied ML conferences over the past decade. Our central research goal is to critically examine how the stated or implied education and societal objectives of these papers are aligned with the ML problems they tackle. That is, to what extent does the technical problem formulation, objectives, approach, and interpretation of results align with the education problem at hand. We find that a cross-disciplinary gap exists and is particularly salient in two parts of the ML life cycle: the formulation of an ML problem from education goals and the translation of predictions to interventions. We use these insights to propose an extended ML life cycle, which may also apply to the use of ML in other domains. Our work joins a growing number of meta-analytical studies across education and ML research, as well as critical analyses of the societal impact of ML. Specifically, it fills a gap between the prevailing technical understanding of machine learning and the perspective of education researchers working with students and in policy.",https://arxiv.org/abs/2209.03933
Sequential Information Design: Learning to Persuade in the Dark,"MartinoBernasconi, MatteoCastiglioni, AlbertoMarchesi, NicolaGatti, FrancescoTrovo",08-sep-22,Machine Learning (cs.LG)," We study a repeated information design problem faced by an informed sender who tries to influence the behavior of a self-interested receiver. We consider settings where the receiver faces a sequential decision making (SDM) problem. At each round, the sender observes the realizations of random events in the SDM problem. This begets the challenge of how to incrementally disclose such information to the receiver to persuade them to follow (desirable) action recommendations. We study the case in which the sender does not know random events probabilities, and, thus, they have to gradually learn them while persuading the receiver. We start by providing a non-trivial polytopal approximation of the set of sender's persuasive information structures. This is crucial to design efficient learning algorithms. Next, we prove a negative result: no learning algorithm can be persuasive. Thus, we relax persuasiveness requirements by focusing on algorithms that guarantee that the receiver's regret in following recommendations grows sub-linearly. In the full-feedback setting -- where the sender observes all random events realizations -- , we provide an algorithm with $\tilde{O}(\sqrt{T})$ regret for both the sender and the receiver. Instead, in the bandit-feedback setting -- where the sender only observes the realizations of random events actually occurring in the SDM problem -- , we design an algorithm that, given an $\alpha \in [1/2, 1]$ as input, ensures $\tilde{O}({T^\alpha})$ and $\tilde{O}( T^{\max \\{ \alpha, 1-\frac{\alpha}{2} \\} })$ regrets, for the sender and the receiver respectively. This result is complemented by a lower bound showing that such a regrets trade-off is essentially tight.",https://arxiv.org/abs/2209.03929
Exploring Target Representations for Masked Autoencoders,"XingbinLiu, JinghaoZhou, TaoKong, Xianming Lin, Rongrong Ji",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Masked autoencoders have become popular training paradigms for self-supervised visual representation learning. These models randomly mask a portion of the input and reconstruct the masked portion according to the target representations. In this paper, we first show that a careful choice of the target representation is unnecessary for learning good representations, since different targets tend to derive similarly behaved models. Driven by this observation, we propose a multi-stage masked distillation pipeline and use a randomly initialized model as the teacher, enabling us to effectively train high-capacity models without any efforts to carefully design target representations. Interestingly, we further explore using teachers of larger capacity, obtaining distilled students with remarkable transferring ability. On different tasks of classification, transfer learning, object detection, and semantic segmentation, the proposed method to perform masked knowledge distillation with bootstrapped teachers (dBOT) outperforms previous self-supervised methods by nontrivial margins. We hope our findings, as well as the proposed method, could motivate people to rethink the roles of target representations in pre-training masked autoencoders.",https://arxiv.org/abs/2209.03927
Data Management Challenges for Internet-scale 3D Search Engines,"JamesWilliams, Shane Scott, TimurHindanov, ChristophRoedig",08-sep-22,Information Retrieval (cs.IR)," This paper describes some of the major challenges encountered by Physna Inc. in developing an internet-scale search engine for three- dimensional (3D) models. The discussion focuses on the most significant data management issues in this domain, including model acquisition, diversity of file formats, shape retrieval, intellectual property, the legality of web crawling, and trustworthy computing. The paper gives an overview of these topics, as well as some of the general design choices that were made in the course of building the Thangs search engine. While numerous works have been published on the topic of algorithms for 3D similarity search, this paper is the first to discuss the real-world challenges that arise in building a practical, internet-scale, 3D search engine.",https://arxiv.org/abs/2209.03917
PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates andFeature-metric Alignment,"PrajwalChidananda, Saurabh Nair, Douglas Lee, AdrianKaehler",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We present PixTrack, a vision based object pose tracking framework using novel view synthesis and deep feature-metric alignment. Our evaluations demonstrate that our method produces highly accurate, robust, and jitter-free 6DoF pose estimates of objects in RGB images without the need of any data annotation or trajectory smoothing. Our method is also computationally efficient making it easy to have multi-object tracking with no alteration to our method and just using CPU multiprocessing.",https://arxiv.org/abs/2209.03913
Best of Both Worlds: Agents with Entitlements,"MartinHoefer, MarcoSchmalhofer, GiovannaVarricchio",08-sep-22,Computer Science and Game Theory (cs.GT)," Fair division of indivisible goods is a central challenge in artificial intelligence. For many prominent fairness criteria including envy-freeness (EF) or proportionality (PROP), no allocations satisfying these criteria might exist. Two popular remedies to this problem are randomization or relaxation of fairness concepts. A timely research direction is to combine the advantages of both, commonly referred to as Best of Both Worlds (BoBW). We consider fair division with entitlements, which allows to adjust notions of fairness to heterogeneous priorities among agents. This is an important generalization to standard fair division models and is not well-understood in terms of BoBW results. Our main result is a lottery for additive valuations and different entitlements that is ex-ante weighted envy-free (WEF), as well as ex-post weighted proportional up to one good (WPROP1) and weighted transfer envy-free up to one good (WEF(1,1)). It can be computed in strongly polynomial time. We show that this result is tight - ex-ante WEF is incompatible with any stronger ex-post WEF relaxation. In addition, we extend BoBW results on group fairness to entitlements and explore generalizations of our results to instances with more expressive valuation functions.",https://arxiv.org/abs/2209.03910
Trustless Cross-chain Communication for Zendoo Sidechains,"AlbertoGaroffolo, DmytroKaidalov, RomanOliynykov",08-sep-22,Cryptography and Security (cs.CR)," In the Zendoo white paper, we introduced a novel sidechain construction for Bitcoin-like blockchains, which allows a mainchain to create and communicate with sidechains of different types without knowing their internal structure. In this paper, we take a step further by introducing a comprehensive method for sidechains to communicate amongst each other. We will also discuss the details of a cross-chain token transfer protocol that extends the generic communication mechanism. With the cross- chain token transfer protocol, it can enable a broad range of new applications, such as an exchange platform, that allows the ability to trade tokens issued from different sidechains.",https://arxiv.org/abs/2209.03908
Reconstruction Attacks on Aggressive Relaxations of DifferentialPrivacy,"ProttayProtivash, JohnDurrell, Zeyu Ding, DanfengZhang, Daniel Kifer",08-sep-22,Cryptography and Security (cs.CR)," Differential privacy is a widely accepted formal privacy definition that allows aggregate information about a dataset to be released while controlling privacy leakage for individuals whose records appear in the data. Due to the unavoidable tension between privacy and utility, there have been many works trying to relax the requirements of differential privacy to achieve greater utility. One class of relaxation, which is starting to gain support outside the privacy community is embodied by the definitions of individual differential privacy (IDP) and bootstrap differential privacy (BDP). The original version of differential privacy defines a set of neighboring database pairs and achieves its privacy guarantees by requiring that each pair of neighbors should be nearly indistinguishable to an attacker. The privacy definitions we study, however, aggressively reduce the set of neighboring pairs that are protected. Both IDP and BDP define a measure of ""privacy loss"" that satisfies formal privacy properties such as postprocessing invariance and composition, and achieve dramatically better utility than the traditional variants of differential privacy. However, there is a significant downside - we show that they allow a significant portion of the dataset to be reconstructed using algorithms that have arbitrarily low privacy loss under their privacy accounting rules. We demonstrate these attacks using the preferred mechanisms of these privacy definitions. In particular, we design a set of queries that, when protected by these mechanisms with high noise settings (i.e., with claims of very low privacy loss), yield more precise information about the dataset than if they were not protected at all.",https://arxiv.org/abs/2209.03907
Analyzing the Effect of Sampling in GNNs on Individual Fairness,"RebeccaSalganik, FernandoDiaz, GolnooshFarnadi",08-sep-22,Machine Learning (cs.LG)," Graph neural network (GNN) based methods have saturated the field of recommender systems. The gains of these systems have been significant, showing the advantages of interpreting data through a network structure. However, despite the noticeable benefits of using graph structures in recommendation tasks, this representational form has also bred new challenges which exacerbate the complexity of mitigating algorithmic bias. When GNNs are integrated into downstream tasks, such as recommendation, bias mitigation can become even more difficult. Furthermore, the intractability of applying existing methods of fairness promotion to large, real world datasets places even more serious constraints on mitigation attempts. Our work sets out to fill in this gap by taking an existing method for promoting individual fairness on graphs and extending it to support mini-batch, or sub-sample based, training of a GNN, thus laying the groundwork for applying this method to a downstream recommendation task. We evaluate two popular GNN methods: Graph Convolutional Network (GCN), which trains on the entire graph, and GraphSAGE, which uses probabilistic random walks to create subgraphs for mini-batch training, and assess the effects of sub-sampling on individual fairness. We implement an individual fairness notion called \textit{REDRESS}, proposed by Dong et al., which uses rank optimization to learn individual fair node, or item, embeddings. We empirically show on two real world datasets that GraphSAGE is able to achieve, not just, comparable accuracy, but also, improved fairness as compared with the GCN model. These finding have consequential ramifications to individual fairness promotion, GNNs, and in downstream form, recommender systems, showing that mini-batch training facilitate individual fairness promotion by allowing for local nuance to guide the process of fairness promotion in representation learning.",https://arxiv.org/abs/2209.03905
Dyadic Interaction Assessment from Free-living Audio for DepressionSeverity Assessment,"BishalLamichhane, NidalMoukaddam, Ankit B.Patel, AshutoshSabharwal",08-sep-22,Sound (cs.SD)," Psychomotor retardation in depression has been associated with speech timing changes from dyadic clinical interviews. In this work, we investigate speech timing features from free-living dyadic interactions. Apart from the possibility of continuous monitoring to complement clinical visits, a study in free-living conditions would also allow inferring sociability features such as dyadic interaction frequency implicated in depression. We adapted a speaker count estimator as a dyadic interaction detector with a specificity of 89.5% and a sensitivity of 86.1% in the DIHARD dataset. Using the detector, we obtained speech timing features from the detected dyadic interactions in multi-day audio recordings of 32 participants comprised of 13 healthy individuals, 11 individuals with depression, and 8 individuals with psychotic disorders. The dyadic interaction frequency increased with depression severity in participants with no or mild depression, indicating a potential diagnostic marker of depression onset. However, the dyadic interaction frequency decreased with increasing depression severity for participants with moderate or severe depression. In terms of speech timing features, the response time had a significant positive correlation with depression severity. Our work shows the potential of dyadic interaction analysis from audio recordings of free- living to obtain markers of depression severity.",https://arxiv.org/abs/2209.03904
Interactive Imitation Learning in Robotics based on Simulations,XinjieLiu,26-jul-22,Robotics (cs.RO)," The transformation towards intelligence in various industries is creating more demand for intelligent and flexible products. In the field of robotics, learning-based methods are increasingly being applied, with the purpose of training robots to learn to deal with complex and changing external environments through data. In this context, reinforcement learning and imitation learning are becoming research hotspots with their respective characteristics. However, the two have their own limitations in some cases, such as the high cost of data acquisition for reinforcement learning. Moreover, it is difficult for imitation learning to provide perfect demonstrations. As a branch of imitation learning, interactive imitation learning aims at transferring human knowledge to the agent through interactions between the demonstrator and the robot, which alleviates the difficulty of teaching. This thesis implements IIL algorithms in four simulation scenarios and conducts extensive experiments, aiming at providing exhaustive information about IIL methods both in action space and state space as well as comparison with RL methods.",https://arxiv.org/abs/2209.03901
IDIAPers @ Causal News Corpus 2022: Efficient Causal RelationIdentification Through a Prompt-based Few-shot Approach,"SergioBurdisso, Juan Zuluaga-Gomez, Esau Villatoro-Tello, MartinFajcik, MuskaanSingh, PavelSmrz, PetrMotlicek",08-sep-22,Computation and Language (cs.CL)," In this paper, we describe our participation in the subtask 1 of CASE-2022, Event Causality Identification with Casual News Corpus. We address the Causal Relation Identification (CRI) task by exploiting a set of simple yet complementary techniques for fine-tuning language models (LMs) on a small number of annotated examples (i.e., a few-shot configuration). We follow a prompt-based prediction approach for fine-tuning LMs in which the CRI task is treated as a masked language modeling problem (MLM). This approach allows LMs natively pre-trained on MLM problems to directly generate textual responses to CRI-specific prompts. We compare the performance of this method against ensemble techniques trained on the entire dataset. Our best-performing submission was trained only with 256 instances per class, a small portion of the entire dataset, and yet was able to obtain the second-best precision (0.82), third-best accuracy (0.82), and an F1-score (0.85) very close to what was reported by the winner team (0.86).",https://arxiv.org/abs/2209.03900
IDIAPers @ Causal News Corpus 2022: Extracting Cause-Effect-SignalTriplets via Pre-trained Autoregressive Language Model,"MartinFajcik, MuskaanSingh, JuanZuluaga-Gomez, EsaÃº Villatoro-Tello, SergioBurdisso, PetrMotlicek, Pavel Smrz",08-sep-22,Computation and Language (cs.CL)," In this paper, we describe our shared task submissions for Subtask 2 in CASE-2022, Event Causality Identification with Casual News Corpus. The challenge focused on the automatic detection of all cause-effect-signal spans present in the sentence from news-media. We detect cause-effect-signal spans in a sentence using T5 -- a pre-trained autoregressive language model. We iteratively identify all cause-effect-signal span triplets, always conditioning the prediction of the next triplet on the previously predicted ones. To predict the triplet itself, we consider different causal relationships such as cause$\rightarrow$effect$\rightarrow$signal. Each triplet component is generated via a language model conditioned on the sentence, the previous parts of the current triplet, and previously predicted triplets. Despite training on an extremely small dataset of 160 samples, our approach achieved competitive performance, being placed second in the competition. Furthermore, we show that assuming either cause$\rightarrow$effect or effect$\rightarrow$cause order achieves similar results. Our code and model predictions will be released online.",https://arxiv.org/abs/2209.03895
Optimal Communication and Control Strategies for a Multi-Agent Systemin the Presence of an Adversary,"DhruvaKartik, SagarSudhakara, Rahul Jain, AshutoshNayyar",08-sep-22,Systems and Control (eess.SY)," We consider a multi-agent system in which a decentralized team of agents controls a stochastic system in the presence of an adversary. Instead of committing to a fixed information sharing protocol, the agents can strategically decide at each time whether to share their private information with each other or not. The agents incur a cost whenever they communicate with each other and the adversary may eavesdrop on their communication. Thus, the agents in the team must effectively coordinate with each other while being robust to the adversary's malicious actions. We model this interaction between the team and the adversary as a stochastic zero-sum game where the team aims to minimize a cost while the adversary aims to maximize it. Under some assumptions on the adversary's capabilities, we characterize a min-max control and communication strategy for the team. We supplement this characterization with several structural results that can make the computation of the min-max strategy more tractable.",https://arxiv.org/abs/2209.03891
Mean Field Games on Weighted and Directed Graphs via ColoredDigraphons,"ChristianFabian, KaiCui, HeinzKoeppl",08-sep-22,Multiagent Systems (cs.MA)," The field of multi-agent reinforcement learning (MARL) has made considerable progress towards controlling challenging multi-agent systems by employing various learning methods. Numerous of these approaches focus on empirical and algorithmic aspects of the MARL problems and lack a rigorous theoretical foundation. Graphon mean field games (GMFGs) on the other hand provide a scalable and mathematically well-founded approach to learning problems that involve a large number of connected agents. In standard GMFGs, the connections between agents are undirected, unweighted and invariant over time. Our paper introduces colored digraphon mean field games (CDMFGs) which allow for weighted and directed links between agents that are also adaptive over time. Thus, CDMFGs are able to model more complex connections than standard GMFGs. Besides a rigorous theoretical analysis including both existence and convergence guarantees, we provide a learning scheme and illustrate our findings with an epidemics model and a model of the systemic risk in financial markets.",https://arxiv.org/abs/2209.03888
A Framework for Evaluating Privacy-Utility Trade-off in VerticalFederated Learning,"YanKang, JiahuanLuo, YuanqinHe, XiaojinZhang, LixinFan, QiangYang",08-sep-22,Machine Learning (cs.LG)," Federated learning (FL) has emerged as a practical solution to tackle data silo issues without compromising user privacy. One of its variants, vertical federated learning (VFL), has recently gained increasing attention as the VFL matches the enterprises' demands of leveraging more valuable features to build better machine learning models while preserving user privacy. Current works in VFL concentrate on developing a specific protection or attack mechanism for a particular VFL algorithm. In this work, we propose an evaluation framework that formulates the privacy-utility evaluation problem. We then use this framework as a guide to comprehensively evaluate a broad range of protection mechanisms against most of the state- of-the-art privacy attacks for three widely-deployed VFL algorithms. These evaluations may help FL practitioners select appropriate protection mechanisms given specific requirements. Our evaluation results demonstrate that: the model inversion and most of the label inference attacks can be thwarted by existing protection mechanisms; the model completion (MC) attack is difficult to be prevented, which calls for more advanced MC-targeted protection mechanisms. Based on our evaluation results, we offer concrete advice on improving the privacy-preserving capability of VFL systems.",https://arxiv.org/abs/2209.03887
Valuing Players Over Time,"Tiago Mendes-Neves, LuÃ­sMeireles, JoÃ£o Mendes-Moreira",08-sep-22,Machine Learning (cs.LG)," In soccer (or association football), players quickly go from heroes to zeroes, or vice-versa. Performance is not a static measure but a somewhat volatile one. Analyzing performance as a time series rather than a stationary point in time is crucial to making better decisions. This paper introduces and explores I-VAEP and O-VAEP models to evaluate actions and rate players' intention and execution. Then, we analyze these ratings over time and propose use cases to fundament our option of treating player ratings as a continuous problem. As a result, we present who were the best players and how their performance evolved, define volatility metrics to measure a player's consistency, and build a player development curve to assist decision-making.",https://arxiv.org/abs/2209.03885
Learning Sparse Graphon Mean Field Games,"ChristianFabian, KaiCui, HeinzKoeppl",08-sep-22,Multiagent Systems (cs.MA)," Although the field of multi-agent reinforcement learning (MARL) has made considerable progress in the last years, solving systems with a large number of agents remains a hard challenge. Graphon mean field games (GMFGs) enable the scalable analysis of MARL problems that are otherwise intractable. By the mathematical structure of graphons, this approach is limited to dense graphs which are insufficient to describe many real-world networks such as power law graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems. This especially includes power law networks which are empirically observed in various application areas and cannot be captured by standard graphons. We derive theoretical existence and convergence guarantees and give empirical examples that demonstrate the accuracy of our learning approach for systems with many agents. Furthermore, we rigorously extend the Online Mirror Descent (OMD) learning algorithm to our setup to accelerate learning speed, allow for agent interaction through the mean field in the transition kernel, and empirically show its capabilities. In general, we provide a scalable, mathematically well-founded machine learning approach to a large class of otherwise intractable problems of great relevance in numerous research fields.",https://arxiv.org/abs/2209.03882
Histogram Layers for Synthetic Aperture Sonar Imagery,"JoshuaPeeples, Alina Zare, Jeffrey Dale, JamesKeller",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Synthetic aperture sonar (SAS) imagery is crucial for several applications, including target recognition and environmental segmentation. Deep learning models have led to much success in SAS analysis; however, the features extracted by these approaches may not be suitable for capturing certain textural information. To address this problem, we present a novel application of histogram layers on SAS imagery. The addition of histogram layer(s) within the deep learning models improved performance by incorporating statistical texture information on both synthetic and real- world datasets.",https://arxiv.org/abs/2209.03880
Challenges of Proof-of-Useful-Work (PoUW),FelixHoffmann,08-sep-22,Cryptography and Security (cs.CR)," Proof-of-Work (PoW) is a popular blockchain consensus algorithm that is used in cryptocurrencies like Bitcoin in which hashing operations are repeated until the resulting hash has certain properties. This approach uses lots of computational power and energy for the sole purpose of securing the blockchain. In order to not waste energy on hashing operations that do not have any other purpose than enabling consensus between nodes and therefore securing the blockchain, Proof-of-Useful-Work (PoUW) is an alternative approach which aims to replace excessive usage of hash functions with tasks that bring additional real-world benefit, e.g. supporting scientific experiments that rely on computationally heavy simulations. This publication consists of two parts: In the first part, important properties of conventional hash-based PoW are described. In the second part, theoretical PoUW concepts such as Coinami, CoinAI and the first successful PoUW cryptocurrency Primecoin are analyzed with respects to how PoW properties can be retained while doing useful work.",https://arxiv.org/abs/2209.03878
A Survey on Large-Population Systems and Scalable Multi-AgentReinforcement Learning,"KaiCui, AnamTahir, GizemEkinci, AhmedElshamanhory, Yannick Eich, Mengguang Li, HeinzKoeppl",08-sep-22,Multiagent Systems (cs.MA)," The analysis and control of large-population systems is of great interest to diverse areas of research and engineering, ranging from epidemiology over robotic swarms to economics and finance. An increasingly popular and effective approach to realizing sequential decision-making in multi-agent systems is through multi-agent reinforcement learning, as it allows for an automatic and model-free analysis of highly complex systems. However, the key issue of scalability complicates the design of control and reinforcement learning algorithms particularly in systems with large populations of agents. While reinforcement learning has found resounding empirical success in many scenarios with few agents, problems with many agents quickly become intractable and necessitate special consideration. In this survey, we will shed light on current approaches to tractably understanding and analyzing large-population systems, both through multi- agent reinforcement learning and through adjacent areas of research such as mean-field games, collective intelligence, or complex network theory. These classically independent subject areas offer a variety of approaches to understanding or modeling large-population systems, which may be of great use for the formulation of tractable MARL algorithms in the future. Finally, we survey potential areas of application for large-scale control and identify fruitful future applications of learning algorithms in practical systems. We hope that our survey could provide insight and future directions to junior and senior researchers in theoretical and applied sciences alike.",https://arxiv.org/abs/2209.03865
Simpler is better: Multilevel Abstraction with Graph ConvolutionalRecurrent Neural Network Cells for Traffic Prediction,"Naghmeh ShafieeRoudbari, ZacharyPatterson, UrsulaEicker, CharalambosPoullis",08-sep-22,Machine Learning (cs.LG)," In recent years, graph neural networks (GNNs) combined with variants of recurrent neural networks (RNNs) have reached state-of-the-art performance in spatiotemporal forecasting tasks. This is particularly the case for traffic forecasting, where GNN models use the graph structure of road networks to account for spatial correlation between links and nodes. Recent solutions are either based on complex graph operations or avoiding predefined graphs. This paper proposes a new sequence-to-sequence architecture to extract the spatiotemporal correlation at multiple levels of abstraction using GNN-RNN cells with sparse architecture to decrease training time compared to more complex designs. Encoding the same input sequence through multiple encoders, with an incremental increase in encoder layers, enables the network to learn general and detailed information through multilevel abstraction. We further present a new benchmark dataset of street-level segment traffic data from Montreal, Canada. Unlike highways, urban road segments are cyclic and characterized by complicated spatial dependencies. Experimental results on the METR-LA benchmark highway and our MSLTD street-level segment datasets demonstrate that our model improves performance by more than 7% for one-hour prediction compared to the baseline methods while reducing computing resource requirements by more than half compared to other competing methods.",https://arxiv.org/abs/2209.03859
SE(3)-DiffusionFields: Learning cost functions for joint grasp andmotion optimization through diffusion,"JulenUrain, Niklas Funk, GeorgiaChalvatzaki, Jan Peters",08-sep-22,Robotics (cs.RO)," Multi-objective high-dimensional motion optimization problems are ubiquitous in robotics and highly benefit from informative gradients. To this end, we require all cost functions to be differentiable. We propose learning task-space, data-driven cost functions as diffusion models. Diffusion models represent expressive multimodal distributions and exhibit proper gradients over the entire space. We exploit these properties for motion optimization by integrating the learned cost functions with other potentially learned or hand-tuned costs in a single objective function and optimize all of them jointly by gradient descent. We showcase the benefits of joint optimization in a set of complex grasp and motion planning problems and compare against hierarchical approaches that decouple grasp selection from motion optimization.",https://arxiv.org/abs/2209.03858
Optimal Offloading Strategies for Edge-Computing via Mean-Field Gamesand Control,"KaiCui, MustafaBurakYilmaz, Anam Tahir, Anja Klein, HeinzKoeppl",08-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," The optimal offloading of tasks in heterogeneous edge-computing scenarios is of great practical interest, both in the selfish and fully cooperative setting. In practice, such systems are typically very large, rendering exact solutions in terms of cooperative optima or Nash equilibria intractable. For this purpose, we adopt a general mean-field formulation in order to solve the competitive and cooperative offloading problems in the limit of infinitely large systems. We give theoretical guarantees for the approximation properties of the limiting solution and solve the resulting mean-field problems numerically. Furthermore, we verify our solutions numerically and find that our approximations are accurate for systems with dozens of edge devices. As a result, we obtain a tractable approach to the design of offloading strategies in large edge-computing scenarios with many users.",https://arxiv.org/abs/2209.03855
Transformer-based classification of premise in tweets related toCOVID-19,"VadimPorvatov, NataliaSemenova",08-sep-22,Computation and Language (cs.CL)," Automation of social network data assessment is one of the classic challenges of natural language processing. During the COVID-19 pandemic, mining people's stances from public messages have become crucial regarding understanding attitudes towards health orders. In this paper, the authors propose the predictive model based on transformer architecture to classify the presence of premise in Twitter texts. This work is completed as part of the Social Media Mining for Health (SMM4H) Workshop 2022. We explored modern transformer-based classifiers in order to construct the pipeline efficiently capturing tweets semantics. Our experiments on a Twitter dataset showed that RoBERTa is superior to the other transformer models in the case of the premise prediction task. The model achieved competitive performance with respect to ROC AUC value 0.807, and 0.7648 for the F1 score.",https://arxiv.org/abs/2209.03854
A structure-preserving variational discretization scheme for the Cahn-Hilliard Navier-Stokes system,"AaronBrunk, HerbertEgger, OliverHabrich, Maria Lukacova-Medvidova",08-sep-22,Numerical Analysis (math.NA), We propose and analyze a novel structure-preserving space-time variational discretization method for the Cahn-Hilliard-Navier-Stokes system with concentration dependent mobility and viscosity. Uniqueness and stability for the discrete problem is established in the presence of nonlinear model parameters by means of the relative energy estimates. Order optimal convergence rates with respect to space and time are proven for all variables using balanced approximation spaces and relaxed regularity conditions on the solution. Numerical tests are presented to demonstrate the reliability of the proposed scheme and to illustrate the theoretical findings.,https://arxiv.org/abs/2209.03851
Transformer based Fingerprint Feature Extraction,"SaraanshTandon, AnoopNamboodiri",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Fingerprint feature extraction is a task that is solved using either a global or a local representation. State-of-the-art global approaches use heavy deep learning models to process the full fingerprint image at once, which makes the corresponding approach memory intensive. On the other hand, local approaches involve minutiae based patch extraction, multiple feature extraction steps and an expensive matching stage, which make the corresponding approach time intensive. However, both these approaches provide useful and sometimes exclusive insights for solving the problem. Using both approaches together for extracting fingerprint representations is semantically useful but quite inefficient. Our convolutional transformer based approach with an in-built minutiae extractor provides a time and memory efficient solution to extract a global as well as a local representation of the fingerprint. The use of these representations along with a smart matching process gives us state-of-the-art performance across multiple databases. The project page can be found at [this https URL](https://saraansh1999.github.io/global-plus-local-fp-transformer).",https://arxiv.org/abs/2209.03849
Assessing Active Distribution Network Flexibility: On the Effects ofNonlinearities and Nonconvexities,"AndreyChurkin, PierluigiMancarella, Eduardo A. MartinezCesena",08-sep-22,Systems and Control (eess.SY)," A widespread approach to characterise the aggregated flexibility of active distribution networks (ADNs) is to estimate the boundary of the feasible network operating areas using convex polygons in the P-Q space. However, such approximations can be inaccurate under realistic conditions where, for example, the nonlinear nature of the network is captured, and the behaviour of flexible units is constrained. This letter demonstrates, using a small ADN example with four flexible units and considering only nonlinearities from the network, that reaching the full P-Q flexibility areas would require perfect coordination of units and high ramping rates. Without these requirements, the P-Q areas become nonconvex. Thus, if the effects of nonlinearities and nonconvexities are ignored, existing approaches in the literature can result in overestimation of ADN flexibility and give rise to impractical solutions, hampering coordination between transmission and distribution system operators.",https://arxiv.org/abs/2209.03846
High-order numerical evaluation of volume potentials via polynomialdensity interpolation,"Thomas G.Anderson, Luiz M.Faria, MarcBonnet, Carlos PÃ©rez-Arancibia",08-sep-22,Numerical Analysis (math.NA)," This short note outlines a simple numerical method for the high- order numerical evaluation of volume integral operators associated with the Poisson and Helmholtz equations in two spatial dimensions. Following the ideas of the density interpolation method for boundary integral operators, the proposed methodology leverages Green's third identity and a local polynomial interpolation of the source function to recast the volume potential as a sum of single- and double-layer potentials and a volume integral with a regularized (bounded or smoother) integrand. The layer potentials can be accurately and efficiently evaluated everywhere in the plane by means of existing methods (e.g. the density interpolation method), while the regularized volume integral can be accurately evaluated by applying elementary quadrature rules. Preliminary numerical examples based on non-overlapping quadrilateral patch representations of the domain in conjunction with Chebyshev-grid discretizations demonstrate the effectiveness of the proposed approach. Direct extensions of this method to other polynomial interpolation strategies in two and three dimensions and the efficient implementation via fast algorithms, will be presented in the complete version of this preliminary paper.",https://arxiv.org/abs/2209.03845
FADE: Enabling Large-Scale Federated Adversarial Training on Resource-Constrained Edge Devices,"MinxueTang, JianyiZhang, Mingyuan Ma, LouisDiValentin, Aolin Ding, AminHassanzadeh, Hai Li, YiranChen",08-sep-22,Machine Learning (cs.LG)," Adversarial Training (AT) has been proven to be an effective method of introducing strong adversarial robustness into deep neural networks. However, the high computational cost of AT prohibits the deployment of large-scale AT on resource-constrained edge devices, e.g., with limited computing power and small memory footprint, in Federated Learning (FL) applications. Very few previous studies have tried to tackle these constraints in FL at the same time. In this paper, we propose a new framework named Federated Adversarial Decoupled Learning (FADE) to enable AT on resource-constrained edge devices in FL. FADE reduces the computation and memory usage by applying Decoupled Greedy Learning (DGL) to federated adversarial training such that each client only needs to perform AT on a small module of the entire model in each communication round. In addition, we improve vanilla DGL by adding an auxiliary weight decay to alleviate objective inconsistency and achieve better performance. FADE offers a theoretical guarantee for the adversarial robustness and convergence. The experimental results also show that FADE can significantly reduce the computing resources consumed by AT while maintaining almost the same accuracy and robustness as fully joint training.",https://arxiv.org/abs/2209.03844
Pre-Training a Graph Recurrent Network for Language Representation,"YileWang, LinyiYang, ZhiyangTeng, MingZhou, YueZhang",08-sep-22,Computation and Language (cs.CL)," Transformer-based pre-trained models have gained much advance in recent years, becoming one of the most important backbones in natural language processing. Recent work shows that the attention mechanism inside Transformer may not be necessary, both convolutional neural networks and multi-layer perceptron based models have also been investigated as Transformer alternatives. In this paper, we consider a graph recurrent network for language model pre-training, which builds a graph structure for each sequence with local token-level communications, together with a sentence-level representation decoupled from other tokens. The original model performs well in domain-specific text classification under supervised training, however, its potential in learning transfer knowledge by self- supervised way has not been fully exploited. We fill this gap by optimizing the architecture and verifying its effectiveness in more general language understanding tasks, for both English and Chinese languages. As for model efficiency, instead of the quadratic complexity in Transformer-based models, our model has linear complexity and performs more efficiently during inference. Moreover, we find that our model can generate more diverse outputs with less contextualized feature redundancy than existing attention- based models.",https://arxiv.org/abs/2209.03839
emgr -- EMpirical GRamian Framework Version 5.99,ChristianHimpe,08-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," Version 5.99 of the empirical Gramian framework -- ""emgr"" -- completes a development cycle which focused on parametric model order reduction of gas network models while preserving compatibility to the previous development for the application of combined state and parameter reduction for neuroscience network models. Secondarily, new features concerning empirical Gramian types, perturbation design, and trajectory post-processing, as well as a Python version in addition to the default MATLAB / Octave implementation, have been added. This work summarizes these changes, particularly since ""emgr"" version 5.4, see Himpe, 2018 [Algorithms 11(7): 91], and gives recent as well as future applications, such as parameter identification in systems biology, based on the current feature set.",https://arxiv.org/abs/2209.03834
Optimised Design and Performance Comparison of Soft RoboticManipulators,"Arnau Garriga-Casanovas, ShenTreratanakulchai, EnricoFranco, Emilia Zari, VarellFerrandy, VaniVirdyawan, Ferdinando Rodriguez yBaena",08-sep-22,Robotics (cs.RO)," Soft robotic manipulators are attractive for a range of applications such as medical interventions or industrial inspections in confined environments. A myriad of soft robotic manipulators have been proposed in the literature, but their designs tend to be relatively similar, and generally offer a relatively low force. This limits the payload they can carry and therefore their usability. A comparison of force of the different designs is not available under a common framework, and designs present different diameters and features that make them hard to compare. In this paper, we present the design of a soft robotic manipulator that is optimised to maximise its force while respecting typical application constraints such as size, workspace, payload capability, and maximum pressure. The design presented here has the advantage that it morphs to an optimal design as it is pressurised to move in different directions, and this leads to higher lateral force. The robot is designed using a set of principles and thus can be adapted to other applications. We also present a non-dimensional analysis for soft robotic manipulators, and we apply it to compare the performance of the design proposed here with other designs in the literature. We show that our design has a higher force than other designs in the same category. Experimental results confirm the higher force of our proposed design.",https://arxiv.org/abs/2209.03833
Hardware-Software Co-Design of BIKE with HLS-Generated Accelerators,"GabrieleMontanaro, AndreaGalimberti, ErnestoColizzi, Davide Zoni",08-sep-22,Hardware Architecture (cs.AR)," In order to mitigate the security threat of quantum computers, NIST is undertaking a process to standardize post-quantum cryptosystems, aiming to assess their security and speed up their adoption in production scenarios. Several hardware and software implementations have been proposed for each candidate, while only a few target heterogeneous platforms featuring CPUs and FPGAs. This work presents a HW/SW co-design of BIKE for embedded platforms featuring both CPUs and small FPGAs and employs high- level synthesis (HLS) to timely deliver the hardware accelerators. In contrast to state-of-the-art solutions targeting performance-optimized HLS accelerators, the proposed solution targets the small FPGAs implemented in the heterogeneous platforms for embedded systems. Compared to the software- only execution of BIKE, the experimental results collected on the systems- on-chip of the entire Xilinx Zynq-7000 family highlight a performance speedup ranging from 1.37x, on Z-7010, to 2.78x, on Z-7020.",https://arxiv.org/abs/2209.03831
Participant Perceptions of a Robotic Coach Conducting PositivePsychology Exercises: A Systematic Analysis,"MinjaAxelsson, NikhilChuramani, AtahanCaldir, Hatice Gunes",08-sep-22,Human-Computer Interaction (cs.HC)," This paper provides a detailed overview of a case study of applying Continual Learning (CL) to a single-session Human-Robot Interaction (HRI) session (avg. 31 +- 10 minutes), where a robotic mental well-being coach conducted Positive Psychology (PP) exercises with (n = 20) participants. We present the results of a Thematic Analysis (TA) of data recorded from brief semi-structured interviews that were conducted with participants after the interaction sessions, as well as an analysis of statistical results demonstrating how participants' personalities may affect how they perceive the robot and its interactions.",https://arxiv.org/abs/2209.03830
Evaluating the Future Device Security Risk Indicator for Hundreds ofIoT Devices,"PascalOser, FelixEngelmann, StefanLÃ¼ders, Frank Kargl",08-sep-22,Cryptography and Security (cs.CR)," IoT devices are present in many, especially corporate and sensitive, networks and regularly introduce security risks due to slow vendor responses to vulnerabilities and high difficulty of patching. In this paper, we want to evaluate to what extent the development of future risk of IoT devices due to new and unpatched vulnerabilities can be predicted based on historic information. For this analysis, we build on existing prediction algorithms available in the SAFER framework (prophet and ARIMA) which we evaluate by means of a large data-set of vulnerabilities and patches from 793 IoT devices. Our analysis shows that the SAFER framework can predict a correct future risk for 91% of the devices, demonstrating its applicability. We conclude that this approach is a reliable means for network operators to efficiently detect and act on risks emanating from IoT devices in their networks.",https://arxiv.org/abs/2209.03827
Eigenvalue Mapping-based Semi-implicit Discretization of theGeneralized Super-Twisting Algorithm,NingningDing,08-sep-22,Systems and Control (eess.SY)," In this paper, an eigenvalue mapping-based semi-implicit discretization method is applied to discretize the generalized super- twisting algorithm. Compared to the commonly used explicit Euler method, the proposed discretization method totally suppresses discretization chattering in the unperturbed case, and the controller is insensitive to the gain overestimation of GSTA or large sampling time, which benefits the gain tuning and sampling time setting in practice. The global asymptotic stability of the unperturbed closed-loop system is proven by Lyapunov's direct method. Numerical simulations verify the effectiveness and superiority of the proposed discretization methodology.",https://arxiv.org/abs/2209.03826
Taking Advice from (Dis)Similar Machines: The Impact of Human-MachineSimilarity on Machine-Assisted Decision-Making,"Nina GrgiÄ‡-HlaÄa, ClaudeCastelluccia, Krishna P.Gummadi",08-sep-22,Computers and Society (cs.CY)," Machine learning algorithms are increasingly used to assist human decision-making. When the goal of machine assistance is to improve the accuracy of human decisions, it might seem appealing to design ML algorithms that complement human knowledge. While neither the algorithm nor the human are perfectly accurate, one could expect that their complementary expertise might lead to improved outcomes. In this study, we demonstrate that in practice decision aids that are not complementary, but make errors similar to human ones may have their own benefits.   In a series of human-subject experiments with a total of 901 participants, we study how the similarity of human and machine errors influences human perceptions of and interactions with algorithmic decision aids. We find that (i) people perceive more similar decision aids as more useful, accurate, and predictable, and that (ii) people are more likely to take opposing advice from more similar decision aids, while (iii) decision aids that are less similar to humans have more opportunities to provide opposing advice, resulting in a higher influence on people's decisions overall.",https://arxiv.org/abs/2209.03825
Ethical and Social Considerations in Automatic Expert Identificationand People Recommendation in Organizational Knowledge Management Systems,"Ida Larsen-Ledet, BhaskarMitra, SiÃ¢nLindley",08-sep-22,Human-Computer Interaction (cs.HC)," Organizational knowledge bases are moving from passive archives to active entities in the flow of people's work. We are seeing machine learning used to enable systems that both collect and surface information as people are working, making it possible to bring out connections between people and content that were previously much less visible in order to automatically identify and highlight experts on a given topic. When these knowledge bases begin to actively bring attention to people and the content they work on, especially as that work is still ongoing, we run into important challenges at the intersection of work and the social. While such systems have the potential to make certain parts of people's work more productive or enjoyable, they may also introduce new workloads, for instance by putting people in the role of experts for others to reach out to. And these knowledge bases can also have profound social consequences by changing what parts of work are visible and, therefore, acknowledged. We pose a number of open questions that warrant attention and engagement across industry and academia. Addressing these questions is an essential step in ensuring that the future of work becomes a good future for those doing the work. With this position paper, we wish to enter into the cross-disciplinary discussion we believe is required to tackle the challenge of developing recommender systems that respect social values.",https://arxiv.org/abs/2209.03821
Presentation: SymDefFix -- Sound Automatic Repair Using SymbolicExecution,"Tareq MohammedNazir, MartinPinzger",08-sep-22,Software Engineering (cs.SE)," In this presentation, we introduce our constraint-based repair approach, called SymDefFix. SymDefFix is based on ExtractFix [3] and replaces the dynamic analysis steps of ExtractFix to detect the error and find the potential fix locations in an input program with symbolic execution. We first briefly motivate and introduce our modifications of ExtractFix, and then demonstrate it with an example.",https://arxiv.org/abs/2209.03819
What and How of Machine Learning Transparency: Building BespokeExplainability Tools with Interoperable Algorithmic Components,"KacperSokol, AlexanderHepburn, Raul Santos-Rodriguez, PeterFlach",08-sep-22,Machine Learning (cs.LG)," Explainability techniques for data-driven predictive models based on artificial intelligence and machine learning algorithms allow us to better understand the operation of such systems and help to hold them accountable. New transparency approaches are developed at breakneck speed, enabling us to peek inside these black boxes and interpret their decisions. Many of these techniques are introduced as monolithic tools, giving the impression of one-size-fits-all and end-to-end algorithms with limited customisability. Nevertheless, such approaches are often composed of multiple interchangeable modules that need to be tuned to the problem at hand to produce meaningful explanations. This paper introduces a collection of hands-on training materials \-- slides, video recordings and Jupyter Notebooks -- that provide guidance through the process of building and evaluating bespoke modular surrogate explainers for tabular data. These resources cover the three core building blocks of this technique: interpretable representation composition, data sampling and explanation generation.",https://arxiv.org/abs/2209.03815
Hardware Accelerator and Neural Network Co-Optimization for Ultra-Low-Power Audio Processing Devices,"ChristophGerum, AdrianFrischknecht, Tobias Hald, Paul PalomeroBernardo, KonstantinLÃ¼beck, OlverBringmann",08-sep-22,Sound (cs.SD)," The increasing spread of artificial neural networks does not stop at ultralow-power edge devices. However, these very often have high computational demand and require specialized hardware accelerators to ensure the design meets power and performance constraints. The manual optimization of neural networks along with the corresponding hardware accelerators can be very challenging. This paper presents HANNAH (Hardware Accelerator and Neural Network seArcH), a framework for automated and combined hardware/software co-design of deep neural networks and hardware accelerators for resource and power-constrained edge devices. The optimization approach uses an evolution-based search algorithm, a neural network template technique, and analytical KPI models for the configurable UltraTrail hardware accelerator template to find an optimized neural network and accelerator configuration. We demonstrate that HANNAH can find suitable neural networks with minimized power consumption and high accuracy for different audio classification tasks such as single-class wake word detection, multi-class keyword detection, and voice activity detection, which are superior to the related work.",https://arxiv.org/abs/2209.03813
"FAT Forensics: A Python Toolbox for Implementing and DeployingFairness, Accountability and Transparency Algorithms in Predictive Systems","KacperSokol, AlexanderHepburn, RafaelPoyiadzi, MatthewClifford, Raul Santos-Rodriguez, PeterFlach",08-sep-22,Machine Learning (cs.LG)," Predictive systems, in particular machine learning algorithms, can take important, and sometimes legally binding, decisions about our everyday life. In most cases, however, these systems and decisions are neither regulated nor certified. Given the potential harm that these algorithms can cause, their qualities such as fairness, accountability and transparency (FAT) are of paramount importance. To ensure high-quality, fair, transparent and reliable predictive systems, we developed an open source Python package called FAT Forensics. It can inspect important fairness, accountability and transparency aspects of predictive algorithms to automatically and objectively report them back to engineers and users of such systems. Our toolbox can evaluate all elements of a predictive pipeline: data (and their features), models and predictions. Published under the BSD 3-Clause open source licence, FAT Forensics is opened up for personal and commercial usage.",https://arxiv.org/abs/2209.03807
Double Q-Learning for Citizen Relocation During Natural Hazards,Alysson Ribeiro daSilva,08-sep-22,Robotics (cs.RO)," Natural disasters can cause substantial negative socio-economic impacts around the world, due to mortality, relocation, rates, and reconstruction decisions. Robotics has been successfully applied to identify and rescue victims during the occurrence of a natural hazard. However, little effort has been taken to deploy solutions where an autonomous robot can save the life of a citizen by itself relocating it, without the need to wait for a rescue team composed of humans. Reinforcement learning approaches can be used to deploy such a solution, however, one of the most famous algorithms to deploy it, the Q-learning, suffers from biased results generated when performing its learning routines. In this research a solution for citizen relocation based on Partially Observable Markov Decision Processes is adopted, where the capability of the Double Q-learning in relocating citizens during a natural hazard is evaluated under a proposed hazard simulation engine based on a grid world. The performance of the solution was measured as a success rate of a citizen relocation procedure, where the results show that the technique portrays a performance above 100% for easy scenarios and near 50% for hard ones.",https://arxiv.org/abs/2209.03805
ReX: A Framework for Generating Local Explanations to Recurrent NeuralNetworks,"JunhaoLiu, XinZhang",08-sep-22,Machine Learning (cs.LG)," We propose a general framework to adapt various local explanation techniques to recurrent neural networks. In particular, our explanations add temporal information, which expand explanations generated from existing techniques to cover data points that have different lengths compared to the original input data point. Our approach is general as it only modifies the perturbation model and feature representation of existing techniques without touching their core algorithms. We have instantiated our approach on LIME and Anchors. Our empirical evaluation shows that it effectively improves the usefulness of explanations generated by these two techniques on a sentiment analysis network and an anomaly detection network.",https://arxiv.org/abs/2209.03800
A geometrically intrinsic Lagrangian-Eulerian scheme for 2D ShallowWater Equations with variable topography and discontinuous data,"EduardoAbreu, ElenaBachini, JohnPerez, MarioPutti",08-sep-22,Numerical Analysis (math.NA)," We present a Lagrangian-Eulerian scheme to solve the shallow water equations in the case of spatially variable bottom geometry. Using a local curvilinear reference system anchored on the bottom surface, we develop an effective first-order and high-resolution space-time discretization of the no-flow surfaces and solve a Lagrangian initial value problem that describes the evolution of the balance laws governing the geometrically intrinsic shallow water equations. The evolved solution set is then projected back to the original surface grid to complete the proposed Lagrangian-Eulerian formulation. The resulting scheme maintains monotonicity and captures shocks without providing excessive numerical dissipation also in the presence of non-autonomous fluxes such as those arising from the geometrically intrinsic shallow water equation on variable topographies. We provide a representative set of numerical examples to illustrate the accuracy and robustness of the proposed Lagrangian-Eulerian formulation for two-dimensional surfaces with general curvatures and discontinuous initial conditions.",https://arxiv.org/abs/2209.03798
Lightweight Long-Range Generative Adversarial Networks,"BowenLi, ThomasLukasiewicz",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In this paper, we introduce novel lightweight generative adversarial networks, which can effectively capture long-range dependencies in the image generation process, and produce high-quality results with a much simpler architecture. To achieve this, we first introduce a long-range module, allowing the network to dynamically adjust the number of focused sampling pixels and to also augment sampling locations. Thus, it can break the limitation of the fixed geometric structure of the convolution operator, and capture long-range dependencies in both spatial and channel-wise directions. Also, the proposed long-range module can highlight negative relations between pixels, working as a regularization to stabilize training. Furthermore, we propose a new generation strategy through which we introduce metadata into the image generation process to provide basic information about target images, which can stabilize and speed up the training process. Our novel long-range module only introduces few additional parameters and is easily inserted into existing models to capture long-range dependencies. Extensive experiments demonstrate the competitive performance of our method with a lightweight architecture.",https://arxiv.org/abs/2209.03795
Applying Transformer-based Text Summarization for Keyphrase Generation,"AnnaGlazkova, DmitryMorozov",08-sep-22,Computation and Language (cs.CL)," Keyphrases are crucial for searching and systematizing scholarly documents. Most current methods for keyphrase extraction are aimed at the extraction of the most significant words in the text. But in practice, the list of keyphrases often includes words that do not appear in the text explicitly. In this case, the list of keyphrases represents an abstractive summary of the source text. In this paper, we experiment with popular transformer-based models for abstractive text summarization using four benchmark datasets for keyphrase extraction. We compare the results obtained with the results of common unsupervised and supervised methods for keyphrase extraction. Our evaluation shows that summarization models are quite effective in generating keyphrases in the terms of the full-match F1-score and BERTScore. However, they produce a lot of words that are absent in the author's list of keyphrases, which makes summarization models ineffective in terms of ROUGE-1. We also investigate several ordering strategies to concatenate target keyphrases. The results showed that the choice of strategy affects the performance of keyphrase generation.",https://arxiv.org/abs/2209.03793
Accented Speech Recognition under the Indian context,AnkitGrover,08-sep-22,Computation and Language (cs.CL)," Accent forms an integral part of identifying cultures, emotions,behavior's, etc. People often perceive each other in a different manner due to their accent. The accent itself can be a conveyor of status, pride, and other emotional information which can be captured through Speech itself. Accent itself can be defined as: ""the way in which people in a particular area, country, or social group pronounce words"" or ""a special emphasis given to a syllable in a word, word in a sentence, or note in a set of musical notes"". Accented Speech Recognition is one the most important problems in the domain of Speech Recognition. Speech recognition is an interdisciplinary sub-field of Computer Science and Linguistics research where the main aim is to develop technologies which enable conversion of speech into text. The speech can be of any form such as read speech or spontaneous speech, conversational speech. Speech unlike text has lot of diversity. This diversity stems from the environmental conditions, variabilities from speaker to speaker, channel noise, differences in Speech production due to disabilities, presence of disfluencies. Speech therefore is indeed a rich source of information waiting to be exploited.",https://arxiv.org/abs/2209.03791
Joint Optimization of STAR-RIS Assisted UAV Communication Systems,"QinZhang, YangZhao, HaiLi, ShujuanHou, ZhengyuSong",08-sep-22,Information Theory (cs.IT)," In this letter, we study the simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted unmanned aerial vehicle (UAV) communications. Our goal is to maximize the sum rate of all users by jointly optimizing the STAR-RIS's beamforming vectors, the UAV's trajectory and power allocation. We decompose the formulated non- convex problem into three subproblems and solve them alternately to obtain the solution. Simulations show that: 1) the STAR-RIS achieves a higher sum rate than traditional RIS; 2) to exploit the benefits of STAR-RIS, the UAV's trajectory is closer to STAR-RIS than that of RIS; 3) the energy splitting for reflection and transmission highly depends on the real-time trajectory of UAV.",https://arxiv.org/abs/2209.03787
Stochastic gradient descent with gradient estimator for categoricalfeatures,"PaulPeseux, MaximeBerar, ThierryPaquet, VictorNicollet",08-sep-22,Machine Learning (cs.LG)," Categorical data are present in key areas such as health or supply chain, and this data require specific treatment. In order to apply recent machine learning models on such data, encoding is needed. In order to build interpretable models, one-hot encoding is still a very good solution, but such encoding creates sparse data. Gradient estimators are not suited for sparse data: the gradient is mainly considered as zero while it simply does not always exists, thus a novel gradient estimator is introduced. We show what this estimator minimizes in theory and show its efficiency on different datasets with multiple model architectures. This new estimator performs better than common estimators under similar settings. A real world retail dataset is also released after anonymization. Overall, the aim of this paper is to thoroughly consider categorical data and adapt models and optimizers to these key features.",https://arxiv.org/abs/2209.03777
Towards Responsible Medical Diagnostics Recommendation Systems,"DanielSchlÃ¶r, AndreasHotho",08-sep-22,Computers and Society (cs.CY)," The early development and deployment of hospital and healthcare information systems have encouraged the ongoing digitization of processes in hospitals. Many of these processes, which previously required paperwork and telephone arrangements, are now integrated into IT solutions and require physicians and medical staff to interact with appropriate interfaces and tools. Although this shift to digital data management and process support has benefited patient care in many ways, it requires physicians to accurately capture all relevant information digitally for billing and documentation purposes, which takes a lot of time away from actual patient care work. However, systematic collection of healthcare data over a long period of time offers opportunities to improve this process and support medical staff by introducing recommender systems. Based on a practical working example, in this position paper, we will outline the design of a responsible recommender system in the medical context from a technical, application driven perspective and discuss potential design choices and criteria with a specific focus on accountability, safety, and fairness.",https://arxiv.org/abs/2209.03771
On the Convergence of Randomized and Greedy Relaxation Schemes forSolving Nonsingular Linear Systems of Equations,"AndreasFrommer, Daniel B.Szyld",08-sep-22,Numerical Analysis (math.NA)," We extend results known for the randomized Gauss-Seidel and the Gauss-Southwell methods for the case of a Hermitian and positive definite matrix to certain classes of non-Hermitian matrices. We obtain convergence results for a whole range of parameters describing the probabilities in the randomized method or the greedy choice strategy in the Gauss-Southwell-type methods. We identify those choices which make our convergence bounds best possible. Our main tool is to use weighted l1-norms to measure the residuals. A major result is that the best convergence bounds that we obtain for the expected values in the randomized algorithm are as good as the best for the deterministic, but more costly algorithms of Gauss-Southwell type. Numerical experiments illustrate the convergence of the method and the bounds obtained. Comparisons with the randomized Kaczmarz method are also presented.",https://arxiv.org/abs/2209.03760
Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks againstFact-Verification Systems,"SaharAbdelnabi, Mario Fritz",07-sep-22,Cryptography and Security (cs.CR)," Mis- and disinformation are now a substantial global threat to our security and safety. To cope with the scale of online misinformation, one viable solution is to automate the fact-checking of claims by retrieving and verifying against relevant evidence. While major recent advances have been achieved in pushing forward the automatic fact-verification, a comprehensive evaluation of the possible attack vectors against such systems is still lacking. Particularly, the automated fact-verification process might be vulnerable to the exact disinformation campaigns it is trying to combat. In this work, we assume an adversary that automatically tampers with the online evidence in order to disrupt the fact-checking model via camouflaging the relevant evidence, or planting a misleading one. We first propose an exploratory taxonomy that spans these two targets and the different threat model dimensions. Guided by this, we design and propose several potential attack methods. We show that it is possible to subtly modify claim-salient snippets in the evidence, in addition to generating diverse and claim- aligned evidence. As a result, we highly degrade the fact-checking performance under many different permutations of the taxonomy's dimensions. The attacks are also robust against post-hoc modifications of the claim. Our analysis further hints at potential limitations in models' inference when faced with contradicting evidence. We emphasize that these attacks can have harmful implications on the inspectable and human-in-the-loop usage scenarios of such models, and we conclude by discussing challenges and directions for future defenses.",https://arxiv.org/abs/2209.03757
Improved Robust Algorithms for Learning with Discriminative FeatureFeedback,SivanSabato,08-sep-22,Machine Learning (cs.LG)," Discriminative Feature Feedback is a setting proposed by Dastupta et al. (2018), which provides a protocol for interactive learning based on feature explanations that are provided by a human teacher. The features distinguish between the labels of pairs of possibly similar instances. That work has shown that learning in this model can have considerable statistical and computational advantages over learning in standard label-based interactive learning models.   In this work, we provide new robust interactive learning algorithms for the Discriminative Feature Feedback model, with mistake bounds that are significantly lower than those of previous robust algorithms for this setting. In the adversarial setting, we reduce the dependence on the number of protocol exceptions from quadratic to linear. In addition, we provide an algorithm for a slightly more restricted model, which obtains an even smaller mistake bound for large models with many exceptions.   In the stochastic setting, we provide the first algorithm that converges to the exception rate with a polynomial sample complexity. Our algorithm and analysis for the stochastic setting involve a new construction that we call Feature Influence, which may be of wider applicability.",https://arxiv.org/abs/2209.03755
Automatic fetal fat quantification from MRI,"NetanellAvisdris, AviadRabinowich, DanielFridkin, AyalaZilberman, Sapir Lazar, JackyHerzlich, ZeevHananis, Daphna Link-Sourani, Liat Ben-Sira, LiranHiersch, Dafna BenBashat, LeoJoskowicz",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Normal fetal adipose tissue (AT) development is essential for perinatal well-being. AT, or simply fat, stores energy in the form of lipids. Malnourishment may result in excessive or depleted adiposity. Although previous studies showed a correlation between the amount of AT and perinatal outcome, prenatal assessment of AT is limited by lacking quantitative methods. Using magnetic resonance imaging (MRI), 3D fat- and water-only images of the entire fetus can be obtained from two point Dixon images to enable AT lipid quantification. This paper is the first to present a methodology for developing a deep learning based method for fetal fat segmentation based on Dixon MRI. It optimizes radiologists' manual fetal fat delineation time to produce annotated training dataset. It consists of two steps: 1) model-based semi-automatic fetal fat segmentations, reviewed and corrected by a radiologist; 2) automatic fetal fat segmentation using DL networks trained on the resulting annotated dataset. Three DL networks were trained. We show a significant improvement in segmentation times (3:38 hours to < 1 hour) and observer variability (Dice of 0.738 to 0.906) compared to manual segmentation. Automatic segmentation of 24 test cases with the 3D Residual U-Net, nn-UNet and SWIN-UNetR transformer networks yields a mean Dice score of 0.863, 0.787 and 0.856, respectively. These results are better than the manual observer variability, and comparable to automatic adult and pediatric fat segmentation. A radiologist reviewed and corrected six new independent cases segmented using the best performing network, resulting in a Dice score of 0.961 and a significantly reduced correction time of 15:20 minutes. Using these novel segmentation methods and short MRI acquisition time, whole body subcutaneous lipids can be quantified for individual fetuses in the clinic and large-cohort research.",https://arxiv.org/abs/2209.03753
Prior Knowledge-Guided Attention in Self-Supervised VisionTransformers,"KevinMiao, AkashGokul, RaghavSingh, SuzannePetryk, JosephGonzalez, KurtKeutzer, TrevorDarrell, Colorado Reed",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Recent trends in self-supervised representation learning have focused on removing inductive biases from training pipelines. However, inductive biases can be useful in settings when limited data are available or provide additional insight into the underlying data distribution. We present spatial prior attention (SPAN), a framework that takes advantage of consistent spatial and semantic structure in unlabeled image datasets to guide Vision Transformer attention. SPAN operates by regularizing attention masks from separate transformer heads to follow various priors over semantic regions. These priors can be derived from data statistics or a single labeled sample provided by a domain expert. We study SPAN through several detailed real-world scenarios, including medical image analysis and visual quality assurance. We find that the resulting attention masks are more interpretable than those derived from domain-agnostic pretraining. SPAN produces a 58.7 mAP improvement for lung and heart segmentation. We also find that our method yields a 2.2 mAUC improvement compared to domain- agnostic pretraining when transferring the pretrained model to a downstream chest disease classification task. Lastly, we show that SPAN pretraining leads to higher downstream classification performance in low-data regimes compared to domain-agnostic pretraining.",https://arxiv.org/abs/2209.03748
SynSciPass: detecting appropriate uses of scientific text generation,DomenicRosati,07-sep-22,Computation and Language (cs.CL)," Approaches to machine generated text detection tend to focus on binary classification of human versus machine written text. In the scientific domain where publishers might use these models to examine manuscripts under submission, misclassification has the potential to cause harm to authors. Additionally, authors may appropriately use text generation models such as with the use of assistive technologies like translation tools. In this setting, a binary classification scheme might be used to flag appropriate uses of assistive text generation technology as simply machine generated which is a cause of concern. In our work, we simulate this scenario by presenting a state-of-the-art detector trained on the DAGPap22 with machine translated passages from Scielo and find that the model performs at random. Given this finding, we develop a framework for dataset development that provides a nuanced approach to detecting machine generated text by having labels for the type of technology used such as for translation or paraphrase resulting in the construction of SynSciPass. By training the same model that performed well on DAGPap22 on SynSciPass, we show that not only is the model more robust to domain shifts but also is able to uncover the type of technology used for machine generated text. Despite this, we conclude that current datasets are neither comprehensive nor realistic enough to understand how these models would perform in the wild where manuscript submissions can come from many unknown or novel distributions, how they would perform on scientific full-texts rather than small passages, and what might happen when there is a mix of appropriate and inappropriate uses of natural language generation.",https://arxiv.org/abs/2209.03745
"Foundations of Wireless Information and Power Transfer: Theory,Prototypes, and Experiments","BrunoClerckx, Junghoon Kim, Kae WonChoi, DongIn Kim",08-sep-22,Information Theory (cs.IT)," As wireless has disrupted communications, wireless will also disrupt the delivery of energy. Future wireless networks will be equipped with (radiative) wireless power transfer (WPT) capability and exploit radio waves to carry both energy and information through a unified wireless information and power transfer (WIPT). Such networks will make the best use of the RF spectrum and radiation as well as the network infrastructure for the dual purpose of communicating and energizing. Consequently those networks will enable trillions of future low-power devices to sense, compute, connect, and energize anywhere, anytime, and on the move. In this paper, we review the foundations of such future system. We first give an overview of the fundamental theoretical building blocks of WPT and WIPT. Then we discuss some state-of-the-art experimental setups and prototypes of both WPT and WIPT and contrast theoretical and experimental results. We draw a special attention to how the integration of RF, signal and system designs in WPT and WIPT leads to new theoretical and experimental design challenges for both microwave and communication engineers and highlight some promising solutions. Topics and experimental testbeds discussed include closed-loop WPT and WIPT architectures with beamforming, waveform, channel acquisition, and single/multi-antenna energy harvester, centralized and distributed WPT, reconfigurable metasurfaces and intelligent surfaces for WPT, transmitter and receiver architecture for WIPT, modulation, rate-energy trade-off. Moreover, we highlight important theoretical and experimental research directions to be addressed for WPT and WIPT to become a foundational technology of future wireless networks.",https://arxiv.org/abs/2209.03742
Knowledge-Driven Program Synthesis via Adaptive Replacement Mutationand Auto-constructed Subprogram Archives,"YifanHe, ClausAranha, TetsuyaSakurai",08-sep-22,Neural and Evolutionary Computing (cs.NE)," We introduce Knowledge-Driven Program Synthesis (KDPS) as a variant of the program synthesis task that requires the agent to solve a sequence of program synthesis problems. In KDPS, the agent should use knowledge from the earlier problems to solve the later ones. We propose a novel method based on PushGP to solve the KDPS problem, which takes subprograms as knowledge. The proposed method extracts subprograms from the solution of previously solved problems by the Even Partitioning (EP) method and uses these subprograms to solve the upcoming programming task using Adaptive Replacement Mutation (ARM). We call this method PushGP+EP+ARM. With PushGP+EP+ARM, no human effort is required in the knowledge extraction and utilization processes. We compare the proposed method with PushGP, as well as a method using subprograms manually extracted by a human. Our PushGP+EP+ARM achieves better train error, success count, and faster convergence than PushGP. Additionally, we demonstrate the superiority of PushGP+EP+ARM when consecutively solving a sequence of six program synthesis problems.",https://arxiv.org/abs/2209.03739
Beyond Double Ascent via Recurrent Neural Tangent Kernel in SequentialRecommendation,"RuihongQiu, ZiHuang, Hongzhi Yin",08-sep-22,Information Retrieval (cs.IR)," Overfitting has long been considered a common issue to large neural network models in sequential recommendation. In our study, an interesting phenomenon is observed that overfitting is temporary. When the model scale is increased, the trend of the performance firstly ascends, then descends (i.e., overfitting) and finally ascends again, which is named as double ascent in this paper. We therefore raise an assumption that a considerably larger model will generalise better with a higher performance. In an extreme case to infinite-width, performance is expected to reach the limit of this specific structure. Unfortunately, it is impractical to directly build a huge model due to the limit of resources. In this paper, we propose the Overparameterised Recommender (OverRec), which utilises a recurrent neural tangent kernel (RNTK) as a similarity measurement for user sequences to successfully bypass the restriction of hardware for huge models. We further prove that the RNTK for the tied input-output embeddings in recommendation is the same as the RNTK for general untied input-output embeddings, which makes RNTK theoretically suitable for recommendation. Since the RNTK is analytically derived, OverRec does not require any training, avoiding physically building the huge model. Extensive experiments are conducted on four datasets, which verifies the state-of-the-art performance of OverRec.",https://arxiv.org/abs/2209.03736
Developing a multi-variate prediction model for the detection ofCOVID-19 from Crowd-sourced Respiratory Voice Data,"WafaaAljbawi, Sami O.Simmons, Visara Urovi",08-sep-22,Sound (cs.SD)," COVID-19 has affected more than 223 countries worldwide. There is a pressing need for non invasive, low costs and highly scalable solutions to detect COVID-19, especially in low-resource countries where PCR testing is not ubiquitously available. Our aim is to develop a deep learning model identifying COVID-19 using voice data recordings spontaneously provided by the general population (voice recordings and a short questionnaire) via their personal devices. The novelty of this work is in the development of a deep learning model for the identification of COVID-19 patients from voice recordings. Methods: We used the Cambridge University dataset consisting of 893 audio samples, crowd-sourced from 4352 participants that used a COVID-19 Sounds app. Voice features were extracted using a Mel-spectrogram analysis. Based on the voice data, we developed deep learning classification models to detect positive COVID-19 cases. These models included Long-Short Term Memory (LSTM) and Convolutional Neural Network (CNN). We compared their predictive power to baseline classification models, namely Logistic Regression and Support Vector Machine. Results: LSTM based on a Mel-frequency cepstral coefficients (MFCC) features achieved the highest accuracy (89%,) with a sensitivity and specificity of respectively 89% and 89%, The results achieved with the proposed model suggest a significant improvement in the prediction accuracy of COVID-19 diagnosis compared to the results obtained in the state of the art. Conclusion: Deep learning can detect subtle changes in the voice of COVID-19 patients with promising results. As an addition to the current testing techniques this model may aid health professionals in fast diagnosis and tracing of COVID-19 cases using simple voice analysis",https://arxiv.org/abs/2209.03735
A crowdsourced dataset of aerial images with annotated solarphotovoltaic arrays and installation metadata,"GabrielKasmi, Yves-Marie Saint-Drenan, DavidTrebosc, RaphaÃ«lJolivet, JonathanLeloux, Babacar Sarr, LaurentDubus",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Photovoltaic (PV) energy generation plays a crucial role in the energy transition. Small-scale PV installations are deployed at an unprecedented pace, and their integration into the grid can be challenging since public authorities often lack quality data about them. Overhead imagery is increasingly used to improve the knowledge of residential PV installations with machine learning models capable of automatically mapping these installations. However, these models cannot be easily transferred from one region or data source to another due to differences in image acquisition. To address this issue known as domain shift and foster the development of PV array mapping pipelines, we propose a dataset containing aerial images, annotations, and segmentation masks. We provide installation metadata for more than 28,000 installations. We provide ground truth segmentation masks for 13,000 installations, including 7,000 with annotations for two different image providers. Finally, we provide installation metadata that matches the annotation for more than 8,000 installations. Dataset applications include end-to-end PV registry construction, robust PV installations mapping, and analysis of crowdsourced datasets.",https://arxiv.org/abs/2209.03727
Towards explainable evaluation of language models on the semanticsimilarity of visual concepts,"MariaLymperaiou, GeorgeManoliadis, Orfeas MenisMastromichalakis, Edmund G.Dervakos, GiorgosStamou",08-sep-22,Computation and Language (cs.CL)," Recent breakthroughs in NLP research, such as the advent of Transformer models have indisputably contributed to major advancements in several tasks. However, few works research robustness and explainability issues of their evaluation strategies. In this work, we examine the behavior of high-performing pre-trained language models, focusing on the task of semantic similarity for visual vocabularies. First, we address the need for explainable evaluation metrics, necessary for understanding the conceptual quality of retrieved instances. Our proposed metrics provide valuable insights in local and global level, showcasing the inabilities of widely used approaches. Secondly, adversarial interventions on salient query semantics expose vulnerabilities of opaque metrics and highlight patterns in learned linguistic representations.",https://arxiv.org/abs/2209.03726
Incorporating Locality of Images to Generate Targeted TransferableAdversarial Examples,"ZhipengWei, JingjingChen, ZuxuanWu, Yu-GangJiang",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Despite that leveraging the transferability of adversarial examples can attain a fairly high attack success rate for non-targeted attacks, it does not work well in targeted attacks since the gradient directions from a source image to a targeted class are usually different in different DNNs. To increase the transferability of target attacks, recent studies make efforts in aligning the feature of the generated adversarial example with the feature distributions of the targeted class learned from an auxiliary network or a generative adversarial network. However, these works assume that the training dataset is available and require a lot of time to train networks, which makes it hard to apply to real-world scenarios. In this paper, we revisit adversarial examples with targeted transferability from the perspective of universality and find that highly universal adversarial perturbations tend to be more transferable. Based on this observation, we propose the Locality of Images (LI) attack to improve targeted transferability. Specifically, instead of using the classification loss only, LI introduces a feature similarity loss between intermediate features from adversarial perturbed original images and randomly cropped images, which makes the features from adversarial perturbations to be more dominant than that of benign images, hence improving targeted transferability. Through incorporating locality of images into optimizing perturbations, the LI attack emphasizes that targeted perturbations should be universal to diverse input patterns, even local image patches. Extensive experiments demonstrate that LI can achieve high success rates for transfer- based targeted attacks. On attacking the ImageNet-compatible dataset, LI yields an improvement of 12\% compared with existing state-of-the-art methods.",https://arxiv.org/abs/2209.03723
Optimization-based framework for low-voltage grid reinforcementassessment under various levels of flexibility and coordination,"SonerCandas, Beneharo ReveronBaecker, AnuragMohapatra, ThomasHamacher",08-sep-22,Systems and Control (eess.SY)," This paper presents an optimization framework for multi-modal energy systems at the low voltage (LV) distribution grid level, to assess the techno-economic potential of flexibility components and coordination between agents. The expected and rapid electrification of residential heating and mobility sectors will drive the existing distribution grid assets beyond their planned operating conditions. This change will also reveal new flexibility possibilities through sector coupling and end-user prosumer coordination. These options can be collectively harnessed to alleviate the wide-scale LV grid reinforcement needs. However, the size of the planning and optimization problem can now grow to intractable levels even for commercial solvers. Additionally, the presumption of complete coordination within the distribution system is far from the current reality. In this work, we apply a multi-level approach, separating the planning and operation stages of the systems with different temporal scales. This reduces the computational burden and also simulates the realistic level of system coordination for an LV distribution grid. Case studies conducted for a representative rural grid in Germany clearly show a high economic potential in the flexible operation of buildings with increased integration of photovoltaics and reduced capacity sizing of heat pumps. The scenario with an idealized level of system coordination shows a peak reduction to the effect of completely avoiding any grid reinforcement. The framework can be applied to any combination of grid and component types, to outline the broad landscape of future LV distribution grids.",https://arxiv.org/abs/2209.03716
Visual Grounding of Inter-lingual Word-Embeddings,"WafaaMohammed, HassanShahmohammadi, Hendrik P. A.Lensch, R. HaraldBaayen",08-sep-22,Computation and Language (cs.CL)," Visual grounding of Language aims at enriching textual representations of language with multiple sources of visual knowledge such as images and videos. Although visual grounding is an area of intense research, inter-lingual aspects of visual grounding have not received much attention. The present study investigates the inter-lingual visual grounding of word embeddings. We propose an implicit alignment technique between the two spaces of vision and language in which inter-lingual textual information interacts in order to enrich pre-trained textual word embeddings. We focus on three languages in our experiments, namely, English, Arabic, and German. We obtained visually grounded vector representations for these languages and studied whether visual grounding on one or multiple languages improved the performance of embeddings on word similarity and categorization benchmarks. Our experiments suggest that inter-lingual knowledge improves the performance of grounded embeddings in similar languages such as German and English. However, inter-lingual grounding of German or English with Arabic led to a slight degradation in performance on word similarity benchmarks. On the other hand, we observed an opposite trend on categorization benchmarks where Arabic had the most improvement on English. In the discussion section, several reasons for those findings are laid out. We hope that our experiments provide a baseline for further research on inter-lingual visual grounding.",https://arxiv.org/abs/2209.03715
Unsupervised Video Object Segmentation via Prototype Memory Network,"MinhyeokLee, SuhwanCho, SeunghoonLee, ChaewonPark, Sangyoun Lee",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Unsupervised video object segmentation aims to segment a target object in the video without a ground truth mask in the initial frame. This challenging task requires extracting features for the most salient common objects within a video sequence. This difficulty can be solved by using motion information such as optical flow, but using only the information between adjacent frames results in poor connectivity between distant frames and poor performance. To solve this problem, we propose a novel prototype memory network architecture. The proposed model effectively extracts the RGB and motion information by extracting superpixel-based component prototypes from the input RGB images and optical flow maps. In addition, the model scores the usefulness of the component prototypes in each frame based on a self-learning algorithm and adaptively stores the most useful prototypes in memory and discards obsolete prototypes. We use the prototypes in the memory bank to predict the next query frames mask, which enhances the association between distant frames to help with accurate mask prediction. Our method is evaluated on three datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed model with various ablation studies.",https://arxiv.org/abs/2209.03714
What Did I Just Hear? Detecting Pornographic Sounds in Adult VideosUsing Neural Networks,"HolyLovenia, Dessi PujiLestari, RitaFrieske",08-sep-22,Sound (cs.SD)," Audio-based pornographic detection enables efficient adult content filtering without sacrificing performance by exploiting distinct spectral characteristics. To improve it, we explore pornographic sound modeling based on different neural architectures and acoustic features. We find that CNN trained on log mel spectrogram achieves the best performance on Pornography-800 dataset. Our experiment results also show that log mel spectrogram allows better representations for the models to recognize pornographic sounds. Finally, to classify whole audio waveforms rather than segments, we employ voting segment-to-audio technique that yields the best audio-level detection results.",https://arxiv.org/abs/2209.03712
Strong Optimistic Solving for Dynamic Symbolic Execution,"DaryaParygina, AlexeyVishnyakov, AndreyFedotov",08-sep-22,Cryptography and Security (cs.CR)," Dynamic symbolic execution (DSE) is an effective method for automated program testing and bug detection. It is increasing the code coverage by the complex branches exploration during hybrid fuzzing. DSE tools invert the branches along some execution path and help fuzzer examine previously unavailable program parts. DSE often faces over- and underconstraint problems. The first one leads to significant analysis complication while the second one causes inaccurate symbolic execution.   We propose strong optimistic solving method that eliminates irrelevant path predicate constraints for target branch inversion. We eliminate such symbolic constraints that the target branch is not control dependent on. Moreover, we separately handle symbolic branches that have nested control transfer instructions that pass control beyond the parent branch scope, e.g. return, goto, break, etc. We implement the proposed method in our dynamic symbolic execution tool Sydr.   We evaluate the strong optimistic strategy, the optimistic strategy that contains only the last constraint negation, and their combination. The results show that the strategies combination helps increase either the code coverage or the average number of correctly inverted branches per one minute. It is optimal to apply both strategies together in contrast with other configurations.",https://arxiv.org/abs/2209.03711
Kernel-Segregated Transpose Convolution Operation,"Vijay SrinivasTida, SaiVenkateshChilukoti, Xiali Hei, Sonya Hsu",08-sep-22,Machine Learning (cs.LG)," Transpose convolution has shown prominence in many deep learning applications. However, transpose convolution layers are computationally intensive due to the increased feature map size due to adding zeros after each element in each row and column. Thus, convolution operation on the expanded input feature map leads to poor utilization of hardware resources. The main reason for unnecessary multiplication operations is zeros at predefined positions in the input feature map. We propose an algorithmic- level optimization technique for the effective transpose convolution implementation to solve these problems. Based on kernel activations, we segregated the original kernel into four sub-kernels. This scheme could reduce memory requirements and unnecessary multiplications. Our proposed method was $3.09 (3.02) \times$ faster computation using the Titan X GPU (Intel Dual Core CPU) with a flower dataset from the Kaggle website. Furthermore, the proposed optimization method can be generalized to existing devices without additional hardware requirements. A simple deep learning model containing one transpose convolution layer was used to evaluate the optimization method. It showed $2.2 \times$ faster training using the MNIST dataset with an Intel Dual-core CPU than the conventional implementation.",https://arxiv.org/abs/2209.03710
Visual Firewall Log Analysis -- At the Border Between Analytical andAppealing,"MarijaSchufrin, Hendrik LÃ¼cke-Tieke, JÃ¶rnKohlhammer",08-sep-22,Human-Computer Interaction (cs.HC)," In this paper, we present our design study on developing an interactive visual firewall log analysis system in collaboration with an IT service provider. We describe the human-centered design process, in which we additionally considered hedonic qualities by including the usage of personas, psychological need cards and interaction vocabulary. For the problem characterization we especially focus on the demands of the two main clusters of requirements: high-level overview and low-level analysis, represented by the two defined personas, namely information security officer and network analyst. This resulted in the prototype of a visual analysis system consisting of two interlinked parts. One part addresses the needs for rather strategical tasks while also fulfilling the need for an appealing appearance and interaction. The other part rather addresses the requirements for operational tasks and aims to provide a high level of flexibility. We describe our design journey, the derived domain tasks and task abstractions as well as our visual design decisions, and present our final prototypes based on a usage scenario. We also report on our capstone event, where we conducted an observed experiment and collected feedback from the information security officer. Finally, as a reflection, we propose the extension of a widely used design study process with a track for an additional focus on hedonic qualities.",https://arxiv.org/abs/2209.03704
Training Scale-Invariant Neural Networks on the Sphere Can Happen inThree Regimes,"MaximKodryan, EkaterinaLobacheva, MaksimNakhodnov, DmitryVetrov",08-sep-22,Machine Learning (cs.LG)," A fundamental property of deep learning normalization techniques, such as batch normalization, is making the pre-normalization parameters scale invariant. The intrinsic domain of such parameters is the unit sphere, and therefore their gradient optimization dynamics can be represented via spherical optimization with varying effective learning rate (ELR), which was studied previously. In this work, we investigate the properties of training scale-invariant neural networks directly on the sphere using a fixed ELR. We discover three regimes of such training depending on the ELR value: convergence, chaotic equilibrium, and divergence. We study these regimes in detail both on a theoretical examination of a toy example and on a thorough empirical analysis of real scale-invariant deep learning models. Each regime has unique features and reflects specific properties of the intrinsic loss landscape, some of which have strong parallels with previous research on both regular and scale-invariant neural networks training. Finally, we demonstrate how the discovered regimes are reflected in conventional training of normalized networks and how they can be leveraged to achieve better optima.",https://arxiv.org/abs/2209.03702
Aerial View Goal Localization with Reinforcement Learning,"AleksisPirinen, AntonSamuelsson, JohnBacksund, KalleÃ…strÃ¶m",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," With an increased amount and availability of unmanned aerial vehicles (UAVs) and other remote sensing devices (e.g. satellites), we have recently seen a vast increase in computer vision methods for aerial view data. One application of such technologies is within search-and-rescue (SAR), where the task is to localize and assist one or several people who are missing, for example after a natural disaster. In many cases the rough location may be known and a UAV can be deployed to explore a given, confined area to precisely localize the missing people. Due to time and battery constraints it is often critical that localization is performed as efficiently as possible. In this work, we approach this type of problem by abstracting it as an aerial view goal localization task in a framework that emulates a SAR-like setup without requiring access to actual UAVs. In this framework, an agent operates on top of an aerial image (proxy for a search area) and is tasked with localizing a goal that is described in terms of visual cues. To further mimic the situation on an actual UAV, the agent is not able to observe the search area in its entirety, not even at low resolution, and thus it has to operate solely based on partial glimpses when navigating towards the goal. To tackle this task, we propose AiRLoc, a reinforcement learning (RL)-based model that decouples exploration (searching for distant goals) and exploitation (localizing nearby goals). Extensive evaluations show that AiRLoc outperforms heuristic search methods as well as alternative learnable approaches. We also conduct a proof-of- concept study which indicates that the learnable methods outperform humans on average. Code has been made publicly available: [this https URL](https://github.com/aleksispi/airloc).",https://arxiv.org/abs/2209.03695
ExplORB-SLAM: Active Visual SLAM Exploiting the Pose-graph Topology,"Julio A.Placed, Juan J. GÃ³mezRodrÃ­guez, Juan D.TardÃ³s, JosÃ© A.Castellanos",08-sep-22,Robotics (cs.RO)," Deploying autonomous robots capable of exploring unknown environments has long been a topic of great relevance to the robotics community. In this work, we take a further step in that direction by presenting an open-source active visual SLAM framework that leverages the accuracy of a state-of-the-art graph-SLAM system and takes advantage of the fast utility computation that exploiting the structure of the underlying pose-graph offers. Through careful estimation of a posteriori weighted pose- graphs, D-optimal decision-making is achieved online with the objective of improving localization and mapping uncertainties as exploration occurs.",https://arxiv.org/abs/2209.03694
Exploring the Distribution Regularities of User Attention andSentiment toward Product Aspects in Online Reviews,"ChengleiQin, ChengzhiZhang, YiBu",08-sep-22,Computation and Language (cs.CL)," [Purpose] To better understand the online reviews and help potential consumers, businessmen, and product manufacturers effectively obtain users' evaluation on product aspects, this paper explores the distribution regularities of user attention and sentiment toward product aspects from the temporal perspective of online reviews. [Design/methodology/approach] Temporal characteristics of online reviews (purchase time, review time, and time intervals between purchase time and review time), similar attributes clustering, and attribute-level sentiment computing technologies are employed based on more than 340k smartphone reviews of three products from [this http URL](http://JD.COM) (a famous online shopping platform in China) to explore the distribution regularities of user attention and sentiment toward product aspects in this article. [Findings] The empirical results show that a power-law distribution can fit user attention to product aspects, and the reviews posted in short time intervals contain more product aspects. Besides, the results show that the values of user sentiment of product aspects are significantly higher/lower in short time intervals which contribute to judging the advantages and weaknesses of a product. [Research limitations] The paper can't acquire online reviews for more products with temporal characteristics to verify the findings because of the restriction on reviews crawling by the shopping platforms. [Originality/value] This work reveals the distribution regularities of user attention and sentiment toward product aspects, which is of great significance in assisting decision-making, optimizing review presentation, and improving the shopping experience.",https://arxiv.org/abs/2209.03693
"A Review on Method Entities in the Academic Literature: Extraction,Evaluation, and Application","YuzhuoWang, ChengzhiZhang, KaiLi",08-sep-22,Digital Libraries (cs.DL)," In scientific research, the method is an indispensable means to solve scientific problems and a critical research object. With the advancement of sciences, many scientific methods are being proposed, modified, and used in academic literature. The authors describe details of the method in the abstract and body text, and key entities in academic literature reflecting names of the method are called method entities. Exploring diverse method entities in a tremendous amount of academic literature helps scholars understand existing methods, select the appropriate method for research tasks, and propose new methods. Furthermore, the evolution of method entities can reveal the development of a discipline and facilitate knowledge discovery. Therefore, this article offers a systematic review of methodological and empirical works focusing on extracting method entities from full-text academic literature and efforts to build knowledge services using these extracted method entities. Definitions of key concepts involved in this review were first proposed. Based on these definitions, we systematically reviewed the approaches and indicators to extract and evaluate method entities, with a strong focus on the pros and cons of each approach. We also surveyed how extracted method entities are used to build new applications. Finally, limitations in existing works as well as potential next steps were discussed.",https://arxiv.org/abs/2209.03690
Packing $K_r$s in bounded degree graphs,"MichaelMcKay, DavidManlove",08-sep-22,Data Structures and Algorithms (cs.DS)," We study the problem of finding a maximum-cardinality set of $r$-cliques in an undirected graph of fixed maximum degree $\Delta$, subject to the cliques in that set being either vertex-disjoint or edge-disjoint. It is known for $r=3$ that the vertex-disjoint (edge-disjoint) problem is solvable in linear time if $\Delta=3$ ($\Delta=4$) but APX-hard if $\Delta \geq 4$ ($\Delta \geq 5$).   We generalise these results to an arbitrary but fixed $r \geq 3$, and provide a complete complexity classification for both the vertex- and edge- disjoint variants in graphs of maximum degree $\Delta$.   Specifically, we show that the vertex-disjoint problem is solvable in linear time if $\Delta < 3r/2 - 1$, solvable in polynomial time if $\Delta < 5r/3 - 1$, and APX-hard if $\Delta \geq \lceil 5r/3 \rceil - 1$. We also show that if $r\geq 6$ then the above implications also hold for the edge-disjoint problem. If $r \leq 5$, then the edge-disjoint problem is solvable in linear time if $\Delta < 3r/2 - 1$, solvable in polynomial time if $\Delta \leq 2r - 2$, and APX-hard if $\Delta  2r - 2$.",https://arxiv.org/abs/2209.03687
Known by the company we keep: `Triadic influence' as a proxy forcompatibility in social relationships,"Miguel RuÃ­z-GarcÃ­a, JuanOzaita, MarÃ­aPereda, AntonioAlfonso, Pablo BraÃ±as-Garza. Jose A.Cuesta, ÃngelSÃ¡nchez",08-sep-22,Social and Information Networks (cs.SI)," Networks of social interactions are the substrate upon which civilizations are built. Often, we create new bonds with people that we like or feel that our relationships are damaged through the intervention of third parties. Despite their importance and the huge impact that these processes have in our lives, quantitative scientific understanding of them is still in its infancy, mainly due to the difficulty of collecting large datasets of social networks including individual attributes. In this work, we present a thorough study of real social networks of 13 schools, with more than 3,000 students and 60,000 declared positive and negative relations, including tests for personal traits of all the students. We introduce a metric -- the `triadic influence' -- that measures the influence of nearest-neighbors in the relationships of their contacts. We use neural networks to predict the relationships and to extract the probability that two students are friends or enemies depending on their personal attributes or the triadic influence. We alternatively use a high-dimensional embedding of the network structure to also predict the relationships. Remarkably, the triadic influence (a simple one-dimensional metric) achieves the highest accuracy at predicting the relationship between two students. We postulate that the probabilities extracted from the neural networks -- functions of the triadic influence and the personalities of the students -- control the evolution of real social networks, opening a new avenue for the quantitative study of these systems.",https://arxiv.org/abs/2209.03684
Multi-signer Strong Designated Multi-verifier Signature Schemes basedon Multiple Cryptographic Algorithms,"NehaArora, R. K.Sharma",08-sep-22,Cryptography and Security (cs.CR), A designated verifier signature scheme allows a signer to generate a signature that only the designated verifier can verify. This paper proposes multi-signer strong designated multi-verifier signature schemes based on multiple cryptographic algorithms and has proven their security in the random oracle model.,https://arxiv.org/abs/2209.03683
MetaPriv: Acting in Favor of Privacy on Social Media Platforms,"RobertCantaragiu, AntonisMichalas, EugeneFrimpong, AlexandrosBakas",08-sep-22,Social and Information Networks (cs.SI)," Social networks such as Facebook (FB) and Instagram are known for tracking user online behaviour for commercial gain. To this day, there is practically no other way of achieving privacy in said platforms other than renouncing their use. However, many users are reluctant in doing so because of convenience or social and professional reasons. In this work, we propose a means of balancing convenience and privacy on FB through obfuscation. We have created MetaPriv, a tool based on simulating user interaction with FB. MetaPriv allows users to add noise interactions to their account so as to lead FB's profiling algorithms astray, and make them draw inaccurate profiles in relation to their interests and habits. To prove our tool's effectiveness, we ran extensive experiments on a dummy account and two existing user accounts. Our results showed that, by using our tool, users can achieve a higher degree of privacy in just a couple of weeks. We believe that MetaPriv can be further developed to accommodate other social media platforms and help users regain their privacy, while maintaining a reasonable level of convenience. To support open science and reproducible research, our source code is publicly available online.",https://arxiv.org/abs/2209.03682
Epic Fail: Emulators can tolerate polynomially many edge faults forfree,"GregBodwin, MichaelDinitz, YasaminNazari",08-sep-22,Data Structures and Algorithms (cs.DS)," A $t$-emulator of a graph $G$ is a graph $H$ that approximates its pairwise shortest path distances up to multiplicative $t$ error. We study fault tolerant $t$-emulators, under the model recently introduced by Bodwin, Dinitz, and Nazari [ITCS 2022] for vertex failures. In this paper we consider the version for edge failures, and show that they exhibit surprisingly different behavior.   In particular, our main result is that, for $(2k-1)$-emulators with $k$ odd, we can tolerate a polynomial number of edge faults for free. For example: for any $n$-node input graph, we construct a $5$-emulator ($k=3$) on $O(n^{4/3})$ edges that is robust to $f = O(n^{2/9})$ edge faults. It is well known that $\Omega(n^{4/3})$ edges are necessary even if the $5$-emulator does not need to tolerate any faults. Thus we pay no extra cost in the size to gain this fault tolerance. We leave open the precise range of free fault tolerance for odd $k$, and whether a similar phenomenon can be proved for even $k$.",https://arxiv.org/abs/2209.03679
Multisecret-sharing scheme with two-level security and itsapplications in Blockchain,"R. K.Sharma, RitumoniSarma, NehaArora, VidyaSagar",08-sep-22,Cryptography and Security (cs.CR)," A $(t,m)$-threshold secret sharing and multisecret-sharing scheme based on Shamir's SSS are introduced with two-level security using a one-way function. Besides we give its application in smart contract-enabled consortium blockchain network. The proposed scheme is thoroughly examined in terms of security and efficiency. Privacy, security, integrity, and scalability are also analyzed while applying it to the blockchain network.",https://arxiv.org/abs/2209.03675
Predict+Optimize for Packing and Covering LPs with Unknown Parametersin Constraints,"XinyiHu, Jasper C.H.Lee, JimmyH.M. Lee",08-sep-22,Artificial Intelligence (cs.AI)," Predict+Optimize is a recently proposed framework which combines machine learning and constrained optimization, tackling optimization problems that contain parameters that are unknown at solving time. The goal is to predict the unknown parameters and use the estimates to solve for an estimated optimal solution to the optimization problem. However, all prior works have focused on the case where unknown parameters appear only in the optimization objective and not the constraints, for the simple reason that if the constraints were not known exactly, the estimated optimal solution might not even be feasible under the true parameters. The contributions of this paper are two-fold. First, we propose a novel and practically relevant framework for the Predict+Optimize setting, but with unknown parameters in both the objective and the constraints. We introduce the notion of a correction function, and an additional penalty term in the loss function, modelling practical scenarios where an estimated optimal solution can be modified into a feasible solution after the true parameters are revealed, but at an additional cost. Second, we propose a corresponding algorithmic approach for our framework, which handles all packing and covering linear programs. Our approach is inspired by the prior work of Mandi and Guns, though with crucial modifications and re-derivations for our very different setting. Experimentation demonstrates the superior empirical performance of our method over classical approaches.",https://arxiv.org/abs/2209.03670
"R$^3$LIVE++: A Robust, Real-time, Radiance reconstruction package witha tightly-coupled LiDAR-Inertial-Visual state Estimator","JiarongLin, FuZhang",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Simultaneous localization and mapping (SLAM) are crucial for autonomous robots (e.g., self-driving cars, autonomous drones), 3D mapping systems, and AR/VR applications. This work proposed a novel LiDAR-inertial- visual fusion framework termed R$^3$LIVE++ to achieve robust and accurate state estimation while simultaneously reconstructing the radiance map on the fly. R$^3$LIVE++ consists of a LiDAR-inertial odometry (LIO) and a visual- inertial odometry (VIO), both running in real-time. The LIO subsystem utilizes the measurements from a LiDAR for reconstructing the geometric structure (i.e., the positions of 3D points), while the VIO subsystem simultaneously recovers the radiance information of the geometric structure from the input images. R$^3$LIVE++ is developed based on R$^3$LIVE and further improves the accuracy in localization and mapping by accounting for the camera photometric calibration (e.g., non-linear response function and lens vignetting) and the online estimation of camera exposure time. We conduct more extensive experiments on both public and our private datasets to compare our proposed system against other state-of-the-art SLAM systems. Quantitative and qualitative results show that our proposed system has significant improvements over others in both accuracy and robustness. In addition, to demonstrate the extendability of our work, {we developed several applications based on our reconstructed radiance maps, such as high dynamic range (HDR) imaging, virtual environment exploration, and 3D video gaming.} Lastly, to share our findings and make contributions to the community, we make our codes, hardware design, and dataset publicly available on our Github: [this http URL](http://github.com/hku-mars/r3live)",https://arxiv.org/abs/2209.03668
Generalized One-shot Domain Adaption of Generative AdversarialNetworks,"ZichengZhang, Yinglu Liu, Congying Han, Tiande Guo, Ting Yao, TaoMei",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The adaption of Generative Adversarial Network (GAN) aims to transfer a pre-trained GAN to a given domain with limited training data. In this paper, we focus on the one-shot case, which is more challenging and rarely explored in previous works. We consider that the adaptation from source domain to target domain can be decoupled into two parts: the transfer of global style like texture and color, and the emergence of new entities that do not belong to the source domain. While previous works mainly focus on the style transfer, we propose a novel and concise framework\footnote{\url{[this https URL](https://github.com/thevoidname/Generalized-One-shot-GAN-Adaption)}} to address the \textit{generalized one-shot adaption} task for both style and entity transfer, in which a reference image and its binary entity mask are provided. Our core objective is to constrain the gap between the internal distributions of the reference and syntheses by sliced Wasserstein distance. To better achieve it, style fixation is used at first to roughly obtain the exemplary style, and an auxiliary network is introduced to the original generator to disentangle entity and style transfer. Besides, to realize cross-domain correspondence, we propose the variational Laplacian regularization to constrain the smoothness of the adapted generator. Both quantitative and qualitative experiments demonstrate the effectiveness of our method in various scenarios.",https://arxiv.org/abs/2209.03666
Resource Allocation for URLLC and eMBB Traffic in Uplink WirelessNetworks,"Duan-ShinLee, Cheng-Shang Chang, Ruhui Zhang, Mao-Pin Lee",08-sep-22,Networking and Internet Architecture (cs.NI)," In this paper we consider two resource allocation problems of URLLC traffic and eMBB traffic in uplink 5G networks. We propose to divide frequencies into a common region and a grant-based region. Frequencies in the grant-based region can only be used by eMBB traffic, while frequencies in the common region can be used by eMBB traffic as well as URLLC traffic. In the first resource allocation problem we propose a two-player game to address the size of the grant-based region and the size of the common region. We show that this game has specific pure Nash equilibria. In the second resource allocation problem we determine the number of packets that each eMBB user can transmit in a request-grant cycle. We propose a constrained optimization problem to minimize the variance of the number of packets granted to the eMBB users. We show that a water-filling algorithm solves this constrained optimization problem. From simulation, we show that our scheme, consisting of resource allocation according to Nash equilibria of a game, persistent random retransmission of URLLC packets and allocation of eMBB packets by a water-filling algorithm, works better than four other heuristic methods.",https://arxiv.org/abs/2209.03665
Efficient Gender Debiasing of Pre-trained Indic Language Models,"NeerajaKirtane, VManushree, Aditya Kane",08-sep-22,Computation and Language (cs.CL)," The gender bias present in the data on which language models are pre-trained gets reflected in the systems that use these models. The model's intrinsic gender bias shows an outdated and unequal view of women in our culture and encourages discrimination. Therefore, in order to establish more equitable systems and increase fairness, it is crucial to identify and mitigate the bias existing in these models. While there is a significant amount of work in this area in English, there is a dearth of research being done in other gendered and low resources languages, particularly the Indian languages. English is a non-gendered language, where it has genderless nouns. The methodologies for bias detection in English cannot be directly deployed in other gendered languages, where the syntax and semantics vary. In our paper, we measure gender bias associated with occupations in Hindi language models. Our major contributions in this paper are the construction of a novel corpus to evaluate occupational gender bias in Hindi, quantify this existing bias in these systems using a well-defined metric, and mitigate it by efficiently fine-tuning our model. Our results reflect that the bias is reduced post-introduction of our proposed mitigation techniques. Our codebase is available publicly.",https://arxiv.org/abs/2209.03664
Tag-Aware Document Representation for Research Paper Recommendation,"Hebatallah A.Mohamed, GiuseppeSansonetti, AlessandroMicarelli",08-sep-22,Information Retrieval (cs.IR)," Finding online research papers relevant to one's interests is very challenging due to the increasing number of publications. Therefore, personalized research paper recommendation has become a significant and timely research topic. Collaborative filtering is a successful recommendation approach, which exploits the ratings given to items by users as a source of information for learning to make accurate recommendations. However, the ratings are often very sparse as in the research paper domain, due to the huge number of publications growing every year. Therefore, more attention has been drawn to hybrid methods that consider both ratings and content information. Nevertheless, most of the hybrid recommendation approaches that are based on text embedding have utilized bag-of-words techniques, which ignore word order and semantic meaning. In this paper, we propose a hybrid approach that leverages deep semantic representation of research papers based on social tags assigned by users. The experimental evaluation is performed on CiteULike, a real and publicly available dataset. The obtained findings show that the proposed model is effective in recommending research papers even when the rating data is very sparse.",https://arxiv.org/abs/2209.03661
Saliency-based Multiple Region of Interest Detection from a Single360Â° image,"YuukiSawabe, SatoshiIkehata, KiyoharuAizawa",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," 360Â° images are informative -- it contains omnidirectional visual information around the camera. However, the areas that cover a 360Â° image is much larger than the human's field of view, therefore important information in different view directions is easily overlooked. To tackle this issue, we propose a method for predicting the optimal set of Region of Interest (RoI) from a single 360Â° image using the visual saliency as a clue. To deal with the scarce, strongly biased training data of existing single 360Â° image saliency prediction dataset, we also propose a data augmentation method based on the spherical random data rotation. From the predicted saliency map and redundant candidate regions, we obtain the optimal set of RoIs considering both the saliency within a region and the Interaction-Over-Union (IoU) between regions. We conduct the subjective evaluation to show that the proposed method can select regions that properly summarize the input 360Â° image.",https://arxiv.org/abs/2209.03660
Practical Aspects of Membership Problem of Watson-Crick Context-freeGrammars,"JanHammer, ZbynÄ›kKÅ™ivka",08-sep-22,Formal Languages and Automata Theory (cs.FL)," This paper focuses on Watson-Crick languages inspired by DNA computing, their models, and algorithms for deciding the language membership. It analyzes a recently introduced algorithm called WK-CYK and introduces a state space search algorithm that is based on regular Breadth- first search but uses a number of optimizations and heuristics to be efficient in practical use and able to analyze longer inputs. The key parts are the heuristics for pruning the state space (detecting dead ends) and heuristics for choosing the most promising branches to continue the search.   These two algorithms have been tested with 20 different Watson-Crick grammars (40 including their Chomsky normal form versions). While WK-CYK is able to decide the language membership in a reasonable time for inputs of the length of roughly 30-50 symbols and its performance is very consistent for all kinds of grammars and inputs, the state space search is usually (89-98 % of cases) more efficient and able to do the computation for inputs with lengths of hundreds or even thousands of symbols. Thus, the state space search has the potential to be a good tool for practical Watson-Crick membership testing and is a good basis for improvement the efficiency of the algorithm in the future.",https://arxiv.org/abs/2209.03656
FETA: Towards Specializing Foundation Models for Expert TaskApplications,"AmitAlfassy, AssafArbelle, OshriHalimi, SivanHarary, RoeiHerzig, EliSchwartz, RameswarPanda, MicheleDolfi, ChristophAuer, KateSaenko, PeterW. J.Staar, RogerioFeris, LeonidKarlinsky",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Foundation Models (FMs) have demonstrated unprecedented capabilities including zero-shot learning, high fidelity data synthesis, and out of domain generalization. However, as we show in this paper, FMs still have poor out-of-the-box performance on expert tasks (e.g. retrieval of car manuals technical illustrations from language queries), data for which is either unseen or belonging to a long-tail part of the data distribution of the huge datasets used for FM pre-training. This underlines the necessity to explicitly evaluate and finetune FMs on such expert tasks, arguably ones that appear the most in practical real-world applications. In this paper, we propose a first of its kind FETA benchmark built around the task of teaching FMs to understand technical documentation, via learning to match their graphical illustrations to corresponding language descriptions. Our FETA benchmark focuses on text-to-image and image-to-text retrieval in public car manuals and sales catalogue brochures. FETA is equipped with a procedure for completely automatic annotation extraction (code would be released upon acceptance), allowing easy extension of FETA to more documentation types and application domains in the future. Our automatic annotation leads to an automated performance metric shown to be consistent with metrics computed on human-curated annotations (also released). We provide multiple baselines and analysis of popular FMs on FETA leading to several interesting findings that we believe would be very valuable to the FM community, paving the way towards real-world application of FMs for practical expert tasks currently 'overlooked' by standard benchmarks focusing on common objects.",https://arxiv.org/abs/2209.03655
Double stabilizations and convergence analysis of a second-orderlinear numerical scheme for the nonlocal Cahn-Hilliard equation,"XiaoLi, ZhonghuaQiao, ChengWang",08-sep-22,Numerical Analysis (math.NA)," In this paper, we study a second-order accurate and linear numerical scheme for the nonlocal Cahn-Hilliard equation. The scheme is established by combining a modified Crank-Nicolson approximation and the Adams-Bashforth extrapolation for the temporal discretization, and by applying the Fourier spectral collocation to the spatial discretization. In addition, two stabilization terms in different forms are added for the sake of the numerical stability. We conduct a complete convergence analysis by using the higher-order consistency estimate for the numerical scheme, combined with the rough error estimate and the refined estimate. By regarding the numerical solution as a small perturbation of the exact solution, we are able to justify the discrete $\ell^\infty$ bound of the numerical solution, as a result of the rough error estimate. Subsequently, the refined error estimate is derived to obtain the optimal rate of convergence, following the established $\ell^\infty$ bound of the numerical solution. Moreover, the energy stability is also rigorously proved with respect to a modified energy. The proposed scheme can be viewed as the generalization of the second-order scheme presented in an earlier work, and the energy stability estimate has greatly improved the corresponding result therein.",https://arxiv.org/abs/2209.03648
VizBelle: A Design Space of Embellishments for Data Visualization,"QingChen, ZiyanLiu, ChengweiWang, XingyuLan, YingChen, SimingChen, NanCao",08-sep-22,Human-Computer Interaction (cs.HC)," Visual embellishments, as a form of non-linguistic rhetorical figures, are used to help convey abstract concepts or attract readers' attention. Creating data visualizations with appropriate and visually pleasing embellishments is challenging since this process largely depends on the experience and the aesthetic taste of designers. To help facilitate designers in the ideation and creation process, we propose a design space, VizBelle, based on the analysis of 361 classified visualizations from online sources. VizBelle consists of four dimensions, namely, communication goal to fit user intention, object to select the target area, strategy and technique to offer potential approaches. We further provide a website to present detailed explanations and examples of various techniques. We conducted a within-subject study with 20 professional and amateur design enthusiasts to evaluate the effectiveness of our design space. Results show that our design space is illuminating and useful for designers to create data visualizations with embellishments.",https://arxiv.org/abs/2209.03647
Geolocation of Cultural Heritage using Multi-View Knowledge GraphEmbedding,"Hebatallah A.Mohamed, SebastianoVascon, FeliksHibraj, StuartJames, DiegoPilutti, Alessio DelBue, MarcelloPelillo",08-sep-22,Machine Learning (cs.LG)," Knowledge Graphs (KGs) have proven to be a reliable way of structuring data. They can provide a rich source of contextual information about cultural heritage collections. However, cultural heritage KGs are far from being complete. They are often missing important attributes such as geographical location, especially for sculptures and mobile or indoor entities such as paintings. In this paper, we first present a framework for ingesting knowledge about tangible cultural heritage entities from various data sources and their connected multi-hop knowledge into a geolocalized KG. Secondly, we propose a multi-view learning model for estimating the relative distance between a given pair of cultural heritage entities, based on the geographical as well as the knowledge connections of the entities.",https://arxiv.org/abs/2209.03642
Hierarchical Cache-Aided Linear Function Retrieval with Security andPrivacy Constraints,"YunKong, YoulongWu, MinquanCheng",08-sep-22,Information Theory (cs.IT)," The hierarchical caching system where a server connects with multiple mirror sites, each connecting with a distinct set of users, and both the mirror sites and users are equipped with caching memories has been widely studied. However all the existing works focus on single file retrieval, i.e., each user requests one file, and ignore the security and privacy threats in communications. In this paper we investigate the linear function retrieval problem for hierarchical caching systems with content security and demand privacy, i.e., each user requests a linear combination of files, and meanwhile the files in the library are protected against wiretappers and users' demands are kept unknown to other users and unconnected mirror sites. First we propose a new combination structure named hierarchical placement delivery array (HPDA), which characterizes the data placement and delivery strategy of a coded caching scheme. Then we construct two classes of HPDAs. Consequently two classes of schemes with or without security and privacy are obtained respectively where the first dedicates to minimizing the transmission load for the first hop and can achieve the optimal transmission load for the first hop if ignoring the security and privacy constraints; the second has more flexible parameters on the memory sizes and a lower subpacketization compared with the first one, and achieves a tradeoff between subpacketization and transmission loads.",https://arxiv.org/abs/2209.03638
Centralized Hierarchical Coded Caching Scheme Over Two-Layer Networks,"YunKong, YoulongWu, MinquanCheng",30 Apr 2022,Information Theory (cs.IT)," This paper considers a hierarchical caching system where a server connects with multiple mirror sites, each connecting with a distinct set of users, and both the mirror sites and users are equipped with caching memories. Although there already exist works studying this setup and proposing coded caching scheme to reduce transmission loads, two main problems are remained to address: 1) the optimal communication load under the uncoded placement for the first hop, denoted by $R_1$, is still unknown. 2) the previous schemes are based on Maddah-Ali and Niesen's data placement and delivery, which requires high subpacketization level. How to achieve the well tradeoff between transmission loads and subpacketization level for the hierarchical caching system is unclear. In this paper, we aim to address these two problems. We first propose a new combination structure named hierarchical placement delivery array (HPDA), which characterizes the data placement and delivery for any hierarchical caching system. Then we construct two classes of HPDAs, where the first class leads to a scheme achieving the optimal $R_1$ for some cases, and the second class requires a smaller subpacketization level at the cost of slightly increasing transmission loads.",https://arxiv.org/abs/2209.03633
AARGH! End-to-end Retrieval-Generation for Task-Oriented Dialog,"TomÃ¡Å¡Nekvinda, OndÅ™ejDuÅ¡ek",08-sep-22,Computation and Language (cs.CL)," We introduce AARGH, an end-to-end task-oriented dialog system combining retrieval and generative approaches in a single model, aiming at improving dialog management and lexical diversity of outputs. The model features a new response selection method based on an action-aware training objective and a simplified single-encoder retrieval architecture which allow us to build an end-to-end retrieval-enhanced generation model where retrieval and generation share most of the parameters. On the MultiWOZ dataset, we show that our approach produces more diverse outputs while maintaining or improving state tracking and context-to-response generation performance, compared to state-of-the-art baselines.",https://arxiv.org/abs/2205.00233
Enabling Connectivity for Automated Mobility: A Novel MQTT-basedInterface Evaluated in a 5G Case Study on Edge-Cloud Lidar Object Detection,"LennartReiher, BastianLampe, TimoWoopen, Raphael vanKempen, TillBeemelmanns, LutzEckstein",08-sep-22,Robotics (cs.RO)," Enabling secure and reliable high-bandwidth lowlatency connectivity between automated vehicles and external servers, intelligent infrastructure, and other road users is a central step in making fully automated driving possible. The availability of data interfaces, which allow this kind of connectivity, has the potential to distinguish artificial agents' capabilities in connected, cooperative, and automated mobility systems from the capabilities of human operators, who do not possess such interfaces. Connected agents can for example share data to build collective environment models, plan collective behavior, and learn collectively from the shared data that is centrally combined. This paper presents multiple solutions that allow connected entities to exchange data. In particular, we propose a new universal communication interface which uses the Message Queuing Telemetry Transport (MQTT) protocol to connect agents running the Robot Operating System (ROS). Our work integrates methods to assess the connection quality in the form of various key performance indicators in real-time. We compare a variety of approaches that provide the connectivity necessary for the exemplary use case of edge-cloud lidar object detection in a 5G network. We show that the mean latency between the availability of vehicle-based sensor measurements and the reception of a corresponding object list from the edge-cloud is below 87 ms. All implemented solutions are made open-source and free to use. Source code is available at [this https URL](https://github.com/ika-rwth-aachen/ros-v2x-benchmarking-suite).",https://arxiv.org/abs/2209.03632
Hierarchical Graph Pooling is an Effective Citywide Traffic ConditionPrediction Model,"ShilinPu, LiangChu, ZhuoranHou, JinchengHu, YanjunHuang, YuanjianZhang",08-sep-22,Machine Learning (cs.LG)," Accurate traffic conditions prediction provides a solid foundation for vehicle-environment coordination and traffic control tasks. Because of the complexity of road network data in spatial distribution and the diversity of deep learning methods, it becomes challenging to effectively define traffic data and adequately capture the complex spatial nonlinear features in the data. This paper applies two hierarchical graph pooling approaches to the traffic prediction task to reduce graph information redundancy. First, this paper verifies the effectiveness of hierarchical graph pooling methods in traffic prediction tasks. The hierarchical graph pooling methods are contrasted with the other baselines on predictive performance. Second, two mainstream hierarchical graph pooling methods, node clustering pooling and node drop pooling, are applied to analyze advantages and weaknesses in traffic prediction. Finally, for the mentioned graph neural networks, this paper compares the predictive effects of different graph network inputs on traffic prediction accuracy. The efficient ways of defining graph networks are analyzed and summarized.",https://arxiv.org/abs/2209.03630
Application of image-to-image translation in improving pedestriandetection,"DevarshPatel, SarthakPatel, MeghPatel",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The lack of effective target regions makes it difficult to perform several visual functions in low intensity light, including pedestrian recognition, and image-to-image translation. In this situation, with the accumulation of high-quality information by the combined use of infrared and visible images it is possible to detect pedestrians even in low light. In this study we are going to use advanced deep learning models like pix2pixGAN and YOLOv7 on LLVIP dataset, containing visible-infrared image pairs for low light vision. This dataset contains 33672 images and most of the images were captured in dark scenes, tightly synchronized with time and location.",https://arxiv.org/abs/2209.03629
Representing Camera Response Function by a Single Latent Variable andFully Connected Neural Network,"YunfengZhao, StuartFerguson, Huiyu Zhou, KarenRafferty",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Modelling the mapping from scene irradiance to image intensity is essential for many computer vision tasks. Such mapping is known as the camera response. Most digital cameras use a nonlinear function to map irradiance, as measured by the sensor to an image intensity used to record the photograph. Modelling of the response is necessary for the nonlinear calibration. In this paper, a new high-performance camera response model that uses a single latent variable and fully connected neural network is proposed. The model is produced using unsupervised learning with an autoencoder on real-world (example) camera responses. Neural architecture searching is then used to find the optimal neural network architecture. A latent distribution learning approach was introduced to constrain the latent distribution. The proposed model achieved state-of-the-art CRF representation accuracy in a number of benchmark tests, but is almost twice as fast as the best current models when performing the maximum likelihood estimation during camera response calibration due to the simple yet efficient model representation.",https://arxiv.org/abs/2209.03625
A Survey of Recent Advances in Deep Learning Models for DetectingMalware in Desktop and Mobile Platforms,"PascalManiriho, Abdun NaserMahmood, Mohammad Jabed MorshedChowdhury",08-sep-22,Cryptography and Security (cs.CR)," Malware is one of the most common and severe cyber-attack today. Malware infects millions of devices and can perform several malicious activities including mining sensitive data, encrypting data, crippling system performance, and many more. Hence, malware detection is crucial to protect our computers and mobile devices from malware attacks. Deep learning (DL) is one of the emerging and promising technologies for detecting malware. The recent high production of malware variants against desktop and mobile platforms makes DL algorithms powerful approaches for building scalable and advanced malware detection models as they can handle big datasets. This work explores current deep learning technologies for detecting malware attacks on the Windows, Linux, and Android platforms. Specifically, we present different categories of DL algorithms, network optimizers, and regularization methods. Different loss functions, activation functions, and frameworks for implementing DL models are presented. We also present feature extraction approaches and a review of recent DL-based models for detecting malware attacks on the above platforms. Furthermore, this work presents major research issues on malware detection including future directions to further advance knowledge and research in this field.",https://arxiv.org/abs/2209.03624
Black-Box Audits for Group Distribution Shifts,"MarcJuarez, Samuel Yeom, MattFredrikson",08-sep-22,Machine Learning (cs.LG)," When a model informs decisions about people, distribution shifts can create undue disparities. However, it is hard for external entities to check for distribution shift, as the model and its training set are often proprietary. In this paper, we introduce and study a black-box auditing method to detect cases of distribution shift that lead to a performance disparity of the model across demographic groups. By extending techniques used in membership and property inference attacks -- which are designed to expose private information from learned models -- we demonstrate that an external auditor can gain the information needed to identify these distribution shifts solely by querying the model. Our experimental results on real-world datasets show that this approach is effective, achieving 80-- 100% AUC-ROC in detecting shifts involving the underrepresentation of a demographic group in the training set. Researchers and investigative journalists can use our tools to perform non-collaborative audits of proprietary models and expose cases of underrepresentation in the training datasets.",https://arxiv.org/abs/2209.03622
Maximizing Consumer Satisfaction of IoT Energy Services,"AmaniAbusafia, AthmanBouguettaya, AbdallahLakhdari",08-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," We propose a novel Quality of Experience (QoE)-aware framework to crowdsource IoT energy services efficiently. The proposed framework leverages the provisioning of energy services as an auxiliary to increase consumers' satisfaction. A novel QoE model is developed as a metric to assess the consumers' satisfaction with the provisioning of energy services. Two novel composition algorithms, namely, Partial-Based (PB) and Demand- Based (DB) approaches, are proposed to ensure the highest QoE for consumers. Both approaches leverage the providers' flexibility and shareable nature of energy services to efficiently allocate services and optimize the QoE. A set of extensive experiments is conducted to evaluate the proposed approaches' efficiency and effectiveness.",https://arxiv.org/abs/2209.03620
Quality of Experience Optimization in IoT Energy Services,"AmaniAbusafia, AthmanBouguettaya, AbdallahLakhdari",29 Aug 2022,"Distributed, Parallel, and Cluster Computing (cs.DC)", We propose a novel Quality of Experience (QoE) metric as a key criterion to optimize the composition of energy services in a crowdsourced IoT environment. A novel importance-based composition algorithm is proposed to ensure the highest QoE for consumers. A set of experiments is conducted to evaluate the proposed approaches' effectiveness and efficiency.,https://arxiv.org/abs/2209.03619
Adaptive Combination of a Genetic Algorithm and Novelty Search forDeep Neuroevolution,"EyalSegal, MosheSipper",08-sep-22,Neural and Evolutionary Computing (cs.NE)," Evolutionary Computation (EC) has been shown to be able to quickly train Deep Artificial Neural Networks (DNNs) to solve Reinforcement Learning (RL) problems. While a Genetic Algorithm (GA) is well-suited for exploiting reward functions that are neither deceptive nor sparse, it struggles when the reward function is either of those. To that end, Novelty Search (NS) has been shown to be able to outperform gradient-following optimizers in some cases, while under-performing in others. We propose a new algorithm: Explore-Exploit $\gamma$-Adaptive Learner ($E^2\gamma AL$, or EyAL). By preserving a dynamically-sized niche of novelty-seeking agents, the algorithm manages to maintain population diversity, exploiting the reward signal when possible and exploring otherwise. The algorithm combines both the exploitation power of a GA and the exploration power of NS, while maintaining their simplicity and elegance. Our experiments show that EyAL outperforms NS in most scenarios, while being on par with a GA -- and in some scenarios it can outperform both. EyAL also allows the substitution of the exploiting component (GA) and the exploring component (NS) with other algorithms, e.g., Evolution Strategy and Surprise Search, thus opening the door for future research.",https://arxiv.org/abs/2208.13506
IMAP: Individual huMAn mobility Patterns visualizing platform,"Yisheng AlisonZheng, AmaniAbusafia, AbdallahLakhdari, Shing Tai TonyLui, AthmanBouguettaya",08-sep-22,Social and Information Networks (cs.SI)," Understanding human mobility is essential for the development of smart cities and social behavior research. Human mobility models may be used in numerous applications, including pandemic control, urban planning, and traffic management. The existing models' accuracy in predicting users' mobility patterns is less than 25%. The low accuracy may be justified by the flexible nature of the human movement. Indeed, humans are not rigid in their daily movement. In addition, the rigid mobility models may result in missing the hidden regularities in users' records. Thus, we propose a novel perspective to study and analyze human mobility patterns and capture their flexibility. Typically, the mobility patterns are represented by a sequence of locations. We propose to define the mobility patterns by abstracting these locations into a set of places. Labeling these locations will allow us to detect close-to-reality hidden patterns. We present IMAP, an Individual huMAn mobility Patterns visualizing platform. Our platform enables users to visualize a graph of the places they visited based on their history records. In addition, our platform displays the most frequent mobility patterns computed using a modified PrefixSpan approach.",https://arxiv.org/abs/2209.03618
DIY-IPS: Towards an Off-the-Shelf Accurate Indoor Positioning System,"RiccardoMenon, AbdallahLakhdari, AmaniAbusafia, Qijun He, AthmanBouguettaya",08-sep-22,Networking and Internet Architecture (cs.NI)," We present DIY-IPS - Do It Yourself - Indoor Positioning System, an open-source real-time indoor positioning mobile application. DIY-IPS detects users' indoor position by employing dual-band RSSI fingerprinting of available WiFi access points. The app can be used, without additional infrastructural costs, to detect users' indoor positions in real time. We published our app as an open source to save other researchers time recreating it. The app enables researchers/users to (1) collect indoor positioning datasets with a ground truth label, (2) customize the app for higher accuracy or other research purposes (3) test the accuracy of modified methods by live testing with ground truth. We ran preliminary experiments to demonstrate the effectiveness of the app.",https://arxiv.org/abs/2209.03615
Frame-Subtitle Self-Supervision for Multi-Modal Video QuestionAnswering,"JiongWang, ZhouZhao, WeikeJin",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Multi-modal video question answering aims to predict correct answer and localize the temporal boundary relevant to the question. The temporal annotations of questions improve QA performance and interpretability of recent works, but they are usually empirical and costly. To avoid the temporal annotations, we devise a weakly supervised question grounding (WSQG) setting, where only QA annotations are used and the relevant temporal boundaries are generated according to the temporal attention scores. To substitute the temporal annotations, we transform the correspondence between frames and subtitles to Frame-Subtitle (FS) self- supervision, which helps to optimize the temporal attention scores and hence improve the video-language understanding in VideoQA model. The extensive experiments on TVQA and TVQA+ datasets demonstrate that the proposed WSQG strategy gets comparable performance on question grounding, and the FS self- supervision helps improve the question answering and grounding performance on both QA-supervision only and full-supervision settings.",https://arxiv.org/abs/2209.03613
H2 Performance Analysis and Synthesis for Discrete-Time Linear Systemswith Dynamics Determined by an i.i.d. Process,"YoheiHosoe, TakashiOkamoto, TomomichiHagiwara",08-sep-22,Systems and Control (eess.SY)," This paper is concerned with H2 control of discrete-time linear systems with dynamics determined by an independent and identically distributed (i.i.d.) process. A definition of H2 norm is first discussed for the class of systems. Then, a linear matrix inequality (LMI) condition is derived for the associated performance analysis, which is tractable in the sense of numerical computation. The results about analysis are also extended toward state-feedback controller synthesis.",https://arxiv.org/abs/2209.03609
Analysis of the local discontinuous Galerkin method with generalizedfluxes for 1D nonlinear convection-diffusion systems,"HongjuanZhang, Boying Wu, Xiong Meng",08-sep-22,Numerical Analysis (math.NA)," In this paper, we present optimal error estimates of the local discontinuous Galerkin method with generalized numerical fluxes for one- dimensional nonlinear convection-diffusion systems. The upwind-biased flux with adjustable numerical viscosity for the convective term is chosen based on the local characteristic decomposition, which is helpful in resolving discontinuities of degenerate parabolic equations without enforcing any limiting procedure. For the diffusive term, a pair of generalized alternating fluxes are considered. By constructing and analyzing generalized Gauss-Radau projections with respect to different convective or diffusive terms, we derive optimal error estimates for nonlinear convection-diffusion systems with the symmetrizable flux Jacobian and fully nonlinear diffusive problems. Numerical experiments including long time simulations, different boundary conditions and degenerate equations with discontinuous initial data are provided to demonstrate the sharpness of theoretical results.",https://arxiv.org/abs/2209.03606
nVFNet-RDC: Replay and Non-Local Distillation Collaboration forContinual Object Detection,"JinxiangLai, WenlongLiu, JunLiu",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Continual Learning (CL) focuses on developing algorithms with the ability to adapt to new environments and learn new skills. This very challenging task has generated a lot of interest in recent years, with new solutions appearing rapidly. In this paper, we propose a nVFNet-RDC approach for continual object detection. Our nVFNet-RDC consists of teacher-student models, and adopts replay and feature distillation strategies. As the 1st place solutions, we achieve 55.94% and 54.65% average mAP on the 3rd CLVision Challenge Track 2 and Track 3, respectively.",https://arxiv.org/abs/2209.03604
Template-based Program Synthesis using StellensÃ¤tze,"Amir KafshdarGoharshady, S.Hitarth, FatemehMohammadi, Harshit JMotwani",08-sep-22,Programming Languages (cs.PL)," Template-based synthesis, also known as sketching, is a localized approach to program synthesis in which the programmer provides not only a specification, but also a high-level ``sketch'' of the program. The sketch is basically a partial program that models the general intuition of the programmer, while leaving the low-level details as unimplemented ``holes''. The role of the synthesis engine is then to fill in these holes such that the completed program satisfies the desired specification. In this work, we focus on template-based synthesis of polynomial imperative programs with real variables, i.e.~imperative programs in which all expressions appearing in assignments, conditions and guards are polynomials over program variables. While this problem can be solved in a sound and complete manner by a reduction to the first-order theory of the reals, the resulting formulas will contain a quantifier alternation and are extremely hard for modern SMT solvers, even when considering toy programs with a handful of lines. Moreover, the classical algorithms for quantifier elimination are notoriously unscalable and not at all applicable to this use-case.   In contrast, our main contribution is an algorithm, based on several well- known theorems in polyhedral and real algebraic geometry, namely Putinar's Positivstellensatz, the Real Nullstellensatz, Handelman's Theorem and Farkas' Lemma, which sidesteps the quantifier elimination difficulty and reduces the problem directly to Quadratic Programming (QP). Alternatively, one can view our algorithm as an efficient way of eliminating quantifiers in the particular formulas that appear in the synthesis problem. The resulting QP instances can then be handled quite easily by SMT solvers. Notably, our reduction to QP is sound and semi-complete, i.e.~it is complete if polynomials of a sufficiently high degree are used in the templates...",https://arxiv.org/abs/2209.03603
Wavenumber-explicit stability and convergence analysis of hp finiteelement discretizations of Helmholtz problems in piecewise smooth media,"M.Bernkopf, T. Chaumont-Frelet, J.M.Melenk",08-sep-22,Numerical Analysis (math.NA)," We present a wavenumber-explicit convergence analysis of the hp finite element method applied to a class of heterogeneous Helmholtz problems with piecewise analytic coefficients at large wavenumber $k$. Our analysis covers the heterogeneous Helmholtz equation with Robin, exact Dirichlet-to- Neumann, and second order absorbing boundary conditions, as well as perfectly matched layers.",https://arxiv.org/abs/2209.03602
Security Analysis of the EDHOC protocol,"BaptisteCottier, DavidPointcheval",08-sep-22,Cryptography and Security (cs.CR)," Ephemeral Diffie-Hellman Over COSE (EDHOC) aims at being a very compact and lightweight authenticated Diffie-Hellman key exchange with ephemeral keys. It is expected to provide mutual authentication, forward secrecy, and identity protection, with a 128-bit security level.A formal analysis has already been proposed at SECRYPT '21, on a former version, leading to some improvements, in the ongoing evaluation process by IETF. Unfortunately, while formal analysis can detect some misconceptions in the protocol, it cannot evaluate the actual security [this http URL](http://level.In) this paper, we study the last version. Without complete breaks, we anyway exhibit attacks in 2^64 operations, which contradict the expected 128-bit security level. We thereafter propose improvements, some of them being at no additional cost, to achieve 128-bit security for all the security properties (i.e. key privacy, mutual authentication, and identity-protection).",https://arxiv.org/abs/2209.03601
An Empirical Evaluation of Posterior Sampling for ConstrainedReinforcement Learning,"DanilProvodin, PratikGajane, MykolaPechenizkiy, MauritsKaptein",08-sep-22,Machine Learning (cs.LG)," We study a posterior sampling approach to efficient exploration in constrained reinforcement learning. Alternatively to existing algorithms, we propose two simple algorithms that are more efficient statistically, simpler to implement and computationally cheaper. The first algorithm is based on a linear formulation of CMDP, and the second algorithm leverages the saddle- point formulation of CMDP. Our empirical results demonstrate that, despite its simplicity, posterior sampling achieves state-of-the-art performance and, in some cases, significantly outperforms optimistic algorithms.",https://arxiv.org/abs/2209.03599
Levenshtein OCR,"ChengDa, PengWang, CongYao",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," A novel scene text recognizer based on Vision-Language Transformer (VLT) is presented. Inspired by Levenshtein Transformer in the area of NLP, the proposed method (named Levenshtein OCR, and LevOCR for short) explores an alternative way for automatically transcribing textual content from cropped natural images. Specifically, we cast the problem of scene text recognition as an iterative sequence refinement process. The initial prediction sequence produced by a pure vision model is encoded and fed into a cross-modal transformer to interact and fuse with the visual features, to progressively approximate the ground truth. The refinement process is accomplished via two basic character-level operations: deletion and insertion, which are learned with imitation learning and allow for parallel decoding, dynamic length change and good interpretability. The quantitative experiments clearly demonstrate that LevOCR achieves state-of-the-art performances on standard benchmarks and the qualitative analyses verify the effectiveness and advantage of the proposed LevOCR algorithm. Code will be released soon.",https://arxiv.org/abs/2209.03596
Multi-Granularity Prediction for Scene Text Recognition,"PengWang, ChengDa, CongYao",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Scene text recognition (STR) has been an active research topic in computer vision for years. To tackle this challenging problem, numerous innovative methods have been successively proposed and incorporating linguistic knowledge into STR models has recently become a prominent trend. In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet powerful vision STR model, which is built upon ViT and outperforms previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods. To integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an implicit way, i.e. , subword representations (BPE and WordPiece) widely-used in NLP are introduced into the output space, in addition to the conventional character level representation, while no independent language model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push the performance envelop of STR to an even higher level. Specifically, it achieves an average recognition accuracy of 93.35% on standard benchmarks. Code will be released soon.",https://arxiv.org/abs/2209.03594
Conformal Methods for Quantifying Uncertainty in Spatiotemporal Data:A Survey,SophiaSun,08-sep-22,Artificial Intelligence (cs.AI)," Machine learning methods are increasingly widely used in high-risk settings such as healthcare, transportation, and finance. In these settings, it is important that a model produces calibrated uncertainty to reflect its own confidence and avoid failures. In this paper we survey recent works on uncertainty quantification (UQ) for deep learning, in particular distribution-free Conformal Prediction method for its mathematical properties and wide applicability. We will cover the theoretical guarantees of conformal methods, introduce techniques that improve calibration and efficiency for UQ in the context of spatiotemporal data, and discuss the role of UQ in the context of safe decision making.",https://arxiv.org/abs/2209.03592
Sign Language Detection,"ShubhamDeshmukh, FavinFernandes, Amey Chavan",08-sep-22,Computer Vision and Pattern Recognition (cs.CV), With the advancements in Computer vision techniques the need to classify images based on its features have become a huge task and necessity. In this project we proposed 2 models i.e. feature extraction and classification using ORB and SVM and the second is using CNN architecture. The end result of the project is to understand the concept behind feature extraction and image classification. The trained CNN model will also be used to convert it to tflite format for Android Development.,https://arxiv.org/abs/2209.03580
Suspicious and Anomaly Detection,"ShubhamDeshmukh, FavinFernandes, MonaliAhire, DevarshiBorse, AmeyChavan",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In this project we propose a CNN architecture to detect anomaly and suspicious activities; the activities chosen for the project are running, jumping and kicking in public places and carrying gun, bat and knife in public places. With the trained model we compare it with the pre- existing models like Yolo, vgg16, vgg19. The trained Model is then implemented for real time detection and also used the. tflite format of the trained .h5 model to build an android classification.",https://arxiv.org/abs/2209.03578
SANIP: Shopping Assistant and Navigation for the visually impaired,"ShubhamDeshmukh, FavinFernandes, AmeyChavan, MonaliAhire, DevashriBorse, JyotiMadake",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The proposed shopping assistant model SANIP is going to help blind persons to detect hand held objects and also to get a video feedback of the information retrieved from the detected and recognized objects. The proposed model consists of three python models i.e. Custom Object Detection, Text Detection and Barcode detection. For object detection of the hand held object, we have created our own custom dataset that comprises daily goods such as Parle-G, Tide, and Lays. Other than that we have also collected images of Cart and Exit signs as it is essential for any person to use a cart and also notice the exit sign in case of emergency. For the other 2 models proposed the text and barcode information retrieved is converted from text to speech and relayed to the Blind person. The model was used to detect objects that were trained on and was successful in detecting and recognizing the desired output with a good accuracy and precision.",https://arxiv.org/abs/2209.03576
Real-time object detection method based on improved YOLOv4-tiny,"ZicongJiang, Liquan Zhao, Shuaiyang Li, Yanfei Jia","9 Nov 2020 (v1(https://arxiv.org/abs/2011.04244v1)), lastrevised 2 Dec 2020 (this version, v2)",Computer Vision and Pattern Recognition (cs.CV)," The ""You only look once v4""(YOLOv4) is one type of object detection methods in deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network structure and reduce parameters, which makes it be suitable for developing on the mobile and embedded devices. To improve the real-time of object detection, a fast object detection method is proposed based on YOLOv4-tiny. It firstly uses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules in Yolov4-tiny, which reduces the computation complexity. Secondly, it designs an auxiliary residual network block to extract more feature information of object to reduce detection error. In the design of auxiliary network, two consecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract global features, and channel attention and spatial attention are also used to extract more effective information. In the end, it merges the auxiliary network and backbone network to construct the whole network structure of improved YOLOv4-tiny. Simulation results show that the proposed method has faster object detection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of average precision as the YOLOv4-tiny. It is more suitable for real- time object detection.",https://arxiv.org/abs/2209.03570
GoonDAE: Denoising-Based Driver Assistance for Off-Road Teleoperation,"YounggeolCho, HyeonggeunYun, JinwonLee, ArimHa, JihyeokYun",08-sep-22,Robotics (cs.RO)," Because of the limitations of autonomous driving technologies, teleoperation is widely used in dangerous environments such as military operations. However, the teleoperated driving performance depends considerably on the driver's skill level. Moreover, unskilled drivers need extensive training time for teleoperations in unusual and harsh environments. To address this problem, we propose a novel denoising-based driver assistance method, namely GoonDAE, for real-time teleoperated off- road driving. The unskilled driver control input is assumed to be the same as the skilled driver control input but with noise. We designed a skip- connected long short-term memory (LSTM)-based denoising autoencoder (DAE) model to assist the unskilled driver control input by denoising. The proposed GoonDAE was trained with skilled driver control input and sensor data collected from our simulated off-road driving environment. To evaluate GoonDAE, we conducted an experiment with unskilled drivers in the simulated environment. The results revealed that the proposed system considerably enhanced driving performance in terms of driving stability.",https://arxiv.org/abs/2011.04244
Constructive Equivariant Observer Design for Inertial Velocity-AidedAttitude,"Pieter vanGoor, TarekHamel, RobertMahony",08-sep-22,Systems and Control (eess.SY)," Inertial Velocity-Aided Attitude (VAA) is an important problem in the control of Remotely Piloted Aerial Systems (RPAS), and involves estimating the velocity and attitude of a vehicle using gyroscope, accelerometer, and inertial-frame velocity (e.g. GPS velocity) measurements. Existing solutions tend to be complex and provide limited stability guarantees, relying on either high gain designs or assuming constant acceleration of the vehicle. This paper proposes a novel observer for inertial VAA that exploits Lie group symmetries of the system dynamics, and shows that the observer is synchronous with the system trajectories. This is achieved by adding a virtual state of only three dimensions, in contrast to the larger virtual states typically used in the literature. The error dynamics of the observer are shown to be almost globally asymptotically and locally exponentially stable. Finally, the observer is verified in simulation, where it is shown that the estimation error converges to zero even with an extremely poor initial condition.",https://arxiv.org/abs/2209.03568
SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained bySelf-supervised Learning,"PeizhuoLv, PanLi, ShenchenZhu, ShengzhiZhang, KaiChen, RuigangLiang, ChangYue, FanXiang, Yuling Cai, Hualong Ma, YingjunZhang, Guozhu Meng",08-sep-22,Cryptography and Security (cs.CR)," Recent years have witnessed significant success in Self-Supervised Learning (SSL), which facilitates various downstream tasks. However, attackers may steal such SSL models and commercialize them for profit, making it crucial to protect their Intellectual Property (IP). Most existing IP protection solutions are designed for supervised learning models and cannot be used directly since they require that the models' downstream tasks and target labels be known and available during watermark embedding, which is not always possible in the domain of SSL. To address such a problem especially when downstream tasks are diverse and unknown during watermark embedding, we propose a novel black-box watermarking solution, named SSL-WM, for protecting the ownership of SSL models. SSL-WM maps watermarked inputs by the watermarked encoders into an invariant representation space, which causes any downstream classifiers to produce expected behavior, thus allowing the detection of embedded watermarks. We evaluate SSL-WM on numerous tasks, such as Computer Vision (CV) and Natural Language Processing (NLP), using different SSL models, including contrastive-based and generative-based. Experimental results demonstrate that SSL-WM can effectively verify the ownership of stolen SSL models in various downstream tasks. Furthermore, SSL-WM is robust against model fine-tuning and pruning attacks. Lastly, SSL-WM can also evade detection from evaluated watermark detection approaches, demonstrating its promising application in protecting the IP of SSL models.",https://arxiv.org/abs/2209.03564
Video Vision Transformers for Violence Detection,"SanskarSingh, ShivaibhavDewangan, Ghanta SaiKrishna, VanditTyagi, SainathReddy",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Law enforcement and city safety are significantly impacted by detecting violent incidents in surveillance systems. Although modern (smart) cameras are widely available and affordable, such technological solutions are impotent in most instances. Furthermore, personnel monitoring CCTV recordings frequently show a belated reaction, resulting in the potential cause of catastrophe to people and property. Thus automated detection of violence for swift actions is very crucial. The proposed solution uses a novel end-to-end deep learning-based video vision transformer (ViViT) that can proficiently discern fights, hostile movements, and violent events in video sequences. The study presents utilizing a data augmentation strategy to overcome the downside of weaker inductive biasness while training vision transformers on a smaller training datasets. The evaluated results can be subsequently sent to local concerned authority, and the captured video can be analyzed. In comparison to state-of-theart (SOTA) approaches the proposed method achieved auspicious performance on some of the challenging benchmark datasets.",https://arxiv.org/abs/2209.03563
Automated Validation of Insurance Applications against CalculationSpecifications,"AdvaitaDatar, AmeyZare, AsiaA, RVenkatesh, Dr. ShrawanKumar, UlkaShrotri",08-sep-22,Software Engineering (cs.SE)," Insurance companies rely on their Legacy Insurance System (LIS) to govern day-to-day operations. These LIS operate as per the companys business rules that are formally specified in Calculation Specification (CS) sheets. To meet ever-changing business demands, insurance companies are increasingly transforming their outdated LIS to modern Policy Administration Systems (PAS). Quality Assurance (QA) of such PAS involves manual validation of calculations implementation against the corresponding CS sheets from the LIS. This manual QA approach is effort-intensive and error-prone, which may fail to detect inconsistencies in PAS implementations and ultimately result in monetary loss. To address this challenge, we propose a novel low-code no- code technique to automatically validate PAS implementation against CS sheets. Our technique has been evaluated on a digital transformation project of a large insurance company on 12 real-world calculations through 254 policies. The evaluation resulted in effort savings of approximately 92 percent against the conventional manual validation approach.",https://arxiv.org/abs/2209.03561
Does Attention Mechanism Possess the Feature of Human Reading? APerspective of Sentiment Classification Task,"LeiZhao, YingyiZhang, ChengzhiZhang",08-sep-22,Computation and Language (cs.CL)," [Purpose] To understand the meaning of a sentence, humans can focus on important words in the sentence, which reflects our eyes staying on each word in different gaze time or times. Thus, some studies utilize eye- tracking values to optimize the attention mechanism in deep learning models. But these studies lack to explain the rationality of this approach. Whether the attention mechanism possesses this feature of human reading needs to be explored. [Design/methodology/approach] We conducted experiments on a sentiment classification task. Firstly, we obtained eye-tracking values from two open-source eye-tracking corpora to describe the feature of human reading. Then, the machine attention values of each sentence were learned from a sentiment classification model. Finally, a comparison was conducted to analyze machine attention values and eye-tracking values. [Findings] Through experiments, we found the attention mechanism can focus on important words, such as adjectives, adverbs, and sentiment words, which are valuable for judging the sentiment of sentences on the sentiment classification task. It possesses the feature of human reading, focusing on important words in sentences when reading. Due to the insufficient learning of the attention mechanism, some words are wrongly focused. The eye-tracking values can help the attention mechanism correct this error and improve the model performance. [Originality/value] Our research not only provides a reasonable explanation for the study of using eye-tracking values to optimize the attention mechanism, but also provides new inspiration for the interpretability of attention mechanism.",https://arxiv.org/abs/2209.03558
Knowledge Based Template Machine Translation In Low-Resource Setting,"ZiluTang, DerryWijaya",08-sep-22,Computation and Language (cs.CL)," Incorporating tagging into neural machine translation (NMT) systems has shown promising results in helping translate rare words such as named entities (NE). However, translating NE in low-resource setting remains a challenge. In this work, we investigate the effect of using tags and NE hypernyms from knowledge graphs (KGs) in parallel corpus in different levels of resource conditions. We find the tag-and-copy mechanism (tag the NEs in the source sentence and copy them to the target sentence) improves translation in high-resource settings only. Introducing copying also results in polarizing effects in translating different parts-of-speech (POS). Interestingly, we find that copy accuracy for hypernyms is consistently higher than that of entities. As a way of avoiding ""hard"" copying and utilizing hypernym in bootstrapping rare entities, we introduced a ""soft"" tagging mechanism and found consistent improvement in high and low-resource settings.",https://arxiv.org/abs/2209.03557
Extractive is not Faithful: An Investigation of Broad UnfaithfulnessProblems in Extractive Summarization,"ShiyueZhang, DavidWan, MohitBansal",08-sep-22,Computation and Language (cs.CL)," The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference, incomplete coreference, incorrect discourse, incomplete discourse, as well as other misleading information. We ask humans to label these problems out of 1500 English summaries produced by 15 diverse extractive systems. We find that 33% of the summaries have at least one of the five issues. To automatically detect these problems, we find that 5 existing faithfulness evaluation metrics for summarization have poor correlations with human judgment. To remedy this, we propose a new metric, ExtEval, that is designed for detecting unfaithful extractive summaries and is shown to have the best performance. We hope our work can increase the awareness of unfaithfulness problems in extractive summarization and help future work to evaluate and resolve these issues. Our data and code are publicly available at [this https URL](https://github.com/ZhangShiyue/extractive_is_not_faithful)",https://arxiv.org/abs/2209.03554
MalDetConv: Automated Behaviour-based Malware Detection FrameworkBased on Natural Language Processing and Deep Learning Techniques,"PascalManiriho, Abdun NaserMahmood, Mohammad Jabed MorshedChowdhury",08-sep-22,Cryptography and Security (cs.CR)," The popularity of Windows attracts the attention of hackers/cyber- attackers, making Windows devices the primary target of malware attacks in recent years. Several sophisticated malware variants and anti-detection methods have been significantly enhanced and as a result, traditional malware detection techniques have become less effective. This work presents MalBehavD-V1, a new behavioural dataset of Windows Application Programming Interface (API) calls extracted from benign and malware executable files using the dynamic analysis approach. In addition, we present MalDetConV, a new automated behaviour-based framework for detecting both existing and zero-day malware attacks. MalDetConv uses a text processing-based encoder to transform features of API calls into a suitable format supported by deep learning models. It then uses a hybrid of convolutional neural network (CNN) and bidirectional gated recurrent unit (CNN-BiGRU) automatic feature extractor to select high-level features of the API Calls which are then fed to a fully connected neural network module for malware classification. MalDetConv also uses an explainable component that reveals features that contributed to the final classification outcome, helping the decision-making process for security analysts. The performance of the proposed framework is evaluated using our MalBehavD-V1 dataset and other benchmark datasets. The detection results demonstrate the effectiveness of MalDetConv over the state-of-the-art techniques with detection accuracy of 96.10%, 95.73%, 98.18%, and 99.93% achieved while detecting unseen malware from MalBehavD-V1, Allan and John, Brazilian, and Ki-D datasets, respectively. The experimental results show that MalDetConv is highly accurate in detecting both known and zero-day malware attacks on Windows devices.",https://arxiv.org/abs/2209.03549
Reward Delay Attacks on Deep Reinforcement Learning,"AnindyaSarkar, Jiarui Feng, YevgeniyVorobeychik, ChristopherGill, NingZhang",08-sep-22,Machine Learning (cs.LG)," Most reinforcement learning algorithms implicitly assume strong synchrony. We present novel attacks targeting Q-learning that exploit a vulnerability entailed by this assumption by delaying the reward signal for a limited time period. We consider two types of attack goals: targeted attacks, which aim to cause a target policy to be learned, and untargeted attacks, which simply aim to induce a policy with a low reward. We evaluate the efficacy of the proposed attacks through a series of experiments. Our first observation is that reward-delay attacks are extremely effective when the goal is simply to minimize reward. Indeed, we find that even naive baseline reward-delay attacks are also highly successful in minimizing the reward. Targeted attacks, on the other hand, are more challenging, although we nevertheless demonstrate that the proposed approaches remain highly effective at achieving the attacker's targets. In addition, we introduce a second threat model that captures a minimal mitigation that ensures that rewards cannot be used out of sequence. We find that this mitigation remains insufficient to ensure robustness to attacks that delay, but preserve the order, of rewards.",https://arxiv.org/abs/2209.03547
CAP: instance complexity-aware network pruning,"JiapengWang, MingMa, ZhenhuaYu",08-sep-22,Machine Learning (cs.LG)," Existing differentiable channel pruning methods often attach scaling factors or masks behind channels to prune filters with less importance, and assume uniform contribution of input samples to filter importance. Specifically, the effects of instance complexity on pruning performance are not yet fully investigated. In this paper, we propose a simple yet effective differentiable network pruning method CAP based on instance complexity-aware filter importance scores. We define instance complexity related weight for each sample by giving higher weights to hard samples, and measure the weighted sum of sample-specific soft masks to model non-uniform contribution of different inputs, which encourages hard samples to dominate the pruning process and the model performance to be well preserved. In addition, we introduce a new regularizer to encourage polarization of the masks, such that a sweet spot can be easily found to identify the filters to be pruned. Performance evaluations on various network architectures and datasets demonstrate CAP has advantages over the state-of-the-arts in pruning large networks. For instance, CAP improves the accuracy of ResNet56 on CIFAR-10 dataset by 0.33% aftering removing 65.64% FLOPs, and prunes 87.75% FLOPs of ResNet50 on ImageNet dataset with only 0.89% Top-1 accuracy loss.",https://arxiv.org/abs/2209.03540
CLaCLab at SocialDisNER: Using Medical Gazetteers for Named-EntityRecognition of Disease Mentions in Spanish Tweets,"HarshVerma, ParsaBagherzadeh, SabineBergler",08-sep-22,Computation and Language (cs.CL)," This paper summarizes the CLaC submission for SMM4H 2022 Task 10 which concerns the recognition of diseases mentioned in Spanish tweets. Before classifying each token, we encode each token with a transformer encoder using features from Multilingual RoBERTa Large, UMLS gazetteer, and DISTEMIST gazetteer, among others. We obtain a strict F1 score of 0.869, with competition mean of 0.675, standard deviation of 0.245, and median of 0.761.",https://arxiv.org/abs/2209.03534
OblivGM: Oblivious Attributed Subgraph Matching as a Cloud Service,"SongleiWang, YifengZheng, Xiaohua Jia, HejiaoHuang, CongWang",08-sep-22,Cryptography and Security (cs.CR)," In recent years there has been growing popularity of leveraging cloud computing for storing and querying attributed graphs, which have been widely used to model complex structured data in various applications. Such trend of outsourced graph analytics, however, is accompanied with critical privacy concerns regarding the information-rich and proprietary attributed graph data. In light of this, we design, implement, and evaluate OblivGM, a new system aimed at oblivious graph analytics services outsourced to the cloud. OblivGM focuses on the support for attributed subgraph matching, one popular and fundamental graph query functionality aiming to retrieve from a large attributed graph subgraphs isomorphic to a small query graph. Built from a delicate synergy of insights from attributed graph modelling and advanced lightweight cryptography, OblivGM protects the confidentiality of data content associated with attributed graphs and queries, conceals the connections among vertices in attributed graphs, and hides search access patterns. Meanwhile, OblivGM flexibly supports oblivious evaluation of varying subgraph queries, which may contain equality and/or range predicates. Extensive experiments over a real-world attributed graph dataset demonstrate that while providing strong security guarantees, OblivGM achieves practically affordable performance (with query latency on the order of a few seconds).",https://arxiv.org/abs/2209.03528
Machine Learning Sensors for Diagnosis of COVID-19 Disease UsingRoutine Blood Values for Internet of Things Application,"AndreiVelichko, Mehmet TahirHuyut, MaksimBelyaev, YuriyIzotov, DmitryKorzun",08-sep-22,Machine Learning (cs.LG)," Healthcare digitalization needs effective methods of human sensorics, when various parameters of the human body are instantly monitored in everyday life and connected to the Internet of Things (IoT). In particular, Machine Learning (ML) sensors for the prompt diagnosis of COVID-19 is an important case for IoT application in healthcare and Ambient Assistance Living (AAL). Determining the infected status of COVID-19 with various diagnostic tests and imaging results is costly and time-consuming. The aim of this study is to provide a fast, reliable and economical alternative tool for the diagnosis of COVID-19 based on the Routine Blood Values (RBV) values measured at admission. The dataset of the study consists of a total of 5296 patients with the same number of negative and positive COVID-19 test results and 51 routine blood values. In this study, 13 popular classifier machine learning models and LogNNet neural network model were exanimated. The most successful classifier model in terms of time and accuracy in the detection of the disease was the Histogram-based Gradient Boosting (HGB). The HGB classifier identified the 11 most important features (LDL, Cholesterol, HDL-C, MCHC, Triglyceride, Amylase, UA, LDH, CK-MB, ALP and MCH) to detect the disease with 100% accuracy, learning time 6.39 sec. In addition, the importance of single, double and triple combinations of these features in the diagnosis of the disease was discussed. We propose to use these 11 traits and their combinations as important biomarkers for ML sensors in diagnosis of the disease, supporting edge computing on Arduino and cloud IoT service.",https://arxiv.org/abs/2209.03526
Measuring Human Perception to Improve Open Set Recognition,"JinHuang, StudentMember, DerekPrijatelj, JustinDulay, WalterScheirer",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The human ability to recognize when an object is known or novel currently outperforms all open set recognition algorithms. Human perception as measured by the methods and procedures of visual psychophysics from psychology can provide an additional data stream for managing novelty in visual recognition tasks in computer vision. For instance, measured reaction time from human subjects can offer insight as to whether a known class sample may be confused with a novel one. In this work, we designed and performed a large-scale behavioral experiment that collected over 200,000 human reaction time measurements associated with object recognition. The data collected indicated reaction time varies meaningfully across objects at the sample level. We therefore designed a new psychophysical loss function that enforces consistency with human behavior in deep networks which exhibit variable reaction time for different images. As in biological vision, this approach allows us to achieve good open set recognition performance in regimes with limited labeled training data. Through experiments using data from ImageNet, significant improvement is observed when training Multi-Scale DenseNets with this new formulation: models trained with our loss function significantly improved top-1 validation accuracy by 7%, top-1 test accuracy on known samples by 18%, and top-1 test accuracy on unknown samples by 33%. We compared our method to 10 open set recognition methods from the literature, which were all outperformed on multiple metrics.",https://arxiv.org/abs/2209.03522
Analyzing and Calibrating Risk Assessment by Software Developers,"YukasaMurakami, MasateruTsunoda, Eduardo C.Campos",08-sep-22,Software Engineering (cs.SE)," In software project management, risk management is a critical factor. Project managers use existing lists of risk or perform brainstorming to identify the risks. However, it is not easy to perceive all the risks objectively. As a result, some risks are perceived based on subjective impression, which leads to risk biases. So, our goals are (i) We clarify the risk perception of developers to enhance the reliability of the brainstorming, and (ii) we calibrate the risk assessment based on a mathematical model to make more accurate risk list. In the analysis, we collected data concerning the risk perception of 69 professional software developers via a questionnaire. The average number of years of experience among these professionals was 18.3. Using the dataset, we applied factor analysis to clarify the factors that affect the evaluation of risk impact. The questionnaire was based on the risk perception theory established by Slovic, in which ""dread"" and ""unknown"" are the major factor of risk perception. The analysis result shows that (i) risk experience (i.e., whether a developer actually faced the risk or not) sometimes affects risk assessment (evaluation of risk impact), (ii) risk perception is considered to be based on unknown and dread factors, and (iii) risk assessment can be calibrated by a mathematical model (the average absolute error was 0.20).",https://arxiv.org/abs/2209.03519
Relationship between Gender and Code Reading Speed in SoftwareDevelopment,"YurikoTakatsuka, YukasaMurakami, MasateruTsunoda, MasahideNakamura",08-sep-22,Software Engineering (cs.SE)," Recently, workforce shortage has become a popular issue in information technology (IT). One solution to increasing the workforce supply is to increase the number of female IT professionals. This is because there is gender imbalance in information technology area. To accomplish this, it is important to suppress the influence of biases, such as the belief that men are more suited for careers in science and technology than women, and to increase the choice of careers available to female professionals. To help suppress the influence of gender bias, we analyzed the relationship between gender and code reading speed in the field of software development. Certain source codes require developers to use substantial memory to properly understand them, such as those with many variables that frequently change values. Several studies have indicated that the performance of memory differs in males and females. To test the veracity of this claim, we analyzed the influence of gender on code-reading speed through an experiment. Pursuant to this, we prepared four programs that required varied amounts of memory to properly understand them. Then, we measured the time required by each of the 17 male and 16 female subjects (33 subjects in total) to comprehend the different programs. The results suggest that there is no explicit difference between male and female subjects in this regard, even in the case of programs that require high memory capacities for proper understanding.",https://arxiv.org/abs/2209.03518
PMU Tracker: A Visualization Platform for Epicentric Event PropagationAnalysis in the Power Grid,"AnjanaArunkumar, AndreaPinceti, LalithaSankar, Chris Bryan",08-sep-22,Human-Computer Interaction (cs.HC)," The electrical power grid is a critical infrastructure, with disruptions in transmission having severe repercussions on daily activities, across multiple sectors. To identify, prevent, and mitigate such events, power grids are being refurbished as 'smart' systems that include the widespread deployment of GPS-enabled phasor measurement units (PMUs). PMUs provide fast, precise, and time-synchronized measurements of voltage and current, enabling real-time wide-area monitoring and control. However, the potential benefits of PMUs, for analyzing grid events like abnormal power oscillations and load fluctuations, are hindered by the fact that these sensors produce large, concurrent volumes of noisy data. In this paper, we describe working with power grid engineers to investigate how this problem can be addressed from a visual analytics perspective. As a result, we have developed PMU Tracker, an event localization tool that supports power grid operators in visually analyzing and identifying power grid events and tracking their propagation through the power grid's network. As a part of the PMU Tracker interface, we develop a novel visualization technique which we term an epicentric cluster dendrogram, which allows operators to analyze the effects of an event as it propagates outwards from a source location. We robustly validate PMU Tracker with: (1) a usage scenario demonstrating how PMU Tracker can be used to analyze anomalous grid events, and (2) case studies with power grid operators using a real-world interconnection dataset. Our results indicate that PMU Tracker effectively supports the analysis of power grid events; we also demonstrate and discuss how PMU Tracker's visual analytics approach can be generalized to other domains composed of time-varying networks with epicentric event characteristics.",https://arxiv.org/abs/2209.03516
A Secure and Efficient Multi-Object Grasping Detection Approach forRobotic Arms,"HuiWang, JierenCheng, Yichen Xu, Sirui Ni, Zaijia Yang, Jiangpeng Li",08-sep-22,Robotics (cs.RO)," Robotic arms are widely used in automatic industries. However, with wide applications of deep learning in robotic arms, there are new challenges such as the allocation of grasping computing power and the growing demand for security. In this work, we propose a robotic arm grasping approach based on deep learning and edge-cloud collaboration. This approach realizes the arbitrary grasp planning of the robot arm and considers the grasp efficiency and information security. In addition, the encoder and decoder trained by GAN enable the images to be encrypted while compressing, which ensures the security of privacy. The model achieves 92% accuracy on the OCID dataset, the image compression ratio reaches 0.03%, and the structural difference value is higher than 0.91.",https://arxiv.org/abs/2209.03514
RGB-X Classification for Electronics Sorting,"FNUAbhimanyu, TejasZodage, UmeshThillaivasan, Xinyue Lai, RahulChakwate, JavierSantillan, Emma Oti, Ming Zhao, RalphBoirum, HowieChoset, MatthewTravers",08-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Effectively disassembling and recovering materials from waste electrical and electronic equipment (WEEE) is a critical step in moving global supply chains from carbon-intensive, mined materials to recycled and renewable ones. Conventional recycling processes rely on shredding and sorting waste streams, but for WEEE, which is comprised of numerous dissimilar materials, we explore targeted disassembly of numerous objects for improved material recovery. Many WEEE objects share many key features and therefore can look quite similar, but their material composition and internal component layout can vary, and thus it is critical to have an accurate classifier for subsequent disassembly steps for accurate material separation and recovery. This work introduces RGB-X, a multi-modal image classification approach, that utilizes key features from external RGB images with those generated from X-ray images to accurately classify electronic objects. More specifically, this work develops Iterative Class Activation Mapping (iCAM), a novel network architecture that explicitly focuses on the finer-details in the multi-modal feature maps that are needed for accurate electronic object classification. In order to train a classifier, electronic objects lack large and well annotated X-ray datasets due to expense and need of expert guidance. To overcome this issue, we present a novel way of creating a synthetic dataset using domain randomization applied to the X-ray domain. The combined RGB-X approach gives us an accuracy of 98.6% on 10 generations of modern smartphones, which is greater than their individual accuracies of 89.1% (RGB) and 97.9% (X-ray) independently. We provide experimental results3 to corroborate our results.",https://arxiv.org/abs/2209.03511
So Much in So Little: Creating Lightweight Embeddings of PythonLibraries,"YaroslavGolubev, EgorBogomolov, EgorBulychev, TimofeyBryksin",07-sep-22,Software Engineering (cs.SE)," In software engineering, different approaches and machine learning models leverage different types of data: source code, textual information, historical data. An important part of any project is its dependencies. The list of dependencies is relatively small but carries a lot of semantics with it, which can be used to compare projects or make judgements about them.   In this paper, we focus on Python projects and their PyPi dependencies in the form of requirements.txt files. We compile a dataset of 7,132 Python projects and their dependencies, as well as use Git to pull their versions from previous years. Using this data, we build 32-dimensional embeddings of libraries by applying Singular Value Decomposition to the co-occurrence matrix of projects and libraries. We then cluster the embeddings and study their semantic relations.   To showcase the usefulness of such lightweight library embeddings, we introduce a prototype tool for suggesting relevant libraries to a given project. The tool computes project embeddings and uses dependencies of projects with similar embeddings to form suggestions. To compare different library recommenders, we have created a benchmark based on the evolution of dependency sets in open-source projects. Approaches based on the created embeddings significantly outperform the baseline of showing the most popular libraries in a given year. We have also conducted a user study that showed that the suggestions differ in quality for different project domains and that even relevant suggestions might be not particularly useful. Finally, to facilitate potentially more useful recommendations, we extended the recommender system with an option to suggest rarer libraries.",https://arxiv.org/abs/2209.03509
Tube-Based Zonotopic Data-Driven Predictive Control,"AlessioRusso, AlexandreProutiere",07-sep-22,Systems and Control (eess.SY)," We present a novel tube-based data-driven predictive control method for linear systems affected by a bounded addictive disturbance. Our method leverages recent results in the reachability analysis of unknown linear systems to formulate and solve a robust tube-based predictive control problem. More precisely, our approach consists in deriving, from the collected data, a zonotope that includes the true error set. In addition to that, we show how to guarantee the stability of the resulting error zonotope in a probabilistic sense. Results on a double-integrator affected by strong adversarial noise demonstrate the effectiveness of the proposed control approach.",https://arxiv.org/abs/2209.03507
Sell Me the Blackbox! Why eXplainable Artificial Intelligence (XAI)May Hurt Customers,"BehnamMohammadi, NikhilMalik, TimDerdenger, KannanSrinivasan",07-sep-22,Artificial Intelligence (cs.AI)," Recent AI algorithms are blackbox models whose decisions are difficult to interpret. eXplainable AI (XAI) seeks to address lack of AI interpretability and trust by explaining to customers their AI decision, e.g., decision to reject a loan application. The common wisdom is that regulating AI by mandating fully transparent XAI leads to greater social welfare. This paper challenges this notion through a game theoretic model for a policy-maker who maximizes social welfare, firms in a duopoly competition that maximize profits, and heterogenous consumers. The results show that XAI regulation may be redundant. In fact, mandating fully transparent XAI may make firms and customers worse off. This reveals a trade-off between maximizing welfare and receiving explainable AI outputs. We also discuss managerial implications for policy-maker and firms.",https://arxiv.org/abs/2209.03500
Evaluating Temporal Patterns in Applied Infant Affect Recognition,"AllenChang, LaurenKlein, Marcelo R.Rosales, Weiyang Deng, Beth A.Smith, Maja J.MatariÄ‡",07-sep-22,Human-Computer Interaction (cs.HC)," Agents must monitor their partners' affective states continuously in order to understand and engage in social interactions. However, methods for evaluating affect recognition do not account for changes in classification performance that may occur during occlusions or transitions between affective states. This paper addresses temporal patterns in affect classification performance in the context of an infant-robot interaction, where infants' affective states contribute to their ability to participate in a therapeutic leg movement activity. To support robustness to facial occlusions in video recordings, we trained infant affect recognition classifiers using both facial and body features. Next, we conducted an in- depth analysis of our best-performing models to evaluate how performance changed over time as the models encountered missing data and changing infant affect. During time windows when features were extracted with high confidence, a unimodal model trained on facial features achieved the same optimal performance as multimodal models trained on both facial and body features. However, multimodal models outperformed unimodal models when evaluated on the entire dataset. Additionally, model performance was weakest when predicting an affective state transition and improved after multiple predictions of the same affective state. These findings emphasize the benefits of incorporating body features in continuous affect recognition for infants. Our work highlights the importance of evaluating variability in model performance both over time and in the presence of missing data when applying affect recognition to social interactions.",https://arxiv.org/abs/2209.03499
Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2DImage Representations,"VadimTschernezki, Iro Laina, DianeLarlus, AndreaVedaldi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We present Neural Feature Fusion Fields (N3F), a method that improves dense 2D image feature extractors when the latter are applied to the analysis of multiple images reconstructible as a 3D scene. Given an image feature extractor, for example pre-trained using self-supervision, N3F uses it as a teacher to learn a student network defined in 3D space. The 3D student network is similar to a neural radiance field that distills said features and can be trained with the usual differentiable rendering machinery. As a consequence, N3F is readily applicable to most neural rendering formulations, including vanilla NeRF and its extensions to complex dynamic scenes. We show that our method not only enables semantic understanding in the context of scene-specific neural fields without the use of manual labels, but also consistently improves over the self-supervised 2D baselines. This is demonstrated by considering various tasks, such as 2D object retrieval, 3D segmentation, and scene editing, in diverse sequences, including long egocentric videos in the EPIC-KITCHENS benchmark.",https://arxiv.org/abs/2209.03496
On the Near-Optimality of Local Policies in Large Cooperative Multi-Agent Reinforcement Learning,"Washim UddinMondal, VaneetAggarwal, Satish V.Ukkusuri",07-sep-22,Machine Learning (cs.LG)," We show that in a cooperative $N$-agent network, one can design locally executable policies for the agents such that the resulting discounted sum of average rewards (value) well approximates the optimal value computed over all (including non-local) policies. Specifically, we prove that, if $|\mathcal{X}|, |\mathcal{U}|$ denote the size of state, and action spaces of individual agents, then for sufficiently small discount factor, the approximation error is given by $\mathcal{O}(e)$ where $e\triangleq \frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|}+\sqrt{|\mathcal{U}|}\right]$. Moreover, in a special case where the reward and state transition functions are independent of the action distribution of the population, the error improves to $\mathcal{O}(e)$ where $e\triangleq \frac{1}{\sqrt{N}}\sqrt{|\mathcal{X}|}$. Finally, we also devise an algorithm to explicitly construct a local policy. With the help of our approximation results, we further establish that the constructed local policy is within $\mathcal{O}(\max\\{e,\epsilon\\})$ distance of the optimal policy, and the sample complexity to achieve such a local policy is $\mathcal{O}(\epsilon^{-3})$, for any $\epsilon0$.",https://arxiv.org/abs/2209.03494
Peer to Peer Learning Platform Optimized With Machine Learning,VikramAnantha,07-sep-22,Computers and Society (cs.CY)," HELM Learning (Helping Everyone Learn More) is the first online peer-to-peer learning platform which allows students (typically middle-to- high school students) to teach classes and students (typically elementary- to-middle school students) to learn from classes for free. This method of class structure (peer-to-peer learning) has been proven effective for learning, as it promotes teamwork and collaboration, and enables active learning. HELM is a unique platform as it provides an easy process for students to create, teach and learn topics in a structured, peer-to-peer environment. Since HELM was created in April 2020, it has gotten over 4000 student sign ups and 80 teachers, in 4 continents around the world. HELM has grown from a simple website-and-Google-Form platform to having a backend system coded with Python, SQL, JavaScript and HTML, hosted on an AWS service. This not only makes it easier for students to sign up (as the students' information is saved in an SQL database, meaning they can sign up for classes without having to put in their information again, as well as getting automated emails about their classes), but also makes it easier for teachers to teach (as supplemental processes such as creating Zoom links, class recording folders, sending emails to students, etc. are done automatically). In addition, HELM has a recommendation machine learning algorithm which suggests classes and subjects students would enjoy taking, based on the previous classes a student has taken. This has created an easier experience for students to sign up for classes they are interested in.",https://arxiv.org/abs/2209.03491
Exposed Buffer Architecture,MicahBeck,07-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," The Internet stack is not a complete description of the resources and services needed to implement distributed applications, as it only accounts for communication services and the protocols that are defined to deliver them. This paper presents an account of the current distributed application architecture using a formal model of strictly layered systems, meaning that services in any layer can only depend on services in the layer immediately below it. By mapping a more complete Internet-based application stack that includes necessary storage and processing resources to this formal model, we are able to apply the Hourglass Theorem in order to compare alternative approaches in terms of their ""deployment scalability."" In particular, we contrast the current distributed application stack with Exposed Buffer Architecture, which has a converged spanning layer that allows for less-than-complete communication connectivity (exposing lower layer topology), but which also offers weak storage and processing services. This comparison shows that Exposed Buffer Architecture can have deployment scalability greater than the current distributed application stack while also providing minimally requisite storage and processing services.",https://arxiv.org/abs/2209.03489
A simple approach for quantizing neural networks,"JohannesMaly, RayanSaab",07-sep-22,Machine Learning (cs.LG)," In this short note, we propose a new method for quantizing the weights of a fully trained neural network. A simple deterministic pre- processing step allows us to quantize network layers via memoryless scalar quantization while preserving the network performance on given training data. On one hand, the computational complexity of this pre-processing slightly exceeds that of state-of-the-art algorithms in the literature. On the other hand, our approach does not require any hyper-parameter tuning and, in contrast to previous methods, allows a plain analysis. We provide rigorous theoretical guarantees in the case of quantizing single network layers and show that the relative error decays with the number of parameters in the network if the training data behaves well, e.g., if it is sampled from suitable random distributions. The developed method also readily allows the quantization of deep networks by consecutive application to single layers.",https://arxiv.org/abs/2209.03488
Energy Optimization of Wind Turbines via a Neural Control Policy Basedon Reinforcement Learning Markov Chain Monte Carlo Algorithm,"Vahid TavakolAghaei, ArdaAÄŸababaoÄŸlu, PeimanNaseradinmousavi, SinanYÄ±ldÄ±rÄ±m, SerhatYeÅŸilyurt, Ahmet Onat",07-sep-22,Systems and Control (eess.SY)," The primary focus of this paper is centered on the numerical analysis and optimal control of vertical axis wind turbines (VAWT) using Bayesian reinforcement learning (RL). We specifically tackle small-scale wind turbines with permanent magnet synchronous generator, which are well- suited to local and compact production of electrical energy in small scale such as urban and rural infrastructure installations. Through this work, we formulate and implement an RL strategy using Markov chain Monte Carlo (MCMC) algorithm to optimize the long-term energy output of the wind turbine. Our MCMC-based RL algorithm is a model-free and gradient-free algorithm, where the designer does not have to know the precise dynamics of the plant and their uncertainties. The method specifically overcomes the shortcomings typically associated with conventional solutions including but not limited to component aging, modeling errors and inaccuracies in the estimation of wind speed patterns. It has been observed to be especially successful in capturing power from wind transients; it modulates the generator load and hence rotor torque load so that the rotor tip speed reaches the optimum value for the anticipated wind speed. This ratio of rotor tip speed to wind speed is known to be critical in wind power applications. The wind to load energy efficiency of the proposed method is shown to be superior to the classical maximum power point tracking method.",https://arxiv.org/abs/2209.03487
Entity-based SpanCopy for Abstractive Summarization to Improve theFactual Consistency,"WenXiao, GiuseppeCarenini",07-sep-22,Computation and Language (cs.CL)," Despite the success of recent abstractive summarizers on automatic evaluation metrics, the generated summaries still present factual inconsistencies with the source document. In this paper, we focus on entity- level factual inconsistency, i.e. reducing the mismatched entities between the generated summaries and the source documents. We therefore propose a novel entity-based SpanCopy mechanism, and explore its extension with a Global Relevance component. Experiment results on four summarization datasets show that SpanCopy can effectively improve the entity-level factual consistency with essentially no change in the word-level and entity-level saliency. The code is available at [this https URL](https://github.com/Wendy-Xiao/Entity-based-SpanCopy)",https://arxiv.org/abs/2209.03485
Higher-order Clustering and Pooling for Graph Neural Networks,"AlexandreDuval, FragkiskosMalliaros",02-sep-22,Machine Learning (cs.LG)," Graph Neural Networks achieve state-of-the-art performance on a plethora of graph classification tasks, especially due to pooling operators, which aggregate learned node embeddings hierarchically into a final graph representation. However, they are not only questioned by recent work showing on par performance with random pooling, but also ignore completely higher- order connectivity patterns. To tackle this issue, we propose HoscPool, a clustering-based graph pooling operator that captures higher-order information hierarchically, leading to richer graph representations. In fact, we learn a probabilistic cluster assignment matrix end-to-end by minimising relaxed formulations of motif spectral clustering in our objective function, and we then extend it to a pooling operator. We evaluate HoscPool on graph classification tasks and its clustering component on graphs with ground-truth community structure, achieving best performance. Lastly, we provide a deep empirical analysis of pooling operators' inner functioning.",https://arxiv.org/abs/2209.03479
Supervised GAN Watermarking for Intellectual Property Protection,"JianweiFei, ZhihuaXia, BenedettaTondi, MauroBarni",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We propose a watermarking method for protecting the Intellectual Property (IP) of Generative Adversarial Networks (GANs). The aim is to watermark the GAN model so that any image generated by the GAN contains an invisible watermark (signature), whose presence inside the image can be checked at a later stage for ownership verification. To achieve this goal, a pre-trained CNN watermarking decoding block is inserted at the output of the generator. The generator loss is then modified by including a watermark loss term, to ensure that the prescribed watermark can be extracted from the generated images. The watermark is embedded via fine-tuning, with reduced time complexity. Results show that our method can effectively embed an invisible watermark inside the generated images. Moreover, our method is a general one and can work with different GAN architectures, different tasks, and different resolutions of the output image. We also demonstrate the good robustness performance of the embedded watermark against several post- processing, among them, JPEG compression, noise addition, blurring, and color transformations.",https://arxiv.org/abs/2209.03473
TAG: Learning Circuit Spatial Embedding From Layouts,"KerenZhu, HaoChen, WalkerJ. Turner, George F.Kokai, Po-Hsuan Wei, David Z.Pan, HaoxingRen",07-sep-22,Hardware Architecture (cs.AR)," Analog and mixed-signal (AMS) circuit designs still rely on human design expertise. Machine learning has been assisting circuit design automation by replacing human experience with artificial intelligence. This paper presents TAG, a new paradigm of learning the circuit representation from layouts leveraging text, self-attention and graph. The embedding network model learns spatial information without manual labeling. We introduce text embedding and a self-attention mechanism to AMS circuit learning. Experimental results demonstrate the ability to predict layout distances between instances with industrial FinFET technology benchmarks. The effectiveness of the circuit representation is verified by showing the transferability to three other learning tasks with limited data in the case studies: layout matching prediction, wirelength estimation, and net parasitic capacitance prediction.",https://arxiv.org/abs/2209.03466
Why So Toxic? Measuring and Triggering Toxic Behavior in Open-DomainChatbots,"Wai ManSi, MichaelBackes, JeremyBlackburn, Emiliano DeCristofaro, GianlucaStringhini, SavvasZannettou, Yand Zhang",07-sep-22,Computers and Society (cs.CY)," Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.",https://arxiv.org/abs/2209.03465
Scheduling Operator Assistance for Shared Autonomy in Multi-RobotTeams,"YifanCai, AbhinavDahiya, Nils Wilde, Stephen L.Smith",07-sep-22,Robotics (cs.RO)," In this paper, we consider the problem of allocating human operator assistance in a system with multiple autonomous robots. Each robot is required to complete independent missions, each defined as a sequence of tasks. While executing a task, a robot can either operate autonomously or be teleoperated by the human operator to complete the task at a faster rate. We show that the problem of creating a teleoperation schedule that minimizes makespan of the system is NP-Hard. We formulate our problem as a Mixed Integer Linear Program, which can be used to optimally solve small to moderate sized problem instances. We also develop an anytime algorithm that makes use of the problem structure to provide a fast and high-quality solution of the operator scheduling problem, even for larger problem instances. Our key insight is to identify blocking tasks in greedily-created schedules and iteratively remove those blocks to improve the quality of the solution. Through numerical simulations, we demonstrate the benefits of the proposed algorithm as an efficient and scalable approach that outperforms other greedy methods.",https://arxiv.org/abs/2209.03463
Information Maximization for Extreme Pose Face Recognition,"Mohammad Saeed EbrahimiSaadabadi, Sahar RahimiMalakshan, SobhanSoleymani, MoktariMostofa, Nasser M.Nasrabadi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In this paper, we seek to draw connections between the frontal and profile face images in an abstract embedding space. We exploit this connection using a coupled-encoder network to project frontal/profile face images into a common latent embedding space. The proposed model forces the similarity of representations in the embedding space by maximizing the mutual information between two views of the face. The proposed coupled- encoder benefits from three contributions for matching faces with extreme pose disparities. First, we leverage our pose-aware contrastive learning to maximize the mutual information between frontal and profile representations of identities. Second, a memory buffer, which consists of latent representations accumulated over past iterations, is integrated into the model so it can refer to relatively much more instances than the mini-batch size. Third, a novel pose-aware adversarial domain adaptation method forces the model to learn an asymmetric mapping from profile to frontal representation. In our framework, the coupled-encoder learns to enlarge the margin between the distribution of genuine and imposter faces, which results in high mutual information between different views of the same identity. The effectiveness of the proposed model is investigated through extensive experiments, evaluations, and ablation studies on four benchmark datasets, and comparison with the compelling state-of-the-art algorithms.",https://arxiv.org/abs/2209.03458
AILAB-Udine@SMM4H 22: Limits of Transformers and BERT Ensembles,"BeatricePortelli, SimoneScaboro, EmmanueleChersoni, EnricoSantus, GiuseppeSerra",07-sep-22,Computation and Language (cs.CL)," This paper describes the models developed by the AILAB-Udine team for the SMM4H 22 Shared Task. We explored the limits of Transformer based models on text classification, entity extraction and entity normalization, tackling Tasks 1, 2, 5, 6 and 10. The main take-aways we got from participating in different tasks are: the overwhelming positive effects of combining different architectures when using ensemble learning, and the great potential of generative models for term normalization.",https://arxiv.org/abs/2209.03456
A Greedy Algorithm for Building Compact Binary Activated NeuralNetworks,"BenjaminLeblanc, PascalGermain",07-sep-22,Machine Learning (cs.LG)," We study binary activated neural networks in the context of regression tasks, provide guarantees on the expressiveness of these particular networks and propose a greedy algorithm for building such networks. Aiming for predictors having small resources needs, the greedy approach does not need to fix in advance an architecture for the network: this one is built one layer at a time, one neuron at a time, leading to predictors that aren't needlessly wide and deep for a given task. Similarly to boosting algorithms, our approach guarantees a training loss reduction every time a neuron is added to a layer. This greatly differs from most binary activated neural networks training schemes that rely on stochastic gradient descent (circumventing the 0-almost-everywhere derivative problem of the binary activation function by surrogates such as the straight through estimator or continuous binarization). We show that our method provides compact and sparse predictors while obtaining similar performances to state- of-the-art methods for training binary activated networks.",https://arxiv.org/abs/2209.03452
Blessing of Class Diversity in Pre-training,"YulaiZhao, JianshuChen, SimonS. Du",07-sep-22,Machine Learning (cs.LG)," This paper presents a new statistical analysis aiming to explain the recent superior achievements of the pre-training techniques in natural language processing (NLP). We prove that when the classes of the pre- training task (e.g., different words in the masked language model task) are sufficiently diverse, in the sense that the least singular value of the last linear layer in pre-training (denoted as $\tilde{\nu}$) is large, then pre- training can significantly improve the sample efficiency of downstream tasks. Specially, we show the transfer learning excess risk enjoys an $O\left(\frac{1}{\tilde{\nu} \sqrt{n}}\right)$ rate, in contrast to the $O\left(\frac{1}{\sqrt{m}}\right)$ rate in the standard supervised learning. Here, $n$ is the number of pre-training data and $m$ is the number of data in the downstream task, and typically $n \gg m$. Our proof relies on a vector-form Rademacher complexity chain rule for disassembling composite function classes and a modified self-concordance condition. These techniques can be of independent interest.",https://arxiv.org/abs/2209.03450
"Same Coverage, Less Bloat: Accelerating Binary-only Fuzzing withCoverage-preserving Coverage-guided Tracing","StefanNagy, AnhNguyen-Tuong, Jason D.Hiser, Jack W.Davidson, MatthewHicks",07-sep-22,Cryptography and Security (cs.CR)," Coverage-guided fuzzing's aggressive, high-volume testing has helped reveal tens of thousands of software security flaws. While executing billions of test cases mandates fast code coverage tracing, the nature of binary-only targets leads to reduced tracing performance. A recent advancement in binary fuzzing performance is Coverage-guided Tracing (CGT), which brings orders-of-magnitude gains in throughput by restricting the expense of coverage tracing to only when new coverage is guaranteed. Unfortunately, CGT suits only a basic block coverage granularity -- yet most fuzzers require finer-grain coverage metrics: edge coverage and hit counts. It is this limitation which prohibits nearly all of today's state-of-the-art fuzzers from attaining the performance benefits of CGT.   This paper tackles the challenges of adapting CGT to fuzzing's most ubiquitous coverage metrics. We introduce and implement a suite of enhancements that expand CGT's introspection to fuzzing's most common code coverage metrics, while maintaining its orders-of-magnitude speedup over conventional always-on coverage tracing. We evaluate their trade-offs with respect to fuzzing performance and effectiveness across 12 diverse real- world binaries (8 open- and 4 closed-source). On average, our coverage- preserving CGT attains near-identical speed to the present block-coverage- only CGT, UnTracer; and outperforms leading binary- and source-level coverage tracers QEMU, Dyninst, RetroWrite, and AFL-Clang by 2-24x, finding more bugs in less time.",https://arxiv.org/abs/2209.03447
SmOOD: Smoothness-based Out-of-Distribution Detection Approach forSurrogate Neural Networks in Aircraft Design,"Houssem BenBraiek, Ali Tfaily, FoutseKhomh, Thomas Reid, Ciro Guida",07-sep-22,Machine Learning (cs.LG)," Aircraft industry is constantly striving for more efficient design optimization methods in terms of human efforts, computation time, and resource consumption. Hybrid surrogate optimization maintains high results quality while providing rapid design assessments when both the surrogate model and the switch mechanism for eventually transitioning to the HF model are calibrated properly. Feedforward neural networks (FNNs) can capture highly nonlinear input-output mappings, yielding efficient surrogates for aircraft performance factors. However, FNNs often fail to generalize over the out-of-distribution (OOD) samples, which hinders their adoption in critical aircraft design optimization. Through SmOOD, our smoothness-based out-of-distribution detection approach, we propose to codesign a model- dependent OOD indicator with the optimized FNN surrogate, to produce a trustworthy surrogate model with selective but credible predictions. Unlike conventional uncertainty-grounded methods, SmOOD exploits inherent smoothness properties of the HF simulations to effectively expose OODs through revealing their suspicious sensitivities, thereby avoiding over- confident uncertainty estimates on OOD samples. By using SmOOD, only high- risk OOD inputs are forwarded to the HF model for re-evaluation, leading to more accurate results at a low overhead cost. Three aircraft performance models are investigated. Results show that FNN-based surrogates outperform their Gaussian Process counterparts in terms of predictive performance. Moreover, SmOOD does cover averagely 85% of actual OODs on all the study cases. When SmOOD plus FNN surrogates are deployed in hybrid surrogate optimization settings, they result in a decrease error rate of 34.65% and a computational speed up rate of 58.36 times, respectively.",https://arxiv.org/abs/2209.03441
An efficient approach for nonconvex semidefinite optimization viacustomized alternating direction method of multipliers,ChuangchuangSun,07-sep-22,Systems and Control (eess.SY)," We investigate a class of general combinatorial graph problems, including MAX-CUT and community detection, reformulated as quadratic objectives over nonconvex constraints and solved via the alternating direction method of multipliers (ADMM).   We propose two reformulations: one using vector variables and a binary constraint, and the other further reformulating the Burer-Monteiro form for simpler subproblems.   Despite the nonconvex constraint, we prove the ADMM iterates converge to a stationary point in both formulations, under mild assumptions.   Additionally, recent work suggests that in this latter form, when the matrix factors are wide enough, local optimum with high probability is also the global optimum.   To demonstrate the scalability of our algorithm, we include results for MAX- CUT, community detection, and image segmentation benchmark and simulated examples.",https://arxiv.org/abs/2209.03438
ADMM for combinatorial graph problems,"ChuangchuangSun, YifanSun, RanDai",27-may-18,Optimization and Control (math.OC)," We investigate a class of general combinatorial graph problems, including MAX-CUT and community detection, reformulated as quadratic objectives over nonconvex constraints and solved via the alternating direction method of multipliers (ADMM). We propose two reformulations: one using vector variables and a binary constraint, and the other further reformulating the Burer-Monteiro form for simpler subproblems. Despite the nonconvex constraint, we prove the ADMM iterates converge to a stationary point in both formulations, under mild assumptions. Additionally, recent work suggests that in this latter form, when the matrix factors are wide enough, local optimum with high probability is also the global optimum. To demonstrate the scalability of our algorithm, we include results for MAX- CUT, community detection, and image segmentation benchmark and simulated examples.",https://arxiv.org/abs/2209.03437
Sporthesia: Augmenting Sports Videos Using Natural Language,"ZhutianChen, QisenYang, XiaoXie, JohannaBeyer, Haijun Xia, Yingcai Wu, HanspeterPfister",07-sep-22,Human-Computer Interaction (cs.HC)," Augmented sports videos, which combine visualizations and video effects to present data in actual scenes, can communicate insights engagingly and thus have been increasingly popular for sports enthusiasts around the world. Yet, creating augmented sports videos remains a challenging task, requiring considerable time and video editing skills. On the other hand, sports insights are often communicated using natural language, such as in commentaries, oral presentations, and articles, but usually lack visual cues. Thus, this work aims to facilitate the creation of augmented sports videos by enabling analysts to directly create visualizations embedded in videos using insights expressed in natural language. To achieve this goal, we propose a three-step approach - 1) detecting visualizable entities in the text, 2) mapping these entities into visualizations, and 3) scheduling these visualizations to play with the video - and analyzed 155 sports video clips and the accompanying commentaries for accomplishing these steps. Informed by our analysis, we have designed and implemented Sporthesia, a proof-of-concept system that takes racket-based sports videos and textual commentaries as the input and outputs augmented videos. We demonstrate Sporthesia's applicability in two exemplar scenarios, i.e., authoring augmented sports videos using text and augmenting historical sports videos based on auditory comments. A technical evaluation shows that Sporthesia achieves high accuracy (F1-score of 0.9) in detecting visualizable entities in the text. An expert evaluation with eight sports analysts suggests high utility, effectiveness, and satisfaction with our language-driven authoring method and provides insights for future improvement and opportunities.",https://arxiv.org/abs/1805.10678
Responsibility: An Example-based Explainable AI approach via TrainingProcess Inspection,"FarazKhadivpour, ArghasreeBanerjee, MatthewGuzdial",07-sep-22,Machine Learning (cs.LG)," Explainable Artificial Intelligence (XAI) methods are intended to help human users better understand the decision making of an AI agent. However, many modern XAI approaches are unintuitive to end users, particularly those without prior AI or ML knowledge. In this paper, we present a novel XAI approach we call Responsibility that identifies the most responsible training example for a particular decision. This example can then be shown as an explanation: ""this is what I (the AI) learned that led me to do that"". We present experimental results across a number of domains along with the results of an Amazon Mechanical Turk user study, comparing responsibility and existing XAI methods on an image classification task. Our results demonstrate that responsibility can help improve accuracy for both human end users and secondary ML models.",https://arxiv.org/abs/2209.03434
Physics-Guided Adversarial Machine Learning for Aircraft SystemsSimulation,"Houssem BenBraiek, Thomas Reid, Foutse Khomh",07-sep-22,Machine Learning (cs.LG)," In the context of aircraft system performance assessment, deep learning technologies allow to quickly infer models from experimental measurements, with less detailed system knowledge than usually required by physics-based modeling. However, this inexpensive model development also comes with new challenges regarding model trustworthiness. This work presents a novel approach, physics-guided adversarial machine learning (ML), that improves the confidence over the physics consistency of the model. The approach performs, first, a physics-guided adversarial testing phase to search for test inputs revealing behavioral system inconsistencies, while still falling within the range of foreseeable operational conditions. Then, it proceeds with physics-informed adversarial training to teach the model the system-related physics domain foreknowledge through iteratively reducing the unwanted output deviations on the previously-uncovered counterexamples. Empirical evaluation on two aircraft system performance models shows the effectiveness of our adversarial ML approach in exposing physical inconsistencies of both models and in improving their propensity to be consistent with physics domain knowledge.",https://arxiv.org/abs/2209.03433
"Foundations and Recent Trends in Multimodal Machine Learning:Principles, Challenges, and Open Questions","Paul PuLiang, Amir Zadeh, Louis-PhilippeMorency",07-sep-22,Machine Learning (cs.LG)," Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining two key principles of modality heterogeneity and interconnections that have driven subsequent innovations, and propose a taxonomy of 6 core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.",https://arxiv.org/abs/2209.03431
Using Computational Approaches in Visual Identity Design: A VisualIdentity for the Design and Multimedia Courses of Faculty of Sciences andTechnology of University of Coimbra,"SÃ©rgio M.Rebelo, TiagoMartins, ArturRebelo, JoÃ£oBicker, PenousalMachado",07-sep-22,Multimedia (cs.MM)," Computational approaches are beginning to be used to design dynamic visual identities fuelled by data and generative processes. In this work, we explore these computational approaches in order to generate a visual identity that creates bespoke letterings and images. We achieve this developing a generative design system that automatically assembles black and white visual modules. This system generates designs performing two main methods: (i) Assisted generation; and (ii) Automatic generation. Assisted generation method produces outputs wherein the placement of modules is determined by a configuration file previous defined. On the other hand, the Automatic generation method produces outputs wherein the modules are assembled to depict an input image. This system speeds up the process of design and deployment of one visual identity design as well as it generates outputs visual coherent among them. In this paper, we compressively describe this system and its achievements.",https://arxiv.org/abs/2209.03430
Bispectral Neural Networks,"SophiaSanborn, ChristianShewmake, BrunoOlshausen, ChristopherHillar",07-sep-22,Machine Learning (cs.LG)," We present a novel machine learning architecture, Bispectral Neural Networks (BNNs), for learning representations of data that are invariant to the actions of groups on the space over which a signal is defined. The model incorporates the ansatz of the bispectrum, an analytically defined group invariant that is complete--that is, it preserves all signal structure while removing only the variation due to group actions. Here, we demonstrate that BNNs are able to discover arbitrary commutative group structure in data, with the trained models learning the irreducible representations of the groups, which allows for the recovery of the group Cayley tables. Remarkably, trained networks learn to approximate bispectra on these groups, and thus possess the robustness, completeness, and generality of the analytical object.",https://arxiv.org/abs/2209.03420
A Survey of Neural Trees,"HaolingLi, JieSong, MengqiXue, HaofeiZhang, Jingwen Ye, LechaoCheng, Mingli Song",07-sep-22,Machine Learning (cs.LG)," Neural networks (NNs) and decision trees (DTs) are both popular models of machine learning, yet coming with mutually exclusive advantages and limitations. To bring the best of the two worlds, a variety of approaches are proposed to integrate NNs and DTs explicitly or implicitly. In this survey, these approaches are organized in a school which we term as neural trees (NTs). This survey aims to present a comprehensive review of NTs and attempts to identify how they enhance the model interpretability. We first propose a thorough taxonomy of NTs that expresses the gradual integration and co-evolution of NNs and DTs. Afterward, we analyze NTs in terms of their interpretability and performance, and suggest possible solutions to the remaining challenges. Finally, this survey concludes with a discussion about other considerations like conditional computation and promising directions towards this field. A list of papers reviewed in this survey, along with their corresponding codes, is available at: [this https URL](https://github.com/zju-vipa/awesome-neural-trees)",https://arxiv.org/abs/2209.03416
Convergence analysis of a block preconditioned steepest descenteigensolver with implicit deflation,"MingZhou, Zhaojun Bai, Yunfeng Cai, KlausNeymeyr",07-sep-22,Numerical Analysis (math.NA)," Gradient-type iterative methods for solving Hermitian eigenvalue problems can be accelerated by using preconditioning and deflation techniques. A preconditioned steepest descent iteration with implicit deflation (PSD-id) is one of such methods. The convergence behavior of the PSD-id is recently investigated based on the pioneering work of Samokish on the preconditioned steepest descent method (PSD). The resulting non- asymptotic estimates indicate a superlinear convergence of the PSD-id under strong assumptions on the initial guess. The present paper utilizes an alternative convergence analysis of the PSD by Neymeyr under much weaker assumptions. We embed Neymeyr's approach into the analysis of the PSD-id using a restricted formulation of the PSD-id. More importantly, we extend the new convergence analysis of the PSD-id to a practically preferred block version of the PSD-id, or BPSD-id, and show the cluster robustness of the BPSD-id. Numerical examples are provided to validate the theoretical estimates.",https://arxiv.org/abs/2209.03415
Counting Subgraphs in Somewhere Dense Graphs,"MarcoBressan, Leslie AnnGoldberg, Kitty Meeks, Marc Roth",07-sep-22,Computational Complexity (cs.CC)," We study the problems of counting copies and induced copies of a small pattern graph $H$ in a large host graph $G$. Recent work fully classified the complexity of those problems according to structural restrictions on the patterns $H$. In this work, we address the more challenging task of analysing the complexity for restricted patterns and restricted hosts. Specifically we ask which families of allowed patterns and hosts imply fixed-parameter tractability, i.e., the existence of an algorithm running in time $f(H)\cdot |G|^{O(1)}$ for some computable function $f$. Our main results present exhaustive and explicit complexity classifications for families that satisfy natural closure properties. Among others, we identify the problems of counting small matchings and independent sets in subgraph-closed graph classes $\mathcal{G}$ as our central objects of study and establish the following crisp dichotomies as consequences of the Exponential Time Hypothesis: (1) Counting $k$-matchings in a graph $G\in\mathcal{G}$ is fixed-parameter tractable if and only if $\mathcal{G}$ is nowhere dense. (2) Counting $k$-independent sets in a graph $G\in\mathcal{G}$ is fixed-parameter tractable if and only if $\mathcal{G}$ is nowhere dense. Moreover, we obtain almost tight conditional lower bounds if $\mathcal{G}$ is somewhere dense, i.e., not nowhere dense. These base cases of our classifications subsume a wide variety of previous results on the matching and independent set problem, such as counting $k$-matchings in bipartite graphs (Curticapean, Marx; FOCS 14), in $F$-colourable graphs (Roth, Wellnitz; SODA 20), and in degenerate graphs (Bressan, Roth; FOCS 21), as well as counting $k$-independent sets in bipartite graphs (Curticapean et al.; Algorithmica 19).",https://arxiv.org/abs/2209.03407
Small Proofs from Congruence Closure,"OliverFlatt, SamuelCoward, MaxWillsey, ZacharyTatlock, PavelPanchekha",07-sep-22,Programming Languages (cs.PL)," Satisfiability Modulo Theory (SMT) solvers and equality saturation engines must generate proof certificates from e-graph-based congruence closure procedures to enable verification and conflict clause generation. Smaller proof certificates speed up these activities. Though the problem of generating proofs of minimal size is known to be NP-complete, existing proof minimization algorithms for congruence closure generate unnecessarily large proofs and introduce asymptotic overhead over the core congruence closure procedure. In this paper, we introduce an O(n^5) time algorithm which generates optimal proofs under a new relaxed ""proof tree size"" metric that directly bounds proof size. We then relax this approach further to a practical O(n \log(n)) greedy algorithm which generates small proofs with no asymptotic overhead. We implemented our techniques in the egg equality saturation toolkit, yielding the first certifying equality saturation engine. We show that our greedy approach in egg quickly generates substantially smaller proofs than the state-of-the-art Z3 SMT solver on a corpus of 3760 benchmarks.",https://arxiv.org/abs/2209.03402
The (Un)Scalability of Heuristic Approximators for NP-Hard SearchProblems,"SumedhPendurkar, Taoan Huang, SvenKoenig, Guni Sharon",07-sep-22,Artificial Intelligence (cs.AI)," The A* algorithm is commonly used to solve NP-hard combinatorial optimization problems. When provided with an accurate heuristic function, A* can solve such problems in time complexity that is polynomial in the solution depth. This fact implies that accurate heuristic approximation for many such problems is also NP-hard. In this context, we examine a line of recent publications that propose the use of deep neural networks for heuristic approximation. We assert that these works suffer from inherent scalability limitations since -- under the assumption that P$\ne$NP -- such approaches result in either (a) network sizes that scale exponentially in the instance sizes or (b) heuristic approximation accuracy that scales inversely with the instance sizes. Our claim is supported by experimental results for three representative NP-hard search problems that show that fitting deep neural networks accurately to heuristic functions necessitates network sizes that scale exponentially with the instance size.",https://arxiv.org/abs/2209.03398
Investigating Reasons for Disagreement in Natural Language Inference,"Nan-JiangJiang, Marie-Catherine deMarneffe",07-sep-22,Computation and Language (cs.CL)," We investigate how disagreement in natural language inference (NLI) annotation arises. We developed a taxonomy of disagreement sources with 10 categories spanning 3 high-level classes. We found that some disagreements are due to uncertainty in the sentence meaning, others to annotator biases and task artifacts, leading to different interpretations of the label distribution. We explore two modeling approaches for detecting items with potential disagreement: a 4-way classification with a ""Complicated"" label in addition to the three standard NLI labels, and a multilabel classification approach. We found that the multilabel classification is more expressive and gives better recall of the possible interpretations in the data.",https://arxiv.org/abs/2209.03393
Securing the Spike: On the Transferabilty and Security of SpikingNeural Networks to Adversarial Examples,"Nuo Xu, KaleelMahmood, Haowen Fang, EthanRathbun, Caiwen Ding, Wujie Wen",07-sep-22,Neural and Evolutionary Computing (cs.NE)," Spiking neural networks (SNNs) have attracted much attention for their high energy efficiency and for recent advances in their classification performance. However, unlike traditional deep learning approaches, the analysis and study of the robustness of SNNs to adversarial examples remains relatively underdeveloped. In this work we advance the field of adversarial machine learning through experimentation and analyses of three important SNN security attributes. First, we show that successful white-box adversarial attacks on SNNs are highly dependent on the underlying surrogate gradient technique. Second, we analyze the transferability of adversarial examples generated by SNNs and other state-of-the-art architectures like Vision Transformers and Big Transfer CNNs. We demonstrate that SNNs are not often deceived by adversarial examples generated by Vision Transformers and certain types of CNNs. Lastly, we develop a novel white-box attack that generates adversarial examples capable of fooling both SNN models and non- SNN models simultaneously. Our experiments and analyses are broad and rigorous covering two datasets (CIFAR-10 and CIFAR-100), five different white-box attacks and twelve different classifier models.",https://arxiv.org/abs/2209.03392
Distilling Deep RL Models Into Interpretable Neuro-Fuzzy Systems,"ArneGevaert, JonathanPeck, YvanSaeys",07-sep-22,Machine Learning (cs.LG)," Deep Reinforcement Learning uses a deep neural network to encode a policy, which achieves very good performance in a wide range of applications but is widely regarded as a black box model. A more interpretable alternative to deep networks is given by neuro-fuzzy controllers. Unfortunately, neuro-fuzzy controllers often need a large number of rules to solve relatively simple tasks, making them difficult to interpret. In this work, we present an algorithm to distill the policy from a deep Q-network into a compact neuro-fuzzy controller. This allows us to train compact neuro-fuzzy controllers through distillation to solve tasks that they are unable to solve directly, combining the flexibility of deep reinforcement learning and the interpretability of compact rule bases. We demonstrate the algorithm on three well-known environments from OpenAI Gym, where we nearly match the performance of a DQN agent using only 2 to 6 fuzzy rules.",https://arxiv.org/abs/2209.03358
AST-GIN: Attribute-Augmented Spatial-Temporal Graph Informer Networkfor Electric Vehicle Charging Station Availability Forecasting,"RuikangLuo, YaofengSong, LipingHuang, YichengZhang, RongSu",07-sep-22,Machine Learning (cs.LG)," Electric Vehicle (EV) charging demand and charging station availability forecasting is one of the challenges in the intelligent transportation system. With the accurate EV station situation prediction, suitable charging behaviors could be scheduled in advance to relieve range anxiety. Many existing deep learning methods are proposed to address this issue, however, due to the complex road network structure and comprehensive external factors, such as point of interests (POIs) and weather effects, many commonly used algorithms could just extract the historical usage information without considering comprehensive influence of external factors. To enhance the prediction accuracy and interpretability, the Attribute- Augmented Spatial-Temporal Graph Informer (AST-GIN) structure is proposed in this study by combining the Graph Convolutional Network (GCN) layer and the Informer layer to extract both external and internal spatial-temporal dependence of relevant transportation data. And the external factors are modeled as dynamic attributes by the attribute-augmented encoder for training. AST-GIN model is tested on the data collected in Dundee City and experimental results show the effectiveness of our model considering external factors influence over various horizon settings compared with other baselines.",https://arxiv.org/abs/2209.03357
A hybrid Bayesian network for medical device risk assessment andmanagement,"JoshuaHunte, Martin Neil, NormanFenton",07-sep-22,Machine Learning (cs.LG)," ISO 14971 is the primary standard used for medical device risk management. While it specifies the requirements for medical device risk management, it does not specify a particular method for performing risk management. Hence, medical device manufacturers are free to develop or use any appropriate methods for managing the risk of medical devices. The most commonly used methods, such as Fault Tree Analysis (FTA), are unable to provide a reasonable basis for computing risk estimates when there are limited or no historical data available or where there is second-order uncertainty about the data. In this paper, we present a novel method for medical device risk management using hybrid Bayesian networks (BNs) that resolves the limitations of classical methods such as FTA and incorporates relevant factors affecting the risk of medical devices. The proposed BN method is generic but can be instantiated on a system-by-system basis, and we apply it to a Defibrillator device to demonstrate the process involved for medical device risk management during production and post-production. The example is validated against real-world data.",https://arxiv.org/abs/2209.03356
On Differential Privacy and Traffic State Estimation Problem forConnected Vehicles,"Suyash C.Vishnoi, Ahmad F.Taha, Sebastian A.Nugroho, Christian G.Claudel",06-sep-22,Systems and Control (eess.SY)," This letter focuses on the problem of traffic state estimation for highway networks with junctions in the form of on- and off-ramps while maintaining differential privacy of traffic data. Two types of sensors are considered, fixed sensors such as inductive loop detectors and connected vehicles which provide traffic density and speed data. The celebrated nonlinear second-order Aw-Rascle- Zhang (ARZ) model is utilized to model the traffic dynamics. The model is formulated as a nonlinear state-space difference equation. Sensitivity relations are derived for the given data which are then used to formulate a differentially private mechanism which adds a Gaussian noise to the data to make it differentially private. A Moving Horizon Estimation (MHE) approach is implemented for traffic state estimation using a linearized ARZ model. MHE is compared with Kalman Filter variants namely Extended Kalman Filter, Ensemble Kalman Filter and Unscented Kalman Filter. Several research and engineering questions are formulated and analysis is performed to find corresponding answers.",https://arxiv.org/abs/2209.03352
Traffic State Estimation for Connected Vehicles using the Second-OrderAw-Rascle-Zhang Traffic Model,"Suyash C.Vishnoi, Sebastian A.Nugroho, Ahmad F.Taha, Christian G.Claudel",06-sep-22,Systems and Control (eess.SY)," This paper addresses the problem of traffic state estimation (TSE) in the presence of heterogeneous sensors which include both fixed and moving sensors. Traditional fixed sensors are expensive and cannot be installed throughout the highway. Moving sensors such as Connected Vehicles (CVs) offer a relatively cheap alternative to measure traffic states across the network. Moving forward it is thus important to develop such models that effectively use the data from CVs. One such model is the nonlinear second- order Aw-Rascle-Zhang (ARZ) model which is a realistic traffic model, reliable for TSE and control. A state-space formulation is presented for the ARZ model considering junctions in the formulation which is important to model real highways with ramps. Linear approximation of the state-space model is investigated with respect to two techniques, first-order Taylor series approximation and Carleman linearization. A Moving Horizon Estimation (MHE) implementation is presented for TSE using a linearized ARZ model. Various state-estimation techniques used for TSE in the literature along with the presented approach are compared with regard to accuracy, computational tractability and parameter tuning with the help of a case study using the VISSIM traffic simulation software. Several research questions are posed and addressed with thorough analysis of the results.",https://arxiv.org/abs/2209.03349
Beyond Random Split for Assessing Statistical Model Performance,"CarlosCatania, JorgeGuerra, Juan ManuelRomero, GabrielCaffaratti, MartinMarchetta",04-sep-22,Machine Learning (cs.LG)," Even though a train/test split of the dataset randomly performed is a common practice, could not always be the best approach for estimating performance generalization under some scenarios. The fact is that the usual machine learning methodology can sometimes overestimate the generalization error when a dataset is not representative or when rare and elusive examples are a fundamental aspect of the detection problem. In the present work, we analyze strategies based on the predictors' variability to split in training and testing sets. Such strategies aim at guaranteeing the inclusion of rare or unusual examples with a minimal loss of the population's representativeness and provide a more accurate estimation about the generalization error when the dataset is not representative. Two baseline classifiers based on decision trees were used for testing the four splitting strategies considered. Both classifiers were applied on CTU19 a low- representative dataset for a network security detection problem. Preliminary results showed the importance of applying the three alternative strategies to the Monte Carlo splitting strategy in order to get a more accurate error estimation on different but feasible scenarios.",https://arxiv.org/abs/2209.02848
Apartness relations between propositions,Zoltan A.Kocsis,08-sep-22,Logic (math.LO)," We classify all apartness relations definable in propositional logics extending intuitionistic logic using Heyting algebra semantics. We show that every Heyting algebra which contains a non-trivial apartness term satisfies the weak law of excluded middle, and every Heyting algebra which contains a tight apartness term is in fact a Boolean algebra. This answers a question of E.~Rijke on the correct notion of apartness for propositions, and yields a short classification of apartness terms that can occur in a Heyting algebra. We also show that Martin-LÃ¶f Type Theory is not able to construct non-trivial apartness relations between propositions either.",https://arxiv.org/abs/2209.03346
Multiobjective Ranking and Selection Using Stochastic Kriging,"Sebastian RojasGonzalez, JuergenBranke, Inneke vanNieuwenhuyse",05-sep-22,Machine Learning (stat.ML)," We consider multiobjective simulation optimization problems, where several conflicting objectives are optimized simultaneously, and can only be observed via stochastic simulation. The goal is to find or approximate a (discrete) set of Pareto-optimal solutions that reveal the essential trade- offs between the objectives, where optimality means that no objective can be improved without deteriorating the quality of any other objective. The noise in the observed performance may lead to two possible misclassification errors: solutions that are truly Pareto-optimal can be wrongly considered dominated, and solutions that are truly dominated can be wrongly considered Pareto-optimal. We propose a Bayesian multiobjective ranking and selection method to reduce the number of errors when identifying the solutions with the true best expected performance. We use stochastic kriging metamodels to build reliable predictive distributions of the objectives, and exploit this information in two efficient screening procedures and two novel sampling criteria. We use these in a sequential sampling algorithm to decide how to allocate samples. Experimental results show that the proposed method only requires a small fraction of samples compared to the standard allocation method, and it's competitive against the state-of-the-art, with the exploitation of the correlation structure being the dominant contributor to the improvement.",https://arxiv.org/abs/2209.03920
A multi view multi stage and multi window framework for pulmonaryartery segmentation from CT scans,"ZeYuLiu, YiWang, YongZhang, HaoYin, ChaoGuo, ZhongyuWang",08-sep-22,Image and Video Processing (eess.IV)," This is the technical report of the 9th place in the final result of PARSE2022 Challenge. We solve the segmentation problem of the pulmonary artery by using a two-stage method based on a 3D CNN network. The coarse model is used to locate the ROI, and the fine model is used to refine the segmentation result. In addition, in order to improve the segmentation performance, we adopt multi-view and multi-window level method, at the same time we employ a fine-tune strategy to mitigate the impact of inconsistent labeling.",https://arxiv.org/abs/2209.03919
Ballot-Polling Audits of Instant-Runoff Voting Elections with aDirichlet-Tree Model,"FloydEverest, MichelleBlom, Philip B.Stark, Peter J.Stuckey, VanessaTeague, DamjanVukcevic",08-sep-22,Applications (stat.AP)," Instant-runoff voting (IRV) is used in several countries around the world. It requires voters to rank candidates in order of preference, and uses a counting algorithm that is more complex than systems such as first- past-the-post or scoring rules. An even more complex system, the single transferable vote (STV), is used when multiple candidates need to be elected. The complexity of these systems has made it difficult to audit the election outcomes. There is currently no known risk-limiting audit (RLA) method for STV, other than a full manual count of the ballots.   A new approach to auditing these systems was recently proposed, based on a Dirichlet-tree model. We present a detailed analysis of this approach for ballot-polling Bayesian audits of IRV elections. We compared several choices for the prior distribution, including some approaches using a Bayesian bootstrap (equivalent to an improper prior). Our findings include that the bootstrap-based approaches can be adapted to perform similarly to a full Bayesian model in practice, and that an overly informative prior can give counter-intuitive results. Via carefully chosen examples, we show why creating an RLA with this model is challenging, but we also suggest ways to overcome this.   As well as providing a practical and computationally feasible implementation of a Bayesian IRV audit, our work is important in laying the foundation for an RLA for STV elections.",https://arxiv.org/abs/2209.03918
Routing permutations on spectral expanders via matchings,RajkoNenadov,08-sep-22,Combinatorics (math.CO)," We consider the following matching-based routing problem. Initially, each vertex $v$ of a connected graph $G$ is occupied by a pebble which has a unique destination $\pi(v)$. In each round the pebbles across the edges of a selected matching in $G$ are swapped, and the goal is to route each pebble to its destination vertex in as few rounds as possible. We show that if $G$ is a sufficiently strong $d$-regular spectral expander then any permutation $\pi$ can be achieved in $O(\log n)$ rounds. This is optimal for constant $d$ and resolves a problem of Alon, Chung, and Graham [SIAM J. Discrete Math., 7 (1994), pp. 516--530].",https://arxiv.org/abs/2209.03881
Tuning arrays with rays: Physics-informed tuning of quantum dot chargestates,"Joshua Ziegler, FlorianLuthi, Mick Ramsey, FelixBorjans, GuojiZheng, Justyna P. Zwolak",08-sep-22,Mesoscale and Nanoscale Physics (cond-mat.mes-hall)," Quantum computers based on gate-defined quantum dots (QDs) are expected to scale. However, as the number of qubits increases, the burden of manually calibrating these systems becomes unreasonable and autonomous tuning must be used. There have been a range of recent demonstrations of automated tuning of various QD parameters such as coarse gate ranges, global state topology (e.g. single QD, double QD), charge, and tunnel coupling with a variety of methods. Here, we demonstrate an intuitive, reliable, and data- efficient set of tools for automated global state and charge tuning in a framework deemed physics-informed tuning (PIT). The first module of PIT is an action-based algorithm that combines a machine learning (ML) classifier with physics knowledge to navigate to a target global state. The second module uses a series of one-dimensional measurements to tune to a target charge state by first emptying the QDs of charge, followed by calibrating capacitive couplings, and navigating to the target charge state. The success rate for the action-based tuning consistently surpasses $95~\%$ on both simulated and experimental data suitable for off-line testing. The success rate for charge setting is comparable when testing with simulated data, at $95.5(5.4)~\%$, and only slightly worse for off-line experimental tests, with an average of $89.7(17.4)~\%$ (median $97.5~\%$). It's noteworthy that the high performance is demonstrated both on data from samples fabricated in an academic cleanroom as well as on an industrial 300 mm process line, further underlining the device-agnosticity of PIT. Together, these tests on a range of simulated and experimental devices demonstrate the effectiveness and robustness of PIT.",https://arxiv.org/abs/2209.03838
T$^2$LR-Net: An Unrolling Reconstruction Network Learning TransformedTensor Low-Rank prior for Dynamic MR Imaging,"YinghaoZhang, YueHu",08-sep-22,Image and Video Processing (eess.IV)," While the methods exploiting the tensor low-rank prior are booming in high-dimensional data processing and have obtained satisfying performance, their applications in dynamic magnetic resonance (MR) image reconstruction are limited. In this paper, we concentrate on the tensor singular value decomposition (t-SVD), which is based on the Fast Fourier Transform (FFT) and only provides the definite and limited tensor low-rank prior in the FFT domain, heavily reliant upon how closely the data and the FFT domain match up. By generalizing the FFT into an arbitrary unitary transformation of the transformed t-SVD and proposing the transformed tensor nuclear norm (TTNN), we introduce a flexible model based on TTNN with the ability to exploit the tensor low-rank prior of a transformed domain in a larger transformation space and elaborately design an iterative optimization algorithm based on the alternating direction method of multipliers (ADMM), which is further unrolled into a model-based deep unrolling reconstruction network to learn the transformed tensor low-rank prior (T$^2$LR-Net). The convolutional neural network (CNN) is incorporated within the T$^2$LR-Net to learn the best-matched transform from the dynamic MR image dataset. The unrolling reconstruction network also provides a new perspective on the low- rank prior utilization by exploiting the low-rank prior in the CNN-extracted feature domain. Experimental results on two cardiac cine MR datasets demonstrate that the proposed framework can provide improved recovery results compared with the state-of-the-art optimization-based and unrolling network-based methods.",https://arxiv.org/abs/2209.03837
Impact of dataset size and long-term ECoG-based BCI usage on deeplearning decoders performance,"MaciejÅšliwowski, MatthieuMartin, AntoineSouloumiac, PierreBlanchart, TetianaAksenova",08-sep-22,Signal Processing (eess.SP)," In brain-computer interfaces (BCI) research, recording data is time-consuming and expensive, which limits access to big datasets. This may influence the BCI system performance as machine learning methods depend strongly on the training dataset size. Important questions arise: taking into account neuronal signal characteristics (e.g., non-stationarity), can we achieve higher decoding performance with more data to train decoders? What is the perspective for further improvement with time in the case of long-term BCI studies? In this study, we investigated the impact of long- term recordings on motor imagery decoding from two main perspectives: model requirements regarding dataset size and potential for patient adaptation. We evaluated the multilinear model and two deep learning (DL) models on a long- term BCI and Tetraplegia NCT02550522 clinical trial dataset containing 43 sessions of ECoG recordings performed with a tetraplegic patient. In the experiment, a participant executed 3D virtual hand translation using motor imagery patterns. We designed multiple computational experiments in which training datasets were increased or translated to investigate the relationship between models' performance and different factors influencing recordings. Our analysis showed that adding more data to the training dataset may not instantly increase performance for datasets already containing 40 minutes of the signal. DL decoders showed similar requirements regarding the dataset size compared to the multilinear model while demonstrating higher decoding performance. Moreover, high decoding performance was obtained with relatively small datasets recorded later in the experiment, suggesting motor imagery patterns improvement and patient adaptation. Finally, we proposed UMAP embeddings and local intrinsic dimensionality as a way to visualize the data and potentially evaluate data quality.",https://arxiv.org/abs/2209.03832
A Novel Semi-supervised Meta Learning Method for Subject-transferBrain-computer Interface,"JingcongLi, FeiWang, HaiyunHuang, Feifei Qi, Jiahui Pan",07-sep-22,Signal Processing (eess.SP)," Brain-computer interface (BCI) provides a direct communication pathway between human brain and external devices. Before a new subject could use BCI, a calibration procedure is usually required. Because the inter- and intra-subject variances are so large that the models trained by the existing subjects perform poorly on new subjects. Therefore, effective subject- transfer and calibration method is essential. In this paper, we propose a semi-supervised meta learning (SSML) method for subject-transfer learning in BCIs. The proposed SSML learns a meta model with the existing subjects first, then fine-tunes the model in a semi-supervised learning manner, i.e. using few labeled and many unlabeled samples of target subject for calibration. It is significant for BCI applications where the labeled data are scarce or expensive while unlabeled data are readily available. To verify the SSML method, three different BCI paradigms are tested: 1) event- related potential detection; 2) emotion recognition; and 3) sleep staging. The SSML achieved significant improvements of over 15% on the first two paradigms and 4.9% on the third. The experimental results demonstrated the effectiveness and potential of the SSML method in BCI applications.",https://arxiv.org/abs/2209.03789
Respiratory Aware Routing for Cyclists,"AbigailLangbridge, PietroFerraro, RobertShorten",12 Aug 2022,Physics and Society (physics.soc-ph)," Cyclists travelling in urban areas are particularly at risk of harm from these emissions due to their increased breathing rate and proximity to vehicles. Our objective in this paper is to present a framework for routing of cyclists to mitigate the effects of pollution. However, in contrast to classical exposure based studies that are based on ambient pollution levels and travel times, the work presented here is also based on individualised fitness and physiological parameters. A key finding of this work is that statistical analysis of random synthetic commutes in London demonstrate that the impact of street-level pollution is significantly higher for less fit individuals. Further, our work suggests that pollution inhalation highly-polluted areas may be modulated through an increase of cycle velocity. These findings establish personalised travel optimisation as an effective method of reducing pollution risk, improving the net benefits of active commuting.",https://arxiv.org/abs/2209.03785
Self-Supervised Multimodal Fusion Transformer for Passive ActivityRecognition,"Armand K.Koupai, Mohammud J.Bocus, Raul Santos-Rodriguez, Robert J.Piechocki, RyanMcConville",15 Aug 2022,Signal Processing (eess.SP)," The pervasiveness of Wi-Fi signals provides significant opportunities for human sensing and activity recognition in fields such as healthcare. The sensors most commonly used for passive Wi-Fi sensing are based on passive Wi-Fi radar (PWR) and channel state information (CSI) data, however current systems do not effectively exploit the information acquired through multiple sensors to recognise the different activities. In this paper, we explore new properties of the Transformer architecture for multimodal sensor fusion. We study different signal processing techniques to extract multiple image-based features from PWR and CSI data such as spectrograms, scalograms and Markov transition field (MTF). We first propose the Fusion Transformer, an attention-based model for multimodal and multi- sensor fusion. Experimental results show that our Fusion Transformer approach can achieve competitive results compared to a ResNet architecture but with much fewer resources. To further improve our model, we propose a simple and effective framework for multimodal and multi-sensor self- supervised learning (SSL). The self-supervised Fusion Transformer outperforms the baselines, achieving a F1-score of 95.9%. Finally, we show how this approach significantly outperforms the others when trained with as little as 1% (2 minutes) of labelled training data to 20% (40 minutes) of labelled training data.",https://arxiv.org/abs/2209.03766
Deep Multi-Scale Representation Learning with Attention for AutomaticModulation Classification,"XiaoweiWu, ShengyunWei, YanZhou",31 Aug 2022,Signal Processing (eess.SP)," Currently, deep learning methods with stacking small size convolutional filters are widely used for automatic modulation classification (AMC). In this report, we find some experienced improvements by using large kernel size for convolutional deep convolution neural network based AMC, which is more efficient in extracting multi-scale features of the raw signal I/Q sequence data. Also, Squeeze-and-Excitation (SE) mechanisms can significantly help AMC networks to focus on the more important features of the signal. As a result, we propose a multi-scale feature network with large kernel size and SE mechanism (SE-MSFN) in this paper. SE-MSFN achieves state-of-the-art classification performance on the public well-known RADIOML 2018.01A dataset, with average classification accuracy of 64.50%, surpassing CLDNN by 1.42%, maximum classification accuracy of 98.5%, and an average classification accuracy of 85.53% in the lower SNR range 0dB to 10dB, surpassing CLDNN by 2.85%. In addition, we also verified that ensemble learning can help further improve classification performance. We hope this report can provide some references for developers and researchers in practical scenes.",https://arxiv.org/abs/2209.03765
Too Fine or Too Coarse? The Goldilocks Composition of Data Complexityfor Robust Left-Right Eye-Tracking Classifiers,"BrianXiang, AbdelrahmanAbdelmonsef",24 Aug 2022,Signal Processing (eess.SP)," The differences in distributional patterns between benchmark data and real-world data have been one of the main challenges of using electroencephalogram (EEG) signals for eye-tracking (ET) classification. Therefore, increasing the robustness of machine learning models in predicting eye-tracking positions from EEG data is integral for both research and consumer use. Previously, we compared the performance of classifiers trained solely on finer-grain data to those trained solely on coarse-grain. Results indicated that despite the overall improvement in robustness, the performance of the fine-grain trained models decreased, compared to coarse-grain trained models, when the testing and training set contained the same distributional patterns \cite{vectorbased}. This paper aims to address this case by training models using datasets of mixed data complexity to determine the ideal distribution of fine- and coarse-grain data. We train machine learning models utilizing a mixed dataset composed of both fine- and coarse-grain data and then compare the accuracies to models trained using solely fine- or coarse-grain data. For our purposes, finer- grain data refers to data collected using more complex methods whereas coarser-grain data refers to data collected using more simple methods. We apply covariate distributional shifts to test for the susceptibility of each training set. Our results indicated that the optimal training dataset for EEG-ET classification is not composed of solely fine- or coarse-grain data, but rather a mix of the two, leaning towards finer-grain.",https://arxiv.org/abs/2209.03764
Vector-Based Data Improves Left-Right Eye-Tracking ClassifierPerformance After a Covariate Distributional Shift,"BrianXiang, AbdelrahmanAbdelmonsef",31-jul-22,Machine Learning (cs.LG)," The main challenges of using electroencephalogram (EEG) signals to make eye-tracking (ET) predictions are the differences in distributional patterns between benchmark data and real-world data and the noise resulting from the unintended interference of brain signals from multiple sources. Increasing the robustness of machine learning models in predicting eye- tracking position from EEG data is therefore integral for both research and consumer use. In medical research, the usage of more complicated data collection methods to test for simpler tasks has been explored to address this very issue. In this study, we propose a fine-grain data approach for EEG-ET data collection in order to create more robust benchmarking. We train machine learning models utilizing both coarse-grain and fine-grain data and compare their accuracies when tested on data of similar/different distributional patterns in order to determine how susceptible EEG-ET benchmarks are to differences in distributional data. We apply a covariate distributional shift to test for this susceptibility. Results showed that models trained on fine-grain, vector-based data were less susceptible to distributional shifts than models trained on coarse-grain, binary-classified data.",https://arxiv.org/abs/2209.03761
Representation Learning for Appliance Recognition: A Comparison toClassical Machine Learning,"MatthiasKahl, DanielJorde, Hans-ArnoJacobsen",26 Aug 2022,Signal Processing (eess.SP)," Non-intrusive load monitoring (NILM) aims at energy consumption and appliance state information retrieval from aggregated consumption measurements, with the help of signal processing and machine learning algorithms. Representation learning with deep neural networks is successfully applied to several related disciplines. The main advantage of representation learning lies in replacing an expert-driven, hand-crafted feature extraction with hierarchical learning from many representations in raw data format. In this paper, we show how the NILM processing-chain can be improved, reduced in complexity and alternatively designed with recent deep learning algorithms. On the basis of an event-based appliance recognition approach, we evaluate seven different classification models: a classical machine learning approach that is based on a hand-crafted feature extraction, three different deep neural network architectures for automated feature extraction on raw waveform data, as well as three baseline approaches for raw data processing. We evaluate all approaches on two large- scale energy consumption datasets with more than 50,000 events of 44 appliances. We show that with the use of deep learning, we are able to reach and surpass the performance of the state-of-the-art classical machine learning approach for appliance recognition with an F-Score of 0.75 and 0.86 compared to 0.69 and 0.87 of the classical approach.",https://arxiv.org/abs/2208.00465
Improved Sensor-Based Animal Behavior Classification Performancethrough Conditional Generative Adversarial Network,"ZhuqingZhao, DongHa, AbhishekDamle, Barbara RoquetoDos, RobinWhite, Sook Ha",06-sep-22,Signal Processing (eess.SP)," Many activity classifications segments data into fixed window size for feature extraction and classification. However, animal behaviors have various durations that do not match the predetermined window size. The dense labeling and dense prediction methods address this limitation by predicting labels for every point. Thus, by tracing the starting and ending points, we could know the time location and duration of all occurring activities. Still, the dense prediction could be noisy with misalignments problems. We modified the U-Net and Conditional Generative Adversarial Network (cGAN) with customized loss functions as a training strategy to reduce fragmentation and other misalignments. In cGAN, the discriminator and generator trained against each other like an adversarial competition. The generator produces dense predictions. The discriminator works as a high- level consistency check, in our case, pushing the generator to predict activities with reasonable duration. The model trained with cGAN shows better or comparable performance in the cow, pig, and UCI HAPT dataset. The cGAN-trained modified U-Net improved from 92.17% to 94.66% for the UCI HAPT dataset and from 90.85% to 93.18% for pig data compared to previous dense prediction work.",https://arxiv.org/abs/2209.03759
Tensor product approach to modelling epidemics on networks,"Sergey V.Dolgov, Dmitry V.Savostyanov",30 Aug 2022,Physics and Society (physics.soc-ph)," To improve mathematical models of epidemics it is essential to move beyond the traditional assumption of homogeneous well--mixed population and involve more precise information on the network of contacts and transport links by which a stochastic process of the epidemics spreads. In general, the number of states of the network grows exponentially with its size, and a master equation description suffers from the curse of dimensionality. Almost all methods widely used in practice are versions of the stochastic simulation algorithm (SSA), which is notoriously known for its slow convergence. In this paper we numerically solve the chemical master equation for an SIR model on a general network using recently proposed tensor product algorithms. In numerical experiments we show that tensor product algorithms converge much faster than SSA and deliver more accurate results, which becomes particularly important for uncovering the probabilities of rare events, e.g. for number of infected people to exceed a (high) threshold.",https://arxiv.org/abs/2209.03758
Towards Multidimensional Textural Perception and ClassificationThrough Whisker,"Prasanna KumarRoutray, Aditya SanjivKanade, PaulinePounds, ManivannanMuniyandi",01-sep-22,Signal Processing (eess.SP)," Texture-based studies and designs have been in focus recently. Whisker-based multidimensional surface texture data is missing in the literature. This data is critical for robotics and machine perception algorithms in the classification and regression of textural surfaces. In this study, we present a novel sensor design to acquire multidimensional texture information. The surface texture's roughness and hardness were measured experimentally using sweeping and dabbing. Three machine learning models (SVM, RF, and MLP) showed excellent classification accuracy for the roughness and hardness of surface textures. We show that the combination of pressure and accelerometer data, collected from a standard machined specimen using the whisker sensor, improves classification accuracy. Further, we experimentally validate that the sensor can classify texture with roughness depths as low as $2.5\mu m$ at an accuracy of $90\%$ or more and segregate materials based on their roughness and hardness. We present a novel metric to consider while designing a whisker sensor to guarantee the quality of texture data acquisition beforehand. The machine learning model performance was validated against the data collected from the laser sensor from the same set of surface textures. As part of our work, we are releasing two- dimensional texture data: roughness and hardness to the research community.",https://arxiv.org/abs/2209.03756
CGAN-ECT: Tomography Image Reconstruction from Electrical CapacitanceMeasurements Using CGANs,"WaelDeabes, Alaa E. Abdel-Hakim",07-sep-22,Image and Video Processing (eess.IV)," Due to the rapid growth of Electrical Capacitance Tomography (ECT) applications in several industrial fields, there is a crucial need for developing high quality, yet fast, methodologies of image reconstruction from raw capacitance measurements. Deep learning, as an effective non-linear mapping tool for complicated functions, has been going viral in many fields including electrical tomography. In this paper, we propose a Conditional Generative Adversarial Network (CGAN) model for reconstructing ECT images from capacitance measurements. The initial image of the CGAN model is constructed from the capacitance measurement. To our knowledge, this is the first time to represent the capacitance measurements in an image form. We have created a new massive ECT dataset of 320K synthetic image measurements pairs for training, and testing the proposed model. The feasibility and generalization ability of the proposed CGAN-ECT model are evaluated using testing dataset, contaminated data and flow patterns that are not exposed to the model during the training phase. The evaluation results prove that the proposed CGAN-ECT model can efficiently create more accurate ECT images than traditional and other deep learning-based image reconstruction algorithms. CGAN-ECT achieved an average image correlation coefficient of more than 99.3% and an average relative image error about 0.07.",https://arxiv.org/abs/2209.03750
Losing momentum in continuous-time stochastic optimisation,"KexinJin, JonasLatz, ChenguangLiu, AlessandroScagliotti",08-sep-22,Optimization and Control (math.OC)," The training of deep neural networks and other modern machine learning models usually consists in solving non-convex optimisation problems that are high-dimensional and subject to large-scale data. Here, momentum- based stochastic optimisation algorithms have become especially popular in recent years. The stochasticity arises from data subsampling which reduces computational cost. Moreover, both, momentum and stochasticity are supposed to help the algorithm to overcome local minimisers and, hopefully, converge globally. Theoretically, this combination of stochasticity and momentum is badly understood.   In this work, we propose and analyse a continuous-time model for stochastic gradient descent with momentum. This model is a piecewise-deterministic Markov process that represents the particle movement by an underdamped dynamical system and the data subsampling through a stochastic switching of the dynamical system. In our analysis, we investigate longtime limits, the subsampling-to-no-subsampling limit, and the momentum-to-no-momentum limit. We are particularly interested in the case of reducing the momentum over time: intuitively, the momentum helps to overcome local minimisers in the initial phase of the algorithm, but prohibits fast convergence to a global minimiser later. Under convexity assumptions, we show convergence of our dynamical system to the global minimiser when reducing momentum over time and let the subsampling rate go to infinity.   We then propose a stable, symplectic discretisation scheme to construct an algorithm from our continuous-time dynamical system. In numerical experiments, we study our discretisation scheme in convex and non-convex test problems. Additionally, we train a convolutional neural network to solve the CIFAR-10 image classification problem. Here, our algorithm reaches competitive results compared to stochastic gradient descent with momentum.",https://arxiv.org/abs/2209.03737
Incremental Correction in Dynamic Systems Modelled with NeuralNetworks for Constraint Satisfaction,"NamhoonCho, Hyo-Sang Shin, AntoniosTsourdos, DavideAmato",08-sep-22,Optimization and Control (math.OC)," This study presents incremental correction methods for refining neural network parameters or control functions entering into a continuous- time dynamic system to achieve improved solution accuracy in satisfying the interim point constraints placed on the performance output variables. The proposed approach is to linearise the dynamics around the baseline values of its arguments, and then to solve for the corrective input required to transfer the perturbed trajectory to precisely known or desired values at specific time points, i.e., the interim points. Depending on the type of decision variables to adjust, parameter correction and control function correction methods are developed. These incremental correction methods can be utilised as a means to compensate for the prediction errors of pre- trained neural networks in real-time applications where high accuracy of the prediction of dynamical systems at prescribed time points is imperative. In this regard, the online update approach can be useful for enhancing overall targeting accuracy of finite-horizon control subject to point constraints using a neural policy. Numerical example demonstrates the effectiveness of the proposed approach in an application to a powered descent problem at Mars.",https://arxiv.org/abs/2209.03705
Learning-based and unrolled motion-compensated reconstruction forcardiac MR CINE imaging,"JiazhenPan, DanielRueckert, ThomasKÃ¼stner, KerstinHammernik",08-sep-22,Image and Video Processing (eess.IV)," Motion-compensated MR reconstruction (MCMR) is a powerful concept with considerable potential, consisting of two coupled sub-problems: Motion estimation, assuming a known image, and image reconstruction, assuming known motion. In this work, we propose a learning-based self-supervised framework for MCMR, to efficiently deal with non-rigid motion corruption in cardiac MR imaging. Contrary to conventional MCMR methods in which the motion is estimated prior to reconstruction and remains unchanged during the iterative optimization process, we introduce a dynamic motion estimation process and embed it into the unrolled optimization. We establish a cardiac motion estimation network that leverages temporal information via a group-wise registration approach, and carry out a joint optimization between the motion estimation and reconstruction. Experiments on 40 acquired 2D cardiac MR CINE datasets demonstrate that the proposed unrolled MCMR framework can reconstruct high quality MR images at high acceleration rates where other state-of-the-art methods fail. We also show that the joint optimization mechanism is mutually beneficial for both sub-tasks, i.e., motion estimation and image reconstruction, especially when the MR image is highly undersampled.",https://arxiv.org/abs/2209.03698
Quasi-Random Influences of Boolean Functions,"FanChung, NicholasSieger",08-sep-22,Combinatorics (math.CO)," We examine a hierarchy of equivalence classes of quasi-random properties of Boolean Functions. In particular, we prove an equivalence between a number of properties including balanced influences, spectral discrepancy, local strong regularity, homomorphism enumerations of colored or weighted graphs and hypergraphs associated with Boolean functions as well as the $k$th-order strict avalanche criterion amongst others. We further construct families of quasi-random boolean functions which exhibit the properties of our equivalence theorem and separate the levels of our hierarchy.",https://arxiv.org/abs/2209.03671
Quadratic Constraints for Local Stability Analysis of QuadraticSystems,"Shih-ChiLiao, Maziar S.Hemati, PeterSeiler",08-sep-22,Dynamical Systems (math.DS), This paper proposes new quadratic constraints (QCs) to bound a quadratic polynomial. Such QCs can be used in dissipation ineqaulities to analyze the stability and performance of nonlinear systems with quadratic vector fields. The proposed QCs utilize the sign-indefiniteness of certain classes of quadratic polynomials. These new QCs provide a tight bound on the quadratic terms along specific directions. This reduces the conservatism of the QC bounds as compared to the QCs in previous work. Two numerical examples of local stability analysis are provided to demonstrate the effectiveness of the proposed QCs.,https://arxiv.org/abs/2209.03573
Implicit Full Waveform Inversion with Deep Neural Representation,"JianSun, KristopherInnanen",08-sep-22,Geophysics (physics.geo-ph)," Full waveform inversion (FWI) commonly stands for the state-of- the-art approach for imaging subsurface structures and physical parameters, however, its implementation usually faces great challenges, such as building a good initial model to escape from local minima, and evaluating the uncertainty of inversion results. In this paper, we propose the implicit full waveform inversion (IFWI) algorithm using continuously and implicitly defined deep neural representations. Compared to FWI, which is sensitive to the initial model, IFWI benefits from the increased degrees of freedom with deep learning optimization, thus allowing to start from a random initialization, which greatly reduces the risk of non-uniqueness and being trapped in local minima. Both theoretical and experimental analyses indicates that, given a random initial model, IFWI is able to converge to the global minimum and produce a high-resolution image of subsurface with fine structures. In addition, uncertainty analysis of IFWI can be easily performed by approximating Bayesian inference with various deep learning approaches, which is analyzed in this paper by adding dropout neurons. Furthermore, IFWI has a certain degree of robustness and strong generalization ability that are exemplified in the experiments of various 2D geological models. With proper setup, IFWI can also be well suited for multi-scale joint geophysical inversion.",https://arxiv.org/abs/2209.03565
Synthesizing efficient circuits for Hamiltonian simulation,"Priyanka Mukhopadhyay, NathanWiebe, Hong Tao Zhang",07-sep-22,Quantum Physics (quant-ph)," We provide a new approach for compiling quantum simulation circuits that appear in Trotter, qDRIFT and multi-product formulas to Clifford and non-Clifford operations that can reduce the number of non- Clifford operations by a factor of up to $4$. The central idea behind our approach is to collect mutually commuting Hamiltonian terms into groups that satisfy one of several symmetries identified in this work which allow an inexpensive simulation of the entire group of terms. We further show that the cost can in some cases be reduced by partially allocating Hamiltonian terms to several groups and provide a polynomial time classical algorithm that can greedily allocate the terms to appropriate groupings. We further specifically discuss these optimizations for the case of fermionic dynamics and provide extensive numerical simulations for qDRIFT of our grouping strategy to 6 and 4-qubit Heisenberg models, $LiH$, $H_2$ and observe a factor of 1.8-3.2 reduction in the number of non-Clifford gates. This suggests Trotter-based simulation of chemistry in second quantization may be even more practical than previously believed.",https://arxiv.org/abs/2209.03525
Convolutional Neural Network (CNN) to reduce construction loss in JPEGcompression,SumanKunwar,26 Aug 2022,Image and Video Processing (eess.IV)," In recent decades, digital image processing has gained enormous popularity. Consequently, a number of data compression strategies have been put forth, with the goal of minimizing the amount of information required to represent images. Among them, JPEG compression is one of the most popular methods that has been widely applied in multimedia and digital applications. The periodic nature of DFT makes it impossible to meet the periodic condition of an image's opposing edges without producing severe artifacts, which lowers the image's perceptual visual quality. On the other hand, deep learning has recently achieved outstanding results for applications like speech recognition, image reduction, and natural language processing. Convolutional Neural Networks (CNN) have received more attention than most other types of deep neural networks. The use of convolution in feature extraction results in a less redundant feature map and a smaller dataset, both of which are crucial for image compression. In this work, an effective image compression method is purposed using autoencoders. The study's findings revealed a number of important trends that suggested better reconstruction along with good compression can be achieved using autoencoders.",https://arxiv.org/abs/2209.03478
Deep Learning-Based Automatic Diagnosis System for DevelopmentalDysplasia of the Hip,"YangLi, Leo YanLi-Han, Hua Tian",07-sep-22,Image and Video Processing (eess.IV)," As the first-line diagnostic imaging modality, radiography plays an essential role in the early detection of developmental dysplasia of the hip (DDH). Clinically, the diagnosis of DDH relies on manual measurements and subjective evaluation of different anatomical features from pelvic radiographs. This process is inefficient and error-prone and requires years of clinical experience. In this study, we propose a deep learning-based system that automatically detects 14 keypoints from a radiograph, measures three anatomical angles (center-edge, TÃ¶nnis, and Sharp angles), and classifies DDH hips as grades I-IV based on the Crowe criteria. Moreover, a novel data-driven scoring system is proposed to quantitatively integrate the information from the three angles for DDH diagnosis. The proposed keypoint detection model achieved a mean (95% confidence interval [CI]) average precision of 0.807 (0.804-0.810). The mean (95% CI) intraclass correlation coefficients between the center-edge, Tonnis, and Sharp angles measured by the proposed model and the ground-truth were 0.957 (0.952-0.962), 0.947 (0.941-0.953), and 0.953 (0.947-0.960), respectively, which were significantly higher than those of experienced orthopedic surgeons (p<0.0001). In addition, the mean (95% CI) test diagnostic agreement (Cohen's kappa) obtained using the proposed scoring system was 0.84 (0.83-0.85), which was significantly higher than those obtained from diagnostic criteria for individual angle (0.76 [0.75-0.77]) and orthopedists (0.71 [0.63-0.79]). To the best of our knowledge, this is the first study for objective DDH diagnosis by leveraging deep learning keypoint detection and integrating different anatomical measurements, which can provide reliable and explainable support for clinical decision-making.",https://arxiv.org/abs/2209.03475
On List Coloring with Separation of the Complete Graph and Set SystemIntersections,"Jean-ChristopheGodin, RÃ©miGrisot, OlivierTogni",07-sep-22,Combinatorics (math.CO)," We consider the following list coloring with separation problem: Given a graph $G$ and integers $a,b$, find the largest integer $c$ such that for any list assignment $L$ of $G$ with $|L(v)|= a$ for any vertex $v$ and $|L(u)\cap L(v)|\le c$ for any edge $uv$ of $G$, there exists an assignment $\varphi$ of sets of integers to the vertices of $G$ such that $\varphi(u)\subset L(u)$ and $|\varphi(v)|=b$ for any vertex $u$ and $\varphi(u)\cap \varphi(v)=\emptyset$ for any edge $uv$. Such a value of $c$ is called the separation number of $(G,a,b)$. Using a special partition of a set of lists for which we obtain an improved version of PoincarÃ©'s crible, we determine the separation number of the complete graph $K_n$ for some values of $a,b$ and $n$, and prove bounds for the remaining values.",https://arxiv.org/abs/2209.03440
Causal discovery for time series with latent confounders,ChristianReiser,07-sep-22,Machine Learning (stat.ML)," Reconstructing the causal relationships behind the phenomena we observe is a fundamental challenge in all areas of science. Discovering causal relationships through experiments is often infeasible, unethical, or expensive in complex systems. However, increases in computational power allow us to process the ever-growing amount of data that modern science generates, leading to an emerging interest in the causal discovery problem from observational data. This work evaluates the LPCMCI algorithm, which aims to find generators compatible with a multi-dimensional, highly autocorrelated time series while some variables are unobserved. We find that LPCMCI performs much better than a random algorithm mimicking not knowing anything but is still far from optimal detection. Furthermore, LPCMCI performs best on auto-dependencies, then contemporaneous dependencies, and struggles most with lagged dependencies. The source code of this project is available online.",https://arxiv.org/abs/2209.03436
Planted matching problems on random hypergraphs,"Urte Adomaityte, AnshulToshniwal, GabrieleSicuro, LenkaZdeborovÃ¡",07-sep-22,Disordered Systems and Neural Networks (cond-mat.dis-nn)," We consider the problem of inferring a matching hidden in a weighted random $k$-hypergraph. We assume that the hyperedges' weights are random and distributed according to two different densities conditioning on the fact that they belong to the hidden matching, or not. We show that, for $k2$ and in the large graph size limit, an algorithmic first order transition in the signal strength separates a regime in which a complete recovery of the hidden matching is feasible from a regime in which partial recovery is possible. This is in contrast to the $k=2$ case where the transition is known to be continuous. Finally, we consider the case of graphs presenting a mixture of edges and $3$-hyperedges, interpolating between the $k=2$ and the $k=3$ cases, and we study how the transition changes from continuous to first order by tuning the relative amount of edges and hyperedges.",https://arxiv.org/abs/2209.03427
Generative Adversarial Super-Resolution at the Edge with KnowledgeDistillation,"SimoneAngarano, FrancescoSalvetti, MauroMartini, MarcelloChiaberge",07-sep-22,Image and Video Processing (eess.IV)," Single-Image Super-Resolution can support robotic tasks in environments where a reliable visual stream is required to monitor the mission, handle teleoperation or study relevant visual details. In this work, we propose an efficient Generative Adversarial Network model for real- time Super-Resolution. We adopt a tailored architecture of the original SRGAN and model quantization to boost the execution on CPU and Edge TPU devices, achieving up to 200 fps inference. We further optimize our model by distilling its knowledge to a smaller version of the network and obtain remarkable improvements compared to the standard training approach. Our experiments show that our fast and lightweight model preserves considerably satisfying image quality compared to heavier state-of-the-art models. Finally, we conduct experiments on image transmission with bandwidth degradation to highlight the advantages of the proposed system for mobile robotic applications.",https://arxiv.org/abs/2209.03423
A Survey on Automated Diagnosis of Alzheimer's Disease Using OpticalCoherence Tomography and Angiography,"YaseminTurkan, F. BorayTek",07-sep-22,Image and Video Processing (eess.IV)," Retinal optical coherence tomography (OCT) and optical coherence tomography angiography (OCTA) are promising tools for the (early) diagnosis of Alzheimer's disease (AD). These non-invasive imaging techniques are cost- effective and more accessible than alternative neuroimaging tools. However, interpreting and classifying multi-slice scans produced by OCT devices is time-consuming and challenging even for trained practitioners.   There are surveys on machine learning and deep learning approaches concerning the automated analysis of OCT scans for various diseases such as glaucoma. However, the current literature lacks an extensive survey on the diagnosis of Alzheimer's disease or cognitive impairment using OCT or OCTA. This has motivated us to do a comprehensive survey aimed at machine/deep learning scientists or practitioners who require an introduction to the problem. The paper contains 1) an introduction to the medical background of Alzheimer's Disease and Cognitive Impairment and their diagnosis using OCT and OCTA imaging modalities, 2) a review of various technical proposals for the problem and the sub-problems from an automated analysis perspective, 3) a systematic review of the recent deep learning studies and available OCT/OCTA datasets directly aimed at the diagnosis of Alzheimer's Disease and Cognitive Impairment. For the latter, we used Publish or Perish Software to search for the relevant studies from various sources such as Scopus, PubMed, and Web of Science. We followed the PRISMA approach to screen an initial pool of 3073 references and determined ten relevant studies (N=10, out of 3073) that directly targeted AD diagnosis. We identified the lack of open OCT/OCTA datasets (about Alzheimer's disease) as the main issue that is impeding the progress in the field.",https://arxiv.org/abs/2209.03355
Learned Image Compression with Generalized Octave Convolution andCross-Resolution Parameter Estimation,"HaishengFu, FengLiang",07-sep-22,Image and Video Processing (eess.IV)," The application of the context-adaptive entropy model significantly improves the rate-distortion (R-D) performance, in which hyperpriors and autoregressive models are jointly utilized to effectively capture the spatial redundancy of the latent representations. However, the latent representations still contain some spatial correlations. In addition, these methods based on the context-adaptive entropy model cannot be accelerated in the decoding process by parallel computing devices, e.g. FPGA or GPU. To alleviate these limitations, we propose a learned multi- resolution image compression framework, which exploits the recently developed octave convolutions to factorize the latent representations into the high-resolution (HR) and low-resolution (LR) parts, similar to wavelet transform, which further improves the R-D performance. To speed up the decoding, our scheme does not use context-adaptive entropy model. Instead, we exploit an additional hyper layer including hyper encoder and hyper decoder to further remove the spatial redundancy of the latent representation. Moreover, the cross-resolution parameter estimation (CRPE) is introduced into the proposed framework to enhance the flow of information and further improve the rate-distortion performance. An additional information-fidelity loss is proposed to the total loss function to adjust the contribution of the LR part to the final bit stream. Experimental results show that our method separately reduces the decoding time by approximately 73.35 % and 93.44 % compared with that of state-of-the-art learned image compression methods, and the R-D performance is still better than H.266/VVC(4:2:0) and some learning-based methods on both PSNR and MS- SSIM metrics across a wide bit rates.",https://arxiv.org/abs/2209.03354
Data Leakage in Notebooks: Static Detection and Better Processes,"ChenyangYang, RachelA Brower-Sinning, Grace A.Lewis, ChristianKÃ¤stner",07-sep-22,Software Engineering (cs.SE)," Data science pipelines to train and evaluate models with machine learning may contain bugs just like any other code. Leakage between training and test data can lead to overestimating the model's accuracy during offline evaluations, possibly leading to deployment of low-quality models in production. Such leakage can happen easily by mistake or by following poor practices, but may be tedious and challenging to detect manually. We develop a static analysis approach to detect common forms of data leakage in data science code. Our evaluation shows that our analysis accurately detects data leakage and that such leakage is pervasive among over 100,000 analyzed public notebooks. We discuss how our static analysis approach can help both practitioners and educators, and how leakage prevention can be designed into the development process.",https://arxiv.org/abs/2209.03353
Large Scale Enrichment and Statistical Cyber Characterization ofNetwork Traffic,"IvanKawaminami, ArmindaEstrada, YoussefElsakkary, HaydenJananthan, AydÄ±nBuluÃ§, Tim Davis, DanielGrant, MichaelJones, ChadMeiners, AndrewMorris, SandeepPisharody, JeremyKepner",07-sep-22,Networking and Internet Architecture (cs.NI)," Modern network sensors continuously produce enormous quantities of raw data that are beyond the capacity of human analysts. Cross-correlation of network sensors increases this challenge by enriching every network event with additional metadata. These large volumes of enriched network data present opportunities to statistically characterize network traffic and quickly answer a key question: ""What are the primary cyber characteristics of my network data?"" The Python GraphBLAS and PyD4M analysis frameworks enable anonymized statistical analysis to be performed quickly and efficiently on very large network data sets. This approach is tested using billions of anonymized network data samples from the largest Internet observatory (CAIDA Telescope) and tens of millions of anonymized records from the largest commercially available background enrichment capability (GreyNoise). The analysis confirms that most of the enriched variables follow expected heavy-tail distributions and that a large fraction of the network traffic is due to a small number of cyber activities. This information can simplify the cyber analysts' task by enabling prioritization of cyber activities based on statistical prevalence.",https://arxiv.org/abs/2209.03345
ESSYS* Sharing #UC: An Emotion-driven Audiovisual Installation,"SÃ©rgio M.Rebelo, MarianaSeiÃ§a, PedroMartins, JoÃ£oBicker, PenousalMachado",07-sep-22,Multimedia (cs.MM)," We present ESSYS* Sharing #UC, an audiovisual installation artwork that reflects upon the emotional context related to the university and the city of Coimbra, based on the data shared about them on Twitter. The installation was presented in an urban art gallery of CÃ­rculo de Artes PlÃ¡sticas de Coimbra during the summer and autumn of 2021. In the installation space, one may see a collection of typographic posters displaying the tweets and listening to an ever-changing ambient sound. The present audiovisuals are created by an autonomous computational creative approach, which employs a neural classifier to recognize the emotional context of a tweet and uses this resulting data as feedstock for the audiovisual generation. The installation's space is designed to promote an approach and blend between the online and physical perceptions of the same location. We applied multiple experiments with the proposed approach to evaluate the capability and performance. Also, we conduct interview-based evaluation sessions to understand how the installation elements, especially poster designs, are experienced by people regarding diversity, expressiveness and possible employment in other commercial and social scenarios.",https://arxiv.org/abs/2209.03341
Detection and Mapping of Specular Surfaces Using Multibounce LidarReturns,"ConnorHenley, SiddharthSomasundaram, JosephHollmann, RameshRaskar",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We propose methods that use specular, multibounce lidar returns to detect and map specular surfaces that might be invisible to conventional lidar systems that rely on direct, single-scatter returns. We derive expressions that relate the time- and angle-of-arrival of these multibounce returns to scattering points on the specular surface, and then use these expressions to formulate techniques for retrieving specular surface geometry when the scene is scanned by a single beam or illuminated with a multi-beam flash. We also consider the special case of transparent specular surfaces, for which surface reflections can be mixed together with light that scatters off of objects lying behind the surface.",https://arxiv.org/abs/2209.03338
Joint Learning of Deep Texture and High-Frequency Features forComputer-Generated Image Detection,"QiangXu, ShanJia, XinghaoJiang, Tanfeng Sun, Zhe Wang, Hong Yan",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Distinguishing between computer-generated (CG) and natural photographic (PG) images is of great importance to verify the authenticity and originality of digital images. However, the recent cutting-edge generation methods enable high qualities of synthesis in CG images, which makes this challenging task even trickier. To address this issue, a joint learning strategy with deep texture and high-frequency features for CG image detection is proposed. We first formulate and deeply analyze the different acquisition processes of CG and PG images. Based on the finding that multiple different modules in image acquisition will lead to different sensitivity inconsistencies to the convolutional neural network (CNN)-based rendering in images, we propose a deep texture rendering module for texture difference enhancement and discriminative texture representation. Specifically, the semantic segmentation map is generated to guide the affine transformation operation, which is used to recover the texture in different regions of the input image. Then, the combination of the original image and the high-frequency components of the original and rendered images are fed into a multi-branch neural network equipped with attention mechanisms, which refines intermediate features and facilitates trace exploration in spatial and channel dimensions respectively. Extensive experiments on two public datasets and a newly constructed dataset with more realistic and diverse images show that the proposed approach outperforms existing methods in the field by a clear margin. Besides, results also demonstrate the detection robustness and generalization ability of the proposed approach to postprocessing operations and generative adversarial network (GAN) generated images.",https://arxiv.org/abs/2209.03336
What does a platypus look like? Generating customized prompts forzero-shot image classification,"SarahPratt, Rosanne Liu, AliFarhadi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Open vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called ""prompts"", typically consists of a set of hand-written templates (e.g., ""a photo of a {}"") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without using explicit knowledge of the image domain and with far fewer hand- constructed sentences. To achieve this, we combine open vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced ""couple""). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that are customized for each object category. We find that this straightforward and general approach improves accuracy on a range of zero-shot image classification benchmarks, including over one percentage point gain on ImageNet. Finally, this method requires no additional training and remains completely zero-shot. Code is available at [this https URL](https://github.com/sarahpratt/CuPL).",https://arxiv.org/abs/2209.03322
Measurement of the Usage of Web Clips in Underground Economy,"QinyuHu, SongyangWu, WenqiSun, ZhushouTang, ChaofanChen, ZhiguoDing, XiaomeiZhang",07-sep-22,Cryptography and Security (cs.CR)," In this paper, we study the ecosystem of the abused Web Clips in underground economy. Through this study, we find the Web Clips is wildly used by perpetrators to penetrate iOS devices to gain profit. This work starts with 1,800 user complaint documents about cyber crimes over Web Clips. We firstly look into the ecosystem of abused Web Clips and point out the main participants and workflow. In addition, what is the Web Clips used for is demystified. Then the main participants, including creators, distributors, and operators are deeply studied based on our dataset. We try to reveal the prominent features of the illicit Web Clips and give some mitigation measures.   Analysis reveals that 1) SSL certificate is overwhelmingly preferred for signing Web Clips instances compared with certificate issued by Apple. The wildly used SSL certificates can be aggregated into a limited group. 2) The content of the abused Web Clips falls into a few categories, `Gambling', `Fraud', and `Pornography' are among the top categories. 3) Instant messenger (IM) and live streaming platform are the most popular medium to trick victims into deploying the Web Clips. 4) The Web Clips are operated by a small amount of perpetrators, and the perpetrators tend to evade detection by taking technical approach, such as registering domain names through oversea domain name service provider, preferring easy-to-acquire new gTLD (global Top Level Domain), and deploying anti-crawler tricks.   Our study gives hints on investigation of cyber crime over Web Clips, we hope that this work can help stakeholders to stay ahead of the threat.",https://arxiv.org/abs/2209.03320
On the Complementarity between Pre-Training and Random-Initializationfor Resource-Rich Machine Translation,"ChangtongZan, LiangDing, LiShen, YuCao, WeifengLiu, DachengTao",07-sep-22,Computation and Language (cs.CL)," Pre-Training (PT) of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains (some- times, even worse) on resource-rich NMT on par with its Random-Initialization (RI) counterpart. We take the first step to investigate the complementarity between PT and RI in resource-rich scenarios via two probing analyses, and find that: 1) PT improves NOT the accuracy, but the generalization by achieving flatter loss landscapes than that of RI; 2) PT improves NOT the confidence of lexical choice, but the negative diversity by assigning smoother lexical probability distributions than that of RI. Based on these insights, we propose to combine their complementarities with a model fusion algorithm that utilizes optimal transport to align neurons between PT and RI. Experiments on two resource-rich translation benchmarks, WMT'17 English-Chinese (20M) and WMT'19 English-German (36M), show that PT and RI could be nicely complementary to each other, achieving substantial improvements considering both translation accuracy, generalization, and negative diversity. Probing tools and code are released at: [this https URL](https://github.com/zanchangtong/PTvsRI).",https://arxiv.org/abs/2209.03319
Riemannian optimization for non-centered mixture of scaled Gaussiandistributions,"AntoineCollas, ArnaudBreloy, Chengfang Ren, GuillaumeGinolhac, Jean-PhilippeOvarlez",07-sep-22,Machine Learning (cs.LG)," This paper studies the statistical model of the non-centered mixture of scaled Gaussian distributions (NC-MSG). Using the Fisher-Rao information geometry associated to this distribution, we derive a Riemannian gradient descent algorithm. This algorithm is leveraged for two minimization problems. The first one is the minimization of a regularized negative log- likelihood (NLL). The latter makes the trade-off between a white Gaussian distribution and the NC-MSG. Conditions on the regularization are given so that the existence of a minimum to this problem is guaranteed without assumptions on the samples. Then, the Kullback-Leibler (KL) divergence between two NC-MSG is derived. This divergence enables us to define a minimization problem to compute centers of mass of several NC-MSGs. The proposed Riemannian gradient descent algorithm is leveraged to solve this second minimization problem. Numerical experiments show the good performance and the speed of the Riemannian gradient descent on the two problems. Finally, a Nearest centroid classifier is implemented leveraging the KL divergence and its associated center of mass. Applied on the large scale dataset Breizhcrops, this classifier shows good accuracies as well as robustness to rigid transformations of the test set.",https://arxiv.org/abs/2209.03316
SZZ in the time of Pull Requests,"FernandoPetrulio, DavidAckermann, EnricoFregnan, GÃ¼lCalikli, MarcoCastelluccio, SylvestreLedru, CalixteDenizet, EmmaHumphries, AlbertoBacchelli",07-sep-22,Software Engineering (cs.SE)," In the multi-commit development model, programmers complete tasks (e.g., implementing a feature) by organizing their work in several commits and packaging them into a commit-set. Analyzing data from developers using this model can be useful to tackle challenging developers' needs, such as knowing which features introduce a bug as well as assessing the risk of integrating certain features in a release. However, to do so one first needs to identify fix-inducing commit-sets. For such an identification, the SZZ algorithm is the most natural candidate, but its performance has not been evaluated in the multi-commit context yet. In this study, we conduct an in- depth investigation on the reliability and performance of SZZ in the multi- commit model. To obtain a reliable ground truth, we consider an already existing SZZ dataset and adapt it to the multi-commit context. Moreover, we devise a second dataset that is more extensive and directly created by developers as well as Quality Assurance (QA) engineers of Mozilla. Based on these datasets, we (1) test the performance of B-SZZ and its non-language- specific SZZ variations in the context of the multi-commit model, (2) investigate the reasons behind their specific behavior, and (3) analyze the impact of non-relevant commits in a commit-set and automatically detect them before using SZZ.",https://arxiv.org/abs/2209.03315
Bayesian and Frequentist Semantics for Common Variations ofDifferential Privacy: Applications to the 2020 Census,"DanielKifer, JohnM. Abowd, RobertAshmead, Ryan Cumings-Menon, PhilipLeclerc, AshwinMachanavajjhala, WilliamSexton, PavelZhuravlev",07-sep-22,Cryptography and Security (cs.CR)," The purpose of this paper is to guide interpretation of the semantic privacy guarantees for some of the major variations of differential privacy, which include pure, approximate, RÃ©nyi, zero-concentrated, and $f$ differential privacy. We interpret privacy-loss accounting parameters, frequentist semantics, and Bayesian semantics (including new results). The driving application is the interpretation of the confidentiality protections for the 2020 Census Public Law 94-171 Redistricting Data Summary File released August 12, 2021, which, for the first time, were produced with formal privacy guarantees.",https://arxiv.org/abs/2209.03311
Accurate Cooperative Sensor Fusion by Parameterized CovarianceGeneration for Sensing and Localization Pipelines in CAVs,"EdwardAndert, AviralShrivastava",07-sep-22,Robotics (cs.RO)," A major challenge in cooperative sensing is to weight the measurements taken from the various sources to get an accurate result. Ideally, the weights should be inversely proportional to the error in the sensing information. However, previous cooperative sensor fusion approaches for autonomous vehicles use a fixed error model, in which the covariance of a sensor and its recognizer pipeline is just the mean of the measured covariance for all sensing scenarios. The approach proposed in this paper estimates error using key predictor terms that have high correlation with sensing and localization accuracy for accurate covariance estimation of each sensor observation. We adopt a tiered fusion model consisting of local and global sensor fusion steps. At the local fusion level, we add in a covariance generation stage using the error model for each sensor and the measured distance to generate the expected covariance matrix for each observation. At the global sensor fusion stage we add an additional stage to generate the localization covariance matrix from the key predictor term velocity and combines that with the covariance generated from the local fusion for accurate cooperative sensing. To showcase our method, we built a set of 1/10 scale model autonomous vehicles with scale accurate sensing capabilities and classified the error characteristics against a motion capture system. Results show an average and max improvement in RMSE when detecting vehicle positions of 1.42x and 1.78x respectively in a four- vehicle cooperative fusion scenario when using our error model versus a typical fixed error model.",https://arxiv.org/abs/2209.03310
Picking Up Speed: Continuous-Time Lidar-Only Odometry using DopplerVelocity Measurements,"YuchenWu, David J.Yoon, KeenanBurnett, SoerenKammel, YiChen, HeetheshVhavle, Timothy D.Barfoot",07-sep-22,Robotics (cs.RO)," Frequency-Modulated Continuous-Wave (FMCW) lidar is a recently emerging technology that additionally enables per-return instantaneous relative radial velocity measurements via the Doppler effect. In this letter, we present the first continuous-time lidar-only odometry algorithm using these Doppler velocity measurements from an FMCW lidar to aid odometry in geometrically degenerate environments. We apply an existing continuous- time framework that efficiently estimates the vehicle trajectory using Gaussian process regression to compensate for motion distortion due to the scanning-while-moving nature of any mechanically actuated lidar (FMCW and non-FMCW). We evaluate our proposed algorithm on several real-world datasets, including publicly available ones and datasets we collected. Our algorithm outperforms the only existing method that also uses Doppler velocity measurements, and we study difficult conditions where including this extra information greatly improves performance. We additionally demonstrate state-of-the-art performance of lidar-only odometry with and without using Doppler velocity measurements in nominal conditions. Code for this project can be found at: [this https URL](https://github.com/utiasASRL/steam_icp).",https://arxiv.org/abs/2209.03306
Quantifying Aleatoric and Epistemic Uncertainty in Machine Learning:Are Conditional Entropy and Mutual Information Appropriate Measures?,EykeHÃ¼llermeier,07-sep-22,Machine Learning (cs.LG)," This short note is a critical discussion of the quantification of aleatoric and epistemic uncertainty in terms of conditional entropy and mutual information, respectively, which has recently been proposed in machine learning and has become quite common since then. More generally, we question the idea of an additive decomposition of total uncertainty into its aleatoric and epistemic constituents.",https://arxiv.org/abs/2209.03304
Geometric multimodal representation learning,"YashaEktefaie, GeorgeDasoulas, Ayush Noori, MahaFarhat, MarinkaZitnik",07-sep-22,Machine Learning (cs.LG)," Graph-centric artificial intelligence (graph AI) has achieved remarkable success in modeling interacting systems prevalent in nature, from dynamical systems in biology to particle physics. The increasing heterogeneity of data calls for graph neural architectures that can combine multiple inductive biases. However, combining data from various sources is challenging because appropriate inductive bias may vary by data modality. Multimodal learning methods fuse multiple data modalities while leveraging cross-modal dependencies to address this challenge. Here, we survey 140 studies in graph-centric AI and realize that diverse data types are increasingly brought together using graphs and fed into sophisticated multimodal models. These models stratify into image-, language-, and knowledge-grounded multimodal learning. We put forward an algorithmic blueprint for multimodal graph learning based on this categorization. The blueprint serves as a way to group state-of-the-art architectures that treat multimodal data by choosing appropriately four different components. This effort can pave the way for standardizing the design of sophisticated multimodal architectures for highly complex real-world problems.",https://arxiv.org/abs/2209.03302
Trading Strategies: Earning More in Investment,"YueyingMa, YanMi, YujingBian",07-sep-22,Other Computer Science (cs.OH)," Gold and bitcoin are not new to us, but with limited cash and time, given only the past stream of the daily price of gold and bitcoin, it is a kind of new problem for us to develop a certain model and determine the best strategy to get the most return. Here, our team members analyzed the data provided and finally made a unified system of models to predict the price and evaluate the risk and return in our act of investment, and we name this series of models and measurements as CTP Model. This is a model which can determine and describe what transaction should the trader make each day and what is the certain maximum return he will get under different risk levels.",https://arxiv.org/abs/2209.03299
Fairly Allocating (Contiguous) Dynamic Indivisible Items with FewAdjustments,MingweiYang,07-sep-22,Computer Science and Game Theory (cs.GT)," We study the problem of dynamically allocating indivisible items to a group of agents in a fair manner. We assume that the items are goods and the valuation functions are additive without specification. Due to the negative results to achieve fairness, we allow adjustments to make fairness attainable with the objective to minimize the number of adjustments. We obtain positive results to achieve EF1 for the default setting, restricted additive or general identical valuations, and two agents with mixed manna. We further impose the contiguity constraint on the items and require that each agent obtains a consecutive block of items. We obtain both positive and negative results to achieve either EF1 or proportionality with an additive approximate factor. In particular, we establish matching lower and upper bounds to achieve approximate proportionality for identical valuations. Our results exhibit the large discrepancy between the identical model and nonidentical model in both contiguous and noncontiguous settings. All our positive results are computationally efficient.",https://arxiv.org/abs/2209.03294
K-VIL: Keypoints-based Visual Imitation Learning,"JianfengGao, ZhiTao, NoÃ©mieJaquier, TamimAsfour",07-sep-22,Robotics (cs.RO)," Visual imitation learning provides efficient and intuitive solutions for robotic systems to acquire novel manipulation skills. However, simultaneously learning geometric task constraints and control policies from visual inputs alone remains a challenging problem. In this paper, we propose an approach for keypoint-based visual imitation (K-VIL) that automatically extracts sparse, object-centric, and embodiment-independent task representations from a small number of human demonstration videos. The task representation is composed of keypoint-based geometric constraints on principal manifolds, their associated local frames, and the movement primitives that are then needed for the task execution. Our approach is capable of extracting such task representations from a single demonstration video, and of incrementally updating them when new demonstrations become available. To reproduce manipulation skills using the learned set of prioritized geometric constraints in novel scenes, we introduce a novel keypoint-based admittance controller. We evaluate our approach in several real-world applications, showcasing its ability to deal with cluttered scenes, new instances of categorical objects, and large object pose and shape variations, as well as its efficiency and robustness in both one-shot and few-shot imitation learning settings. Videos and source code are available at [this https URL](https://sites.google.com/view/k-vil).",https://arxiv.org/abs/2209.03286
Inverse modeling of nonisothermal multiphase poromechanics usingphysics-informed neural networks,"DanialAmini, EhsanHaghighat, RubenJuanes",07-sep-22,Machine Learning (cs.LG)," We propose a solution strategy for parameter identification in multiphase thermo-hydro-mechanical (THM) processes in porous media using physics-informed neural networks (PINNs). We employ a dimensionless form of the THM governing equations that is particularly well suited for the inverse problem, and we leverage the sequential multiphysics PINN solver we developed in previous work. We validate the proposed inverse-modeling approach on multiple benchmark problems, including Terzaghi's isothermal consolidation problem, Barry-Mercer's isothermal injection-production problem, and nonisothermal consolidation of an unsaturated soil layer. We report the excellent performance of the proposed sequential PINN-THM inverse solver, thus paving the way for the application of PINNs to inverse modeling of complex nonlinear multiphysics problems.",https://arxiv.org/abs/2209.03277
Multimodal Speech Enhancement Using Burst Propagation,"Leandro A.Passos, AhmedKhubaib, Mohsin Raza, Ahsan Adeel",07-sep-22,Sound (cs.SD)," This paper proposes the MBURST, a novel multimodal solution for audio-visual speech enhancements that consider the most recent neurological discoveries regarding pyramidal cells of the prefrontal cortex and other brain regions. The so-called burst propagation implements several criteria to address the credit assignment problem in a more biologically plausible manner: steering the sign and magnitude of plasticity through feedback, multiplexing the feedback and feedforward information across layers through different weight connections, approximating feedback and feedforward connections, and linearizing the feedback signals. MBURST benefits from such capabilities to learn correlations between the noisy signal and the visual stimuli, thus attributing meaning to the speech by amplifying relevant information and suppressing noise. Experiments conducted over a Grid Corpus and CHiME3-based dataset show that MBURST can reproduce similar mask reconstructions to the multimodal backpropagation-based baseline while demonstrating outstanding energy efficiency management, reducing the neuron firing rates to values up to \textbf{$70\%$} lower. Such a feature implies more sustainable implementations, suitable and desirable for hearing aids or any other similar embedded systems.",https://arxiv.org/abs/2209.03276
Measuring the Interpretability of Unsupervised Representations viaQuantized Reverse Probing,"IroLaina, YukiM. Asano, AndreaVedaldi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i.e. understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters in representation space. This approach, which we call reverse linear probing, provides a single number sensitive to the semanticity of the representation. This measure is also able to detect when the representation contains combinations of concepts (e.g., ""red apple"") instead of just individual attributes (""red"" and ""apple"" independently). Finally, we propose to use supervised classifiers to automatically label large datasets in order to enrich the space of concepts used for probing. We use our method to evaluate a large number of self-supervised representations, ranking them by interpretability, highlight the differences that emerge compared to the standard evaluation with linear probes and discuss several qualitative insights. Code at: {\scriptsize{\url{[this https URL](https://github.com/iro-cp/ssl-qrp)}}}.",https://arxiv.org/abs/2209.03275
The First-Order Theory of Binary Overlap-Free Words is Decidable,"L.Schaeffer, J. Shallit",07-sep-22,Formal Languages and Automata Theory (cs.FL)," We show that the first-order logical theory of the binary overlap- free words (and, more generally, the ${\alpha}$-free words for rational ${\alpha}$, $2 < {\alpha} \leq 7/3$), is decidable. As a consequence, many results previously obtained about this class through tedious case- based proofs can now be proved ""automatically"", using a decision procedure.",https://arxiv.org/abs/2209.03268
Cooperative trajectory planning algorithm of USV-UAV with hull dynamicconstraints,"TaoHuang, ZheChen, Zhenfeng Xue, Zhuo Chen, Yong Liu",07-sep-22,Robotics (cs.RO)," Efficient trajectory generation in complex dynamic environment stills remains an open problem in the unmanned surface vehicle (USV) domain. In this paper, a cooperative trajectory planning algorithm for the coupled USV-UAV system is proposed, to ensure that USV can execute safe and smooth path in the process of autonomous advance in multi obstacle maps. Specifically, the unmanned aerial vehicle (UAV) plays the role as a flight sensor, and it provides real-time global map and obstacle information with lightweight semantic segmentation network and 3D projection transformation. And then an initial obstacle avoidance trajectory is generated by a graph- based search method. Concerning the unique under-actuated kinematic characteristics of the USV, a numerical optimization method based on hull dynamic constraints is introduced to make the trajectory easier to be tracked for motion control. Finally, a motion control method based on NMPC with the lowest energy consumption constraint during execution is proposed. Experimental results verify the effectiveness of whole system, and the generated trajectory is locally optimal for USV with considerable tracking accuracy.",https://arxiv.org/abs/2209.03266
VulCurator: A Vulnerability-Fixing Commit Detector,"Truong GiangNguyen, Thanh Le-Cong, Hong JinKang, Xuan-Bach D. Le, David Lo",07-sep-22,Cryptography and Security (cs.CR)," Open-source software (OSS) vulnerability management process is important nowadays, as the number of discovered OSS vulnerabilities is increasing over time. Monitoring vulnerability-fixing commits is a part of the standard process to prevent vulnerability exploitation. Manually detecting vulnerability-fixing commits is, however, time consuming due to the possibly large number of commits to review. Recently, many techniques have been proposed to automatically detect vulnerability-fixing commits using machine learning. These solutions either: (1) did not use deep learning, or (2) use deep learning on only limited sources of information. This paper proposes VulCurator, a tool that leverages deep learning on richer sources of information, including commit messages, code changes and issue reports for vulnerability-fixing commit classifica- tion. Our experimental results show that VulCurator outperforms the state-of-the-art baselines up to 16.1% in terms of F1-score. VulCurator tool is publicly available at [this https URL](https://github.com/ntgiang71096/VFDetector) and [this https URL](https://zenodo.org/record/7034132#.Yw3MN-xBzDI), with a demo video at [this https URL](https://youtu.be/uMlFmWSJYOE).",https://arxiv.org/abs/2209.03261
A Test for FLOPs as a Discriminant for Linear Algebra Algorithms,"AravindSankaran, PaoloBientinesi",07-sep-22,Performance (cs.PF)," Linear algebra expressions, which play a central role in countless scientific computations, are often computed via a sequence of calls to existing libraries of building blocks (such as those provided by BLAS and LAPACK). A sequence identifies a computing strategy, i.e., an algorithm, and normally for one linear algebra expression many alternative algorithms exist. Although mathematically equivalent, those algorithms might exhibit significant differences in terms of performance. Several high-level languages and tools for matrix computations such as Julia, Armadillo, Linnea, etc., make algorithmic choices by minimizing the number of Floating Point Operations (FLOPs). However, there can be several algorithms that share the same (or have nearly identical) number of FLOPs; in many cases, these algorithms exhibit execution times which are statistically equivalent and one could arbitrarily select one of them as the best algorithm. It is however not unlikely to find cases where the execution times are significantly different from one another (despite the FLOP count being almost the same). It is also possible that the algorithm that minimizes FLOPs is not the one that minimizes execution time. In this work, we develop a methodology to test the reliability of FLOPs as discriminant for linear algebra algorithms. Given a set of algorithms (for an instance of a linear algebra expression) as input, the methodology ranks them into performance classes; algorithms in the same class are statistically equivalent in performance. To this end, we measure the algorithms iteratively until the changes in the ranks converge to a value close to zero. FLOPs are a valid discriminant for an instance if all the algorithms with minimum FLOPs are assigned the best rank; otherwise, the instance is regarded as an anomaly, which can then be used in the investigation of the root cause of performance differences.",https://arxiv.org/abs/2209.03260
No More Attacks on Proof-of-Stake Ethereum?,"FrancescoD'Amato, Joachim Neu, Ertem NusretTas, DavidTse",07-sep-22,Cryptography and Security (cs.CR)," The latest message driven (LMD) greedy heaviest observed sub-tree (GHOST) consensus protocol is a critical component of future proof-of-stake (PoS) Ethereum. In its current form, the protocol is brittle and intricate to reason about, as evidenced by recent attacks, patching attempts, and GÃ¶rli testnet reorgs. We present Goldfish, which can be seen as a considerably simplified variant of the current protocol, and prove that it is secure and reorg resilient in synchronous networks with dynamic participation, assuming a majority of the nodes (called validators) follows the protocol honestly. Furthermore, we show that subsampling validators can improve the communication efficiency of Goldfish, and that Goldfish is composable with finality gadgets and accountability gadgets. The aforementioned properties make Goldfish a credible candidate for a future protocol upgrade of PoS Ethereum, as well as a versatile pedagogical example. Akin to traditional propose-and-vote-style consensus protocols, Goldfish is organized into slots, at the beginning of which a leader proposes a block containing new transactions, and subsequently members of a committee take a vote towards block confirmation. But instead of using quorums, Goldfish is powered by a new mechanism that carefully synchronizes the inclusion and exclusion of votes in honest validators' views.",https://arxiv.org/abs/2209.03258
3D Textured Shape Recovery with Learned Geometric Priors,"Lei Li, Zhizheng Liu, Weining Ren, Liudi Yang, FangjinhuaWang, MarcPollefeys, Songyou Peng",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," 3D textured shape recovery from partial scans is crucial for many real-world applications. Existing approaches have demonstrated the efficacy of implicit function representation, but they suffer from partial inputs with severe occlusions and varying object types, which greatly hinders their application value in the real world. This technical report presents our approach to address these limitations by incorporating learned geometric priors. To this end, we generate a SMPL model from learned pose prediction and fuse it into the partial input to add prior knowledge of human bodies. We also propose a novel completeness-aware bounding box adaptation for handling different levels of scales and partialness of partial scans.",https://arxiv.org/abs/2209.03255
Explicit Low-Bandwidth Evaluation Schemes for Weighted Sums of Reed-Solomon-Coded Symbols,"Han MaoKiah, Wilton Kim, StanislavKruglik, San Ling, Huaxiong Wang",07-sep-22,Information Theory (cs.IT)," Motivated by applications in distributed storage, distributed computing, and homomorphic secret sharing, we study communication-efficient schemes for computing linear combinations of coded symbols. Specifically, we design low-bandwidth schemes that evaluate the weighted sum of $\ell$ coded symbols in a codeword $\pmb{c}\in\mathbb{F}^n$, when we are given access to $d$ of the remaining components in $\pmb{c}$.   Formally, suppose that $\mathbb{F}$ is a field extension of $\mathbb{B}$ of degree $t$. Let $\pmb{c}$ be a codeword in a Reed-Solomon code of dimension $k$ and our task is to compute the weighted sum of $\ell$ coded symbols. In this paper, for some $s<t$, we provide an explicit scheme that performs this task by downloading $d(t-s)$ sub-symbols in $\mathbb{B}$ from $d$ available nodes, whenever $d\geq \ell|\mathbb{B}|^s-\ell+k$. In many cases, our scheme outperforms previous schemes in the literature.   Furthermore, we provide a characterization of evaluation schemes for general linear codes. Then in the special case of Reed-Solomon codes, we use this characterization to derive a lower bound for the evaluation bandwidth.",https://arxiv.org/abs/2209.03254
Adaptive Passivity-Based Pose Tracking Control of Cable-DrivenParallel Robots for Multiple Attitude Parameterizations,"Sze KwanCheah, Alex Hayes, Ryan J.Caverly",07-sep-22,Robotics (cs.RO)," The proposed control method uses an adaptive feedforward-based controller to establish a passive input-output mapping for the CDPR that is used alongside a linear time-invariant strictly positive real feedback controller to guarantee robust closed-loop input-output stability and asymptotic pose trajectory tracking via the passivity theorem. A novelty of the proposed controller is its formulation for use with a range of payload attitude parameterizations, including any unconstrained attitude parameterization, the quaternion, or the direction cosine matrix (DCM). The performance and robustness of the proposed controller is demonstrated through numerical simulations of a CDPR with rigid and flexible cables. The results demonstrate the importance of carefully defining the CDPR's pose error, which is performed in multiplicative fashion when using the quaternion and DCM, and in a specific additive fashion when using unconstrained attitude parameters (e.g., an Euler-angle sequence).",https://arxiv.org/abs/2209.03251
Sparse Identification of Lagrangian for Nonlinear Dynamical Systemsvia Proximal Gradient Method,"AdamPurnomo, MitsuhiroHayashibe",04-sep-22,Systems and Control (eess.SY)," Distilling physical laws autonomously from data has been of great interest in many scientific areas. The sparse identification of nonlinear dynamics (SINDy) and its variations have been developed to extract the underlying governing equations from observation data. However, SINDy faces certain difficulties when the dynamics contain rational functions. The principle of the least action governs many mechanical systems, mathematically expressed in the Lagrangian formula. Compared to the actual equation of motions, the Lagrangian is much more concise, especially for complex systems, and does not usually contain rational functions for mechanical systems. Only a few methods have been proposed to extract the Lagrangian from measurement data so far. One of such methods, Lagrangian- SINDy, can extract the true form of Lagrangian of dynamical systems from data but suffers when noises are present. In this work, we develop an extended version of Lagrangian-SINDy (xL-SINDy) to obtain the Lagrangian of dynamical systems from noisy measurement data. We incorporate the concept of SINDy and utilize the proximal gradient method to obtain sparse expressions of the Lagrangian. We demonstrated the effectiveness of xL-SINDy against different noise levels with four nonlinear dynamics: a single pendulum, a cart-pendulum, a double pendulum, and a spherical pendulum. Furthermore, we also verified the performance of xL-SINDy against SINDy-PI (parallel, implicit), a recent robust variant of SINDy that can handle implicit dynamics and rational nonlinearities. Our experiment results show that xL- SINDy is 8-20 times more robust than SINDy-PI in the presence of noise.",https://arxiv.org/abs/2209.03250
The HoloLens in Medicine: A systematic Review and Taxonomy,"ChristinaGsaxner, Jianning Li, Antonio Pepe, Yuan Jin, JensKleesiek, DieterSchmalstieg, Jan Egger",06-sep-22,Human-Computer Interaction (cs.HC)," The HoloLens (Microsoft Corp., Redmond, WA), a head-worn, optically see-through augmented reality display, is the main player in the recent boost in medical augmented reality research. In medical settings, the HoloLens enables the physician to obtain immediate insight into patient information, directly overlaid with their view of the clinical scenario, the medical student to gain a better understanding of complex anatomies or procedures, and even the patient to execute therapeutic tasks with improved, immersive guidance. In this systematic review, we provide a comprehensive overview of the usage of the first-generation HoloLens within the medical domain, from its release in March 2016, until the year of 2021, were attention is shifting towards it's successor, the HoloLens 2. We identified 171 relevant publications through a systematic search of the PubMed and Scopus databases. We analyze these publications in regard to their intended use case, technical methodology for registration and tracking, data sources, visualization as well as validation and evaluation. We find that, although the feasibility of using the HoloLens in various medical scenarios has been shown, increased efforts in the areas of precision, reliability, usability, workflow and perception are necessary to establish AR in clinical practice.",https://arxiv.org/abs/2209.03248
Banknote Recognition for Visually Impaired People (Case of Ethiopiannote),Nuredin AliAbdelkadir,25 Aug 2022,Human-Computer Interaction (cs.HC)," Currency is used almost everywhere to facilitate business. In most developing countries, especially the ones in Africa, tangible notes are predominantly used in everyday financial transactions. One of these countries, Ethiopia, is believed to have one of the world highest rates of blindness (1.6%) and low vision (3.7%). There are around 4 million visually impaired people; With 1.7 million people being in complete vision loss. Those people face a number of challenges when they are in a bus station, in shopping centers, or anywhere which requires the physical exchange of money. In this paper, we try to provide a solution to this issue using AI/ML applications. We developed an Android and IOS compatible mobile application with a model that achieved 98.9% classification accuracy on our dataset. The application has a voice integrated feature that tells the type of the scanned currency in Amharic, the working language of Ethiopia. The application is developed to be easily accessible by its users. It is build to reduce the burden of visually impaired people in Ethiopia.",https://arxiv.org/abs/2209.03245
Efficient Trajectory Planning and Control for USV with Vessel Dynamicsand Differential Flatness,"TaoHuang, Zhenfeng Xue, Zhe Chen, Yong Liu",07-sep-22,Robotics (cs.RO)," Unmanned surface vessels (USVs) are widely used in ocean exploration and environmental protection fields. To ensure that USV can successfully perform its mission, trajectory planning and motion tracking are the two most critical technologies. In this paper, we propose a novel trajectory generation and tracking method for USV based on optimization theory. Specifically, the USV dynamic model is described with differential flatness, so that the trajectory can be generated by dynamic RRT* in a linear invariant system expression form under the objective of optimal boundary value. To reduce the sample number and improve efficiency, we adjust the trajectory through local optimization. The dynamic constraints are considered in the optimization process so that the generated trajectory conforms to the kinematic characteristics of the under-actuated hull, and makes it easier to be tracked. Finally, motion tracking is added with model predictive control under a sequential quadratic programming problem. Experimental results show the planned trajectory is more in line with the kinematic characteristics of USV, and the tracking accuracy remains a higher level.",https://arxiv.org/abs/2209.03236
AutoPruner: Transformer-Based Call Graph Pruning,"Thanh Le-Cong, HongJin Kang, Truong GiangNguyen, Stefanus AgusHaryono, David Lo, Xuan-Bach D.Le, HuynhQuyet Thang",07-sep-22,Software Engineering (cs.SE)," Constructing a static call graph requires trade-offs between soundness and precision. Program analysis techniques for constructing call graphs are unfortunately usually imprecise. To address this problem, researchers have recently proposed call graph pruning empowered by machine learning to post-process call graphs constructed by static analysis. A machine learning model is built to capture information from the call graph by extracting structural features for use in a random forest classifier. It then removes edges that are predicted to be false positives. Despite the improvements shown by machine learning models, they are still limited as they do not consider the source code semantics and thus often are not able to effectively distinguish true and false positives. In this paper, we present a novel call graph pruning technique, AutoPruner, for eliminating false positives in call graphs via both statistical semantic and structural analysis. Given a call graph constructed by traditional static analysis tools, AutoPruner takes a Transformer-based approach to capture the semantic relationships between the caller and callee functions associated with each edge in the call graph. To do so, AutoPruner fine-tunes a model of code that was pre-trained on a large corpus to represent source code based on descriptions of its semantics. Next, the model is used to extract semantic features from the functions related to each edge in the call graph. AutoPruner uses these semantic features together with the structural features extracted from the call graph to classify each edge via a feed- forward neural network. Our empirical evaluation on a benchmark dataset of real-world programs shows that AutoPruner outperforms the state-of-the-art baselines, improving on F-measure by up to 13% in identifying false-positive edges in a static call graph.",https://arxiv.org/abs/2209.03232
On the Importance of Quantifying Visibility for Autonomous Vehiclesunder Extreme Precipitation,"ClÃ©mentCourcelle, DominicBaril, FranÃ§oisPomerleau, JohannLaconte",07-sep-22,Robotics (cs.RO)," In the context of autonomous driving, vehicles are inherently bound to encounter more extreme weather during which public safety must be ensured. As climate is quickly changing, the frequency of heavy snowstorms is expected to increase and become a major threat to safe navigation. While there is much literature aiming to improve navigation resiliency to winter conditions, there is a lack of standard metrics to quantify the loss of visibility of lidar sensors related to precipitation. This chapter proposes a novel metric to quantify the lidar visibility loss in real time, relying on the notion of visibility from the meteorology research field. We evaluate this metric on the Canadian Adverse Driving Conditions (CADC) dataset, correlate it with the performance of a state-of-the-art lidar-based localization algorithm, and evaluate the benefit of filtering point clouds before the localization process. We show that the Iterative Closest Point (ICP) algorithm is surprisingly robust against snowfalls, but abrupt events, such as snow gusts, can greatly hinder its accuracy. We discuss such events and demonstrate the need for better datasets focusing on these extreme events to quantify their effect.",https://arxiv.org/abs/2209.03230
Hardware faults that matter: Understanding and Estimating the safetyimpact of hardware faults on object detection DNNs,"SyedQutub, FlorianGeissler, Yang Peng, Ralf Grafe, MichaelPaulitsch, Gereon Hinz, Alois Knoll",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Object detection neural network models need to perform reliably in highly dynamic and safety-critical environments like automated driving or robotics. Therefore, it is paramount to verify the robustness of the detection under unexpected hardware faults like soft errors that can impact a systems perception module. Standard metrics based on average precision produce model vulnerability estimates at the object level rather than at an image level. As we show in this paper, this does not provide an intuitive or representative indicator of the safety-related impact of silent data corruption caused by bit flips in the underlying memory but can lead to an over- or underestimation of typical fault-induced hazards. With an eye towards safety-related real-time applications, we propose a new metric IVMOD (Image-wise Vulnerability Metric for Object Detection) to quantify vulnerability based on an incorrect image-wise object detection due to false positive (FPs) or false negative (FNs) objects, combined with a severity analysis. The evaluation of several representative object detection models shows that even a single bit flip can lead to a severe silent data corruption event with potentially critical safety implications, with e.g., up to (much greater than) 100 FPs generated, or up to approx. 90% of true positives (TPs) are lost in an image. Furthermore, with a single stuck-at-1 fault, an entire sequence of images can be affected, causing temporally persistent ghost detections that can be mistaken for actual objects (covering up to approx. 83% of the image). Furthermore, actual objects in the scene are continuously missed (up to approx. 64% of TPs are lost). Our work establishes a detailed understanding of the safety-related vulnerability of such critical workloads against hardware faults.",https://arxiv.org/abs/2209.03226
Dual Instrumental Method for Confounded Kernelized Bandits,"XuepingGong, JihengZhang",07-sep-22,Machine Learning (cs.LG)," The contextual bandit problem is a theoretically justified framework with wide applications in various fields. While the previous study on this problem usually requires independence between noise and contexts, our work considers a more sensible setting where the noise becomes a latent confounder that affects both contexts and rewards. Such a confounded setting is more realistic and could expand to a broader range of applications. However, the unresolved confounder will cause a bias in reward function estimation and thus lead to a large regret. To deal with the challenges brought by the confounder, we apply the dual instrumental variable regression, which can correctly identify the true reward function. We prove the convergence rate of this method is near-optimal in two types of widely used reproducing kernel Hilbert spaces. Therefore, we can design computationally efficient and regret-optimal algorithms based on the theoretical guarantees for confounded bandit problems. The numerical results illustrate the efficacy of our proposed algorithms in the confounded bandit setting.",https://arxiv.org/abs/2209.03225
Reconstructing signed relations from interaction data,"GeorgesAndres, GionaCasiraghi, GiacomoVaccario, FrankSchweitzer",07-sep-22,Social and Information Networks (cs.SI)," Positive and negative relations play an essential role in human behavior and shape the communities we live in. Despite their importance, data about signed relations is rare and commonly gathered through surveys. Interaction data is more abundant, for instance, in the form of proximity or communication data. So far, though, it could not be utilized to detect signed relations. In this paper, we show how the underlying signed relations can be extracted with such data. Employing a statistical network approach, we construct networks of signed relations in four communities. We then show that these relations correspond to the ones reported in surveys. Additionally, the inferred relations allow us to study the homophily of individuals with respect to gender, religious beliefs, and financial backgrounds. We evaluate the importance of triads in the signed network to study group cohesion.",https://arxiv.org/abs/2209.03224
A Predictive Chance Constraint Rebalancing Approach to Mobility-on-Demand Services,"Sten Elling TingstadJacobsen, BalÃ¡zsKulcsÃ¡r, AndersLindman",07-sep-22,Systems and Control (eess.SY)," This paper considers the problem of supply-demand imbalances in Autonomous Mobility-on-Demand systems (AMoD) where demand uncertainty compromises both the service provider's and the customer objectives. The key idea is to include estimated stochastic travel demand patterns into receding horizon AMoD optimization problems. More precisely, we first estimate passenger demand using Gaussian Process Regression (GPR). GPR provides demand uncertainty bounds for time pattern prediction. Second, we integrate demand predictions with uncertainty bounds into a receding horizon AMoD optimization. In order to guarantee constraint satisfaction in the above optimization under estimated stochastic demand prediction, we employ a probabilistic constraining method with user defined confidence interval. Receding horizon AMoD optimization with probabilistic constraints thereby calls for Chance Constrained Model Predictive Control (CCMPC). The benefit of the proposed method is twofold. First, travel demand uncertainty prediction from data can naturally be embedded into AMoD optimization. Second, CCMPC can further be relaxed into a Mixed-Integer-Linear-Program (MILP) that can efficiently be solved. We show, through high-fidelity transportation simulation, that by tuning the confidence bound on the chance constraint close to ""optimal"" oracle performance can be achieved. The median wait time is reduced by 4% compared to using only the mean prediction of the GP.",https://arxiv.org/abs/2209.03219
INFACT: An Online Human Evaluation Framework for ConversationalRecommendation,"AhtshamManzoor, Dietmarjannach",07-sep-22,Human-Computer Interaction (cs.HC)," Conversational recommender systems (CRS) are interactive agents that support their users in recommendation-related goals through multi-turn conversations. Generally, a CRS can be evaluated in various dimensions. Today's CRS mainly rely on offline(computational) measures to assess the performance of their algorithms in comparison to different baselines. However, offline measures can have limitations, for example, when the metrics for comparing a newly generated response with a ground truth do not correlate with human perceptions, because various alternative generated responses might be suitable too in a given dialog situation. Current research on machine learning-based CRS models therefore acknowledges the importance of humans in the evaluation process, knowing that pure offline measures may not be sufficient in evaluating a highly interactive system like a CRS.",https://arxiv.org/abs/2209.03214
Driverless road-marking Machines: Ma(r)king the Way towards the Futureof Mobility,"DomagojMajstorovic, FrankDiermeyer",07-sep-22,Robotics (cs.RO)," Driverless road maintenance could potentially be highly beneficial to all its stakeholders, with the key goals being increased safety for all road participants, more efficient traffic management, and reduced road maintenance costs such that the standard of the road infrastructure is sufficient for it to be used in Automated Driving (AD). This paper addresses how the current state of technology could be expanded to reach those goals. Within the project 'System for Teleoperated Road-marking' (SToRM), using the road-marking machine as the system, different operation modes based on teleoperation were discussed and developed. Furthermore, a functional system overview considering both hardware and software elements was experimentally validated with an actual road-marking machine and should serve as a baseline for future efforts in this and similar areas.",https://arxiv.org/abs/2209.03213
Real-to-Sim: Deep Learning with Auto-Tuning to Predict Residual Errorsusing Sparse Data,"AlexanderSchperberg, YusukeTanaka, Feng Xu, MarcelMenner, Dennis Hong",07-sep-22,Robotics (cs.RO)," Achieving highly accurate kinematic or simulator models that are close to the real robot can facilitate model-based controls (e.g., model predictive control or linear-quadradic regulators), model-based trajectory planning (e.g., trajectory optimization), and decrease the amount of learning time necessary for reinforcement learning methods. Thus, the objective of this work is to learn the residual errors between a kinematic and/or simulator model and the real robot. This is achieved using auto- tuning and neural networks, where the parameters of a neural network are updated using an auto-tuning method that applies equations from an Unscented Kalman Filter (UKF) formulation. Using this method, we model these residual errors with only small amounts of data - a necessity as we improve the simulator/kinematic model by learning directly from hardware operation. We demonstrate our method on robotic hardware (e.g., manipulator arm), and show that with the learned residual errors, we can further close the reality gap between kinematic models, simulations, and the real robot.",https://arxiv.org/abs/2209.03211
Concept-modulated model-based offline reinforcement learning for rapidgeneralization,"Nicholas A.Ketz, Praveen K.Pilly",07-sep-22,Machine Learning (cs.LG)," The robustness of any machine learning solution is fundamentally bound by the data it was trained on. One way to generalize beyond the original training is through human-informed augmentation of the original dataset; however, it is impossible to specify all possible failure cases that can occur during deployment. To address this limitation we combine model-based reinforcement learning and model-interpretability methods to propose a solution that self-generates simulated scenarios constrained by environmental concepts and dynamics learned in an unsupervised manner. In particular, an internal model of the agent's environment is conditioned on low-dimensional concept representations of the input space that are sensitive to the agent's actions. We demonstrate this method within a standard realistic driving simulator in a simple point-to-point navigation task, where we show dramatic improvements in one-shot generalization to different instances of specified failure cases as well as zero-shot generalization to similar variations compared to model-based and model-free approaches.",https://arxiv.org/abs/2209.03210
Four Algorithms on the Swapped Dragonfly,RichardDraper,07-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," The Swapped Dragonfly with M routers per group and K global ports per router is denoted D3(K;M) [1]. It has n=KMM routers and is a partially populated Dragonfly. A Swapped Dragonfly with K and M restricted is studied in this paper. There are four cases. matrix product: If K is a perfect square, a matrix product of size n can be performed in squareroot n rounds. all-to-all exchange: If K and M have a common factor s, an all-to-all exchange can be performed in n/s rounds. broadcast: If D3(K,M) is equipped with a synchronized source-vector header it can perform x broadcast in 3x/M rounds. ascend-descend: If K and M are powers of 2 an ascend-descend algorithm can be performed at twice the cost of the algorithm on a Boolean hypercube of size n. In each case the algorithm on the Swapped Dragonfly is free of link conflicts and is compared with algorithms on a hypercube as well as on the fully populated Dragonfly. The results on the Swapped Dragonfly are more applicable than the special cases because D3(K,M) contains emulations of every Swapped Dragonfly with J less than equal to K and/or L less than or equal to M.   Keywords: Swapped Interconnection Network, Matrix Product, All-to-all, Universal Exchange, Boolean Hypercube, Ascend-descend algorithm, Broad- cast, Edge-disjoint spanning tree.   References [1] R. Draper. The Swapped Dragonfly , ArXiv for Computer Science:[2202.01843](https://arxiv.org/abs/2202.01843). 1",https://arxiv.org/abs/2209.03207
When Are Names Similar Or the Same? Introducing the Code Names MatcherLibrary,"MosheMunk, Dror G.Feitelson",07-sep-22,Software Engineering (cs.SE)," Program code contains functions, variables, and data structures that are represented by names. To promote human understanding, these names should describe the role and use of the code elements they represent. But the names given by developers show high variability, reflecting the tastes of each developer, with different words used for the same meaning or the same words used for different meanings. This makes comparing names hard. A precise comparison should be based on matching identical words, but also take into account possible variations on the words (including spelling and typing errors), reordering of the words, matching between synonyms, and so on. To facilitate this we developed a library of comparison functions specifically targeted to comparing names in code. The different functions calculate the similarity between names in different ways, so a researcher can choose the one appropriate for his specific needs. All of them share an attempt to reflect human perceptions of similarity, at the possible expense of lexical matching.",https://arxiv.org/abs/2209.03203
TickTock: Detecting Microphone Status in Laptops LeveragingElectromagnetic Leakage of Clock Signals,"SoundaryaRamesh, Ghozali SuhariyantoHadi, SihunYang, MunChoon Chan, Jun Han",07-sep-22,Cryptography and Security (cs.CR)," We are witnessing a heightened surge in remote privacy attacks on laptop computers. These attacks often exploit malware to remotely gain access to webcams and microphones in order to spy on the victim users. While webcam attacks are somewhat defended with widely available commercial webcam privacy covers, unfortunately, there are no adequate solutions to thwart the attacks on mics despite recent industry efforts. As a first step towards defending against such attacks on laptop mics, we propose TickTock, a novel mic on/off status detection system. To achieve this, TickTock externally probes the electromagnetic (EM) emanations that stem from the connectors and cables of the laptop circuitry carrying mic clock signals. This is possible because the mic clock signals are only input during the mic recording state, causing resulting emanations. We design and implement a proof-of-concept system to demonstrate TickTock's feasibility. Furthermore, we comprehensively evaluate TickTock on a total of 30 popular laptops executing a variety of applications to successfully detect mic status in 27 laptops. Of these, TickTock consistently identifies mic recording with high true positive and negative rates.",https://arxiv.org/abs/2209.03198
Efficient Implementation of Non-linear Flow Law Using Neural Networkinto the Abaqus Explicit FEM code,"OlivierPantalÃ©, Pierre TizeMha, AmÃ¨viTongne",07-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," Machine learning techniques are increasingly used to predict material behavior in scientific applications and offer a significant advantage over conventional numerical methods. In this work, an Artificial Neural Network (ANN) model is used in a finite element formulation to define the flow law of a metallic material as a function of plastic strain, plastic strain rate and temperature. First, we present the general structure of the neural network, its operation and focus on the ability of the network to deduce, without prior learning, the derivatives of the flow law with respect to the model inputs. In order to validate the robustness and accuracy of the proposed model, we compare and analyze the performance of several network architectures with respect to the analytical formulation of a Johnson-Cook behavior law for a 42CrMo4 steel. In a second part, after having selected an Artificial Neural Network architecture with $2$ hidden layers, we present the implementation of this model in the Abaqus Explicit computational code in the form of a VUHARD subroutine. The predictive capability of the proposed model is then demonstrated during the numerical simulation of two test cases: the necking of a circular bar and a Taylor impact test. The results obtained show a very high capability of the ANN to replace the analytical formulation of a Johnson-Cook behavior law in a finite element code, while remaining competitive in terms of numerical simulation time compared to a classical approach.",https://arxiv.org/abs/2209.03197
Avast-CTU Public CAPE Dataset,"BranislavBosansky, DominikKouba, OndrejManhal, ThorstenSick, ViliamLisy, JakubKroustek, Petr Somol",06-sep-22,Cryptography and Security (cs.CR)," There is a limited amount of publicly available data to support research in malware analysis technology. Particularly, there are virtually no publicly available datasets generated from rich sandboxes such as Cuckoo/CAPE. The benefit of using dynamic sandboxes is the realistic simulation of file execution in the target machine and obtaining a log of such execution. The machine can be infected by malware hence there is a good chance of capturing the malicious behavior in the execution logs, thus allowing researchers to study such behavior in detail. Although the subsequent analysis of log information is extensively covered in industrial cybersecurity backends, to our knowledge there has been only limited effort invested in academia to advance such log analysis capabilities using cutting edge techniques. We make this sample dataset available to support designing new machine learning methods for malware detection, especially for automatic detection of generic malicious behavior. The dataset has been collected in cooperation between Avast Software and Czech Technical University - AI Center (AIC).",https://arxiv.org/abs/2209.03190
Master equation of discrete-time Stackelberg mean field games withmultiple leaders,DeepanshuVasal,07-sep-22,Systems and Control (eess.SY)," In this paper, we consider a discrete-time Stackelberg graphon mean field game with a finite number of leaders, a finite number of major followers and an infinite number of minor followers. The leaders and the followers each observe types privately that evolve as conditionally independent controlled Markov processes. The leaders and the followers sequentially make strategic decisions where each follower's actions affect her neighbors, which is captured in a graph generated by a known graphon, however, the leaders' actions affect everyone. The leaders are of ""Stackelberg"" kind which means each of them commits to a dynamic policy and all the followers(both major and minor) best respond to that policy and each other. Knowing that the minor followers would best respond (in the sense of a mean-field game) while the major followers will best respond (in the sense of NAsh) based on their policies, each leader chooses a policy that maximizes her reward knowing that other leader's are doing the same. We refer to the resulting outcome as a Stackelberg Graphon Mean Field Equilibrium with multiple leaders (SGMFE-ML). In this paper, we provide a master equation of this game that allows one to compute all SGMFE-ML. We further extend this notion to the case when there are an infinite number of leaders.",https://arxiv.org/abs/2209.03188
Master Equation for Discrete-Time Stackelberg Mean Field Games withsingle leader,"DeepanshuVasal, RandallBerry",16 Jan 2022,Systems and Control (eess.SY)," In this paper, we consider a discrete-time Stackelberg mean field game with a leader and an infinite number of followers. The leader and the followers each observe types privately that evolve as conditionally independent controlled Markov processes. The leader commits to a dynamic policy and the followers best respond to that policy and each other. Knowing that the followers would play a mean field game based on her policy, the leader chooses a policy that maximizes her reward. We refer to the resulting outcome as a Stackelberg mean field equilibrium (SMFE). In this paper, we provide a master equation of this game that allows one to compute all SMFE. Based on our framework, we consider two numerical examples. First, we consider an epidemic model where the followers get infected based on the mean field population. The leader chooses subsidies for a vaccine to maximize social welfare and minimize vaccination costs. In the second example, we consider a technology adoption game where the followers decide to adopt a technology or a product and the leader decides the cost of one product that maximizes his returns, which are proportional to the people adopting that technology",https://arxiv.org/abs/2209.03186
Combining Sequential and Aggregated Data for Churn Prediction inCasual Freemium Games,"Jeppe TheissKristensen, PaoloBurelli",06-sep-22,Artificial Intelligence (cs.AI)," In freemium games, the revenue from a player comes from the in-app purchases made and the advertisement to which that player is exposed. The longer a player is playing the game, the higher will be the chances that he or she will generate a revenue within the game. Within this scenario, it is extremely important to be able to detect promptly when a player is about to quit playing (churn) in order to react and attempt to retain the player within the game, thus prolonging his or her game lifetime. In this article we investigate how to improve the current state-of-the-art in churn prediction by combining sequential and aggregate data using different neural network architectures. The results of the comparative analysis show that the combination of the two data types grants an improvement in the prediction accuracy over predictors based on either purely sequential or purely aggregated data.",https://arxiv.org/abs/2201.05959
On the Effectiveness of Compact Biomedical Transformers,"OmidRohanian, MohammadmahdiNouriborji, SamanehKouchaki, David A.Clifton",07-sep-22,Computation and Language (cs.CL)," Language models pre-trained on biomedical corpora, such as BioBERT, have recently shown promising results on downstream biomedical tasks. Many existing pre-trained models, on the other hand, are resource- intensive and computationally heavy owing to factors such as embedding size, hidden dimension, and number of layers. The natural language processing (NLP) community has developed numerous strategies to compress these models utilising techniques such as pruning, quantisation, and knowledge distillation, resulting in models that are considerably faster, smaller, and subsequently easier to use in practice. By the same token, in this paper we introduce six lightweight models, namely, BioDistilBERT, BioTinyBERT, BioMobileBERT, DistilBioBERT, TinyBioBERT, and CompactBioBERT which are obtained either by knowledge distillation from a biomedical teacher or continual learning on the Pubmed dataset via the Masked Language Modelling (MLM) objective. We evaluate all of our models on three biomedical tasks and compare them with BioBERT-v1.1 to create efficient lightweight models that perform on par with their larger counterparts. All the models will be publicly available on our Huggingface profile at [this https URL](https://huggingface.co/nlpie) and the codes used to run the experiments will be available at [this https URL](https://github.com/nlpie- research/Compact-Biomedical-Transformers).",https://arxiv.org/abs/2209.03184
Composite Community-Aware Diversified Influence Maximization withEfficient Approximation,"JianxiongGuo, WeiliWu, Ding-ZhuDu",07-sep-22,Social and Information Networks (cs.SI)," Influence Maximization (IM) is a famous topic in mobile networks and social computing, which aims at finding a small subset of users to maximize the influence spread through online information cascade. Recently, some careful researchers paid attention to diversity of information dissemination, especially community-aware diversity, and formulated the diversified IM problem. The diversity is ubiquitous in a lot of real-world applications, but they are all based on a given community structure. In social networks, we can form heterogeneous community structures for the same group of users according to different metrics. Therefore, how to quantify the diversity based on multiple community structures is an interesting question. In this paper, we propose the Composite Community-Aware Diversified IM (CC-DIM) problem, which aims at selecting a seed set to maximize the influence spread and the composite diversity over all possible community structures under consideration. To address the NP-hardness of CC- DIM problem, we adopt the technique of reverse influence sampling and design a random Generalized Reverse Reachable (G-RR) set to estimate the objective function. The composition of a random G-RR set is much more complex than the RR set used for the IM problem, which will lead to inefficiency of traditional sampling-based approximation algorithms. Because of this, we further propose a two-stage algorithm, Generalized HIST (G-HIST). It can not only return a $(1-1/e-\varepsilon)$ approximate solution with at least $(1-\delta)$ probability, but also improve the efficiency of sampling and ease the difficulty of searching by significantly reducing the average size of G-RR sets. Finally, we evaluate our G-HIST on real datasets against existing algorithms. The experimental results show the effectiveness of our proposed algorithm and its superiority over other baseline algorithms.",https://arxiv.org/abs/2209.03182
On the utility and protection of optimization with differentialprivacy and classic regularization techniques,"EugenioLomurno, Matteomatteucci",07-sep-22,Machine Learning (cs.LG)," Nowadays, owners and developers of deep learning models must consider stringent privacy-preservation rules of their training data, usually crowd-sourced and retaining sensitive information. The most widely adopted method to enforce privacy guarantees of a deep learning model nowadays relies on optimization techniques enforcing differential privacy. According to the literature, this approach has proven to be a successful defence against several models' privacy attacks, but its downside is a substantial degradation of the models' performance. In this work, we compare the effectiveness of the differentially-private stochastic gradient descent (DP-SGD) algorithm against standard optimization practices with regularization techniques. We analyze the resulting models' utility, training performance, and the effectiveness of membership inference and model inversion attacks against the learned models. Finally, we discuss differential privacy's flaws and limits and empirically demonstrate the often superior privacy-preserving properties of dropout and l2-regularization.",https://arxiv.org/abs/2209.03176
Computing the Hit Rate of Similarity Caching,"Younes BenMazziane, Sara Alouf, GiovanniNeglia, Daniel SadocMenasche",07-sep-22,Data Structures and Algorithms (cs.DS)," Similarity caching allows requests for an item \\(i\\) to be served by a similar item \\(i'\\). Applications include recommendation systems, multimedia retrieval, and machine learning. Recently, many similarity caching policies have been proposed, but still we do not know how to compute the hit rate even for the simplest policies, like SIM-LRU and RND-LRU that are straightforward modifications of classical caching algorithms. This paper proposes the first algorithm to compute the hit rate of similarity caching policies under the independent reference model for the request process. In particular, our work shows how to extend the popular TTL approximation from classic caching to similarity caching. The algorithm is evaluated on both synthetic and real world traces.",https://arxiv.org/abs/2209.03175
Explainable Artificial Intelligence to Detect Image Spam UsingConvolutional Neural Network,"ZhiboZhang, ErnestoDamiani, Hussam AlHamadi, Chan YeobYeun, FatmaTaher",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Image spam threat detection has continually been a popular area of research with the internet's phenomenal expansion. This research presents an explainable framework for detecting spam images using Convolutional Neural Network(CNN) algorithms and Explainable Artificial Intelligence (XAI) algorithms. In this work, we use CNN model to classify image spam respectively whereas the post-hoc XAI methods including Local Interpretable Model Agnostic Explanation (LIME) and Shapley Additive Explanations (SHAP) were deployed to provide explanations for the decisions that the black-box CNN models made about spam image detection. We train and then evaluate the performance of the proposed approach on a 6636 image dataset including spam images and normal images collected from three different publicly available email corpora. The experimental results show that the proposed framework achieved satisfactory detection results in terms of different performance metrics whereas the model-independent XAI algorithms could provide explanations for the decisions of different models which could be utilized for comparison for the future study.",https://arxiv.org/abs/2209.03174
Machine Learning-based Automatic Annotation and Detection of COVID-19Fake News,"Mohammad MajidAkhtar, BibhasSharma, IshanKarunanayake, RahatMasood, MuhammadIkram, SalilS.Kanhere",07-sep-22,Social and Information Networks (cs.SI)," COVID-19 impacted every part of the world, although the misinformation about the outbreak traveled faster than the virus. Misinformation spread through online social networks (OSN) often misled people from following correct medical practices. In particular, OSN bots have been a primary source of disseminating false information and initiating cyber propaganda. Existing work neglects the presence of bots that act as a catalyst in the spread and focuses on fake news detection in 'articles shared in posts' rather than the post (textual) content. Most work on misinformation detection uses manually labeled datasets that are hard to scale for building their predictive models. In this research, we overcome this challenge of data scarcity by proposing an automated approach for labeling data using verified fact-checked statements on a Twitter dataset. In addition, we combine textual features with user-level features (such as followers count and friends count) and tweet-level features (such as number of mentions, hashtags and urls in a tweet) to act as additional indicators to detect misinformation. Moreover, we analyzed the presence of bots in tweets and show that bots change their behavior over time and are most active during the misinformation campaign. We collected 10.22 Million COVID-19 related tweets and used our annotation model to build an extensive and original ground truth dataset for classification purposes. We utilize various machine learning models to accurately detect misinformation and our best classification model achieves precision (82%), recall (96%), and false positive rate (3.58%). Also, our bot analysis indicates that bots generated approximately 10% of misinformation tweets. Our methodology results in substantial exposure of false information, thus improving the trustworthiness of information disseminated through social media platforms.",https://arxiv.org/abs/2209.03166
AI Illustrator: Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation,"YiyangMa, HuanYang, BeiLiu, JianlongFu, JiayingLiu","7 Sep 2022 (v1(https://arxiv.org/abs/2209.03160v1)), lastrevised 8 Sep 2022 (this version, v2)",Computer Vision and Pattern Recognition (cs.CV)," AI illustrator aims to automatically design visually appealing images for books to provoke rich thoughts and emotions. To achieve this goal, we propose a framework for translating raw descriptions with complex semantics into semantically corresponding images. The main challenge lies in the complexity of the semantics of raw descriptions, which may be hard to be visualized (e.g., ""gloomy"" or ""Asian""). It usually poses challenges for existing methods to handle such descriptions. To address this issue, we propose a Prompt-based Cross-Modal Generation Framework (PCM-Frame) to leverage two powerful pre-trained models, including CLIP and StyleGAN. Our framework consists of two components: a projection module from Text Embeddings to Image Embeddings based on prompts, and an adapted image generation module built on StyleGAN which takes Image Embeddings as inputs and is trained by combined semantic consistency losses. To bridge the gap between realistic images and illustration designs, we further adopt a stylization model as post-processing in our framework for better visual effects. Benefiting from the pre-trained models, our method can handle complex descriptions and does not require external paired data for training. Furthermore, we have built a benchmark that consists of 200 raw descriptions. We conduct a user study to demonstrate our superiority over the competing methods with complicated texts. We release our code at [this https URL](https://github.com/researchmm/AI_Illustrator).",https://arxiv.org/abs/2209.03162
Fault Signature Identification for BLDC motor Drive System -AStatistical Signal Fusion Approach,"Tribeni PrasadBanerjee, Susanta Roy, B. K.Panigrahi",07-sep-22,Systems and Control (eess.SY)," A hybrid approach based on multirate signal processing and sensory data fusion is proposed for the condition monitoring and identification of fault signal signatures used in the Flight ECS (Engine Control System) unit. Though motor current signature analysis (MCSA) is widely used for fault detection now-a-days, the proposed hybrid method qualifies as one of the most powerful online/offline techniques for diagnosing the process faults. Existing approaches have some drawbacks that can degrade the performance and accuracy of a process-diagnosis system. In particular, it is very difficult to detect random stochastic noise due to the nonlinear behavior of valve controller. Using only Short Time Fourier Transform (STFT), frequency leakage and the small amplitude of the current components related to the fault can be observed, but the fault due to the controller behavior cannot be observed. Therefore, a framework of advanced multirate signal and data- processing aided with sensor fusion algorithms is proposed in this article and satisfactory results are obtained. For implementing the system, a DSP- based BLDC motor controller with three-phase inverter module (TMS 320F2812) is used and the performance of the proposed method is validated on real time data.",https://arxiv.org/abs/2209.03160
FasterX: Real-Time Object Detection Based on Edge GPUs for UAVApplications,"WeiZhou, XuanlinMin, RuiHu, YiwenLong, HuanLuo, JunYi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Real-time object detection on Unmanned Aerial Vehicles (UAVs) is a challenging issue due to the limited computing resources of edge GPU devices as Internet of Things (IoT) nodes. To solve this problem, in this paper, we propose a novel lightweight deep learning architectures named FasterX based on YOLOX model for real-time object detection on edge GPU. First, we design an effective and lightweight PixSF head to replace the original head of YOLOX to better detect small objects, which can be further embedded in the depthwise separable convolution (DS Conv) to achieve a lighter head. Then, a slimmer structure in the Neck layer termed as SlimFPN is developed to reduce parameters of the network, which is a trade-off between accuracy and speed. Furthermore, we embed attention module in the Head layer to improve the feature extraction effect of the prediction head. Meanwhile, we also improve the label assignment strategy and loss function to alleviate category imbalance and box optimization problems of the UAV dataset. Finally, auxiliary heads are presented for online distillation to improve the ability of position embedding and feature extraction in PixSF head. The performance of our lightweight models are validated experimentally on the NVIDIA Jetson NX and Jetson Nano GPU embedded platforms.Extensive experiments show that FasterX models achieve better trade-off between accuracy and latency on VisDrone2021 dataset compared to state-of-the-art models.",https://arxiv.org/abs/2209.03159
A multiclass Q-NLP sentiment analysis experiment using DisCoCat,"VictorMartinez, Guilhaume Leroy-Meline",07-sep-22,Computation and Language (cs.CL)," Sentiment analysis is a branch of Natural Language Processing (NLP) which goal is to assign sentiments or emotions to particular sentences or words. Performing this task is particularly useful for companies wishing to take into account customer feedback through chatbots or verbatim. This has been done extensively in the literature using various approaches, ranging from simple models to deep transformer neural networks. In this paper, we will tackle sentiment analysis in the Noisy Intermediate Scale Computing (NISQ) era, using the DisCoCat model of language. We will first present the basics of quantum computing and the DisCoCat model. This will enable us to define a general framework to perform NLP tasks on a quantum computer. We will then extend the two-class classification that was performed by Lorenz et al. (2021) to a four-class sentiment analysis experiment on a much larger dataset, showing the scalability of such a framework.",https://arxiv.org/abs/2209.03157
MRF-PINN: A Multi-Receptive-Field convolutional physics-informedneural network for solving partial differential equations,"ShihongZhang, ChiZhang, BosenWang",06-sep-22,Machine Learning (cs.LG)," Physics-informed neural networks (PINN) can achieve lower development and solving cost than traditional partial differential equation (PDE) solvers in scenarios such as reconstructing the physics field and solving the inverse problem. Due to the advantages of parameter sharing, spatial feature extraction and low inference cost, convolutional neural networks (CNN) are increasingly used in PINN. To adapt convolutional PINN to different equations, researchers have to spend much time tuning critical hyperparameters. Furthermore, the effects of finite difference accuracy, model complexity, and mesh resolution on the prediction result of convolutional PINN are unclear. To fill the above research gaps, in this paper, (1) A Multi-Receptive-Field PINN (MRF-PINN) model is constructed to adapt different equation types and mesh resolutions without manual tuning.(2) The generality and advantages of the MRF-PINN are verified in three typical linear PDEs (elliptic, parabolic, hyperbolic) and nonlinear PDEs (Navier-Stokes equations). (3) The contribution of each receptive field to the final MRF-PINN result is analyzed, and the influence of finite difference accuracy, model complexity (channel number) and mesh resolution on the MRF-PINN result is tested. This paper shows that MRF-PINN can adapt to completely different equation types and mesh resolutions without any hyperparameter tuning. Further, the solving error is significantly decreased under high-order finite difference, large channel number, and high mesh resolution, which is expected to become a general convolutional PINN scheme.",https://arxiv.org/abs/2209.03152
Remote Work Optimization with Robust Multi-channel Graph NeuralNetworks,"QinyiZhu, LiangWu, QiGuo, LiangjieHong",26 Aug 2022,Social and Information Networks (cs.SI)," The spread of COVID-19 leads to the global shutdown of many corporate offices, and encourages companies to open more opportunities that allow employees to work from a remote location. As the workplace type expands from onsite offices to remote areas, an emerging challenge for an online hiring marketplace is how these remote opportunities and user intentions to work remotely can be modeled and matched without prior information. Despite the unprecedented amount of remote jobs posted amid COVID-19, there is no existing approach that can be directly applied.   Introducing a brand new workplace type naturally leads to the cold-start problem, which is particularly more common for less active job seekers. It is challenging, if not impossible, to onboard a new workplace type for any predictive model if existing information sources can provide little information related to a new category of jobs, including data from resumes and job descriptions. Hence, in this work, we aim to propose a principled approach that jointly models the remoteness of job seekers and job opportunities with limited information, which also suffices the needs of web-scale applications. Existing research on the emerging type of remote workplace mainly focuses on qualitative studies, and classic predictive modeling approaches are inapplicable considering the problem of cold-start and information scarcity. We precisely try to close this gap with a novel graph neural architecture. Extensive experiments on large-scale data from real-world applications have been conducted to validate the superiority of the proposed approach over competitive baselines. The improvement may translate to more rapid onboarding of the new workplace type that can benefit job seekers who are interested in working remotely.",https://arxiv.org/abs/2209.03151
MultiViz: A Gephi Plugin for Scalable Visualization of Multi-LayerNetworks,"Jayamohan PillaiC.S., AyanChatterjee, Geetha M., AmitavaMukherjee",06-sep-22,Social and Information Networks (cs.SI)," The process of visually presenting networks is an effective way to understand entity relationships within the networks since it reveals the overall structure and topology of the network. Real networks are extremely difficult to visualize due to their immense complexity, which includes vast amounts of data, several types of interactions, various subsystems and several levels of connectivity as well as changes over time. This paper introduces the ""MultiViz Plugin,"" a plugin for gephi, an open-source software tool for graph visualization and modification, in order to to visualize complex networks in a multi-layer manner. A collection of settings are availabe through the plugin to transform an existing network into a multi-layered network. The plugin supports several layout algorithms and lets user to choose which property of the network to be used as the layer. The goal of the study is to give the user complete control over how the network is visualized in a multi-layer fashion. We demonstrate the ability of the plugin to visualize multi-layer data using a real-life complex multi- layer datasets.",https://arxiv.org/abs/2209.03150
Improving Out-of-Distribution Detection via Epistemic UncertaintyAdversarial Training,"DerekEverett, Andre T.Nguyen, Luke E.Richards, Edward Raff",05-sep-22,Machine Learning (cs.LG)," The quantification of uncertainty is important for the adoption of machine learning, especially to reject out-of-distribution (OOD) data back to human experts for review. Yet progress has been slow, as a balance must be struck between computational efficiency and the quality of uncertainty estimates. For this reason many use deep ensembles of neural networks or Monte Carlo dropout for reasonable uncertainty estimates at relatively minimal compute and memory. Surprisingly, when we focus on the real-world applicable constraint of $\leq 1\%$ false positive rate (FPR), prior methods fail to reliably detect OOD samples as such. Notably, even Gaussian random noise fails to trigger these popular OOD techniques. We help to alleviate this problem by devising a simple adversarial training scheme that incorporates an attack of the epistemic uncertainty predicted by the dropout ensemble. We demonstrate this method improves OOD detection performance on standard data (i.e., not adversarially crafted), and improves the standardized partial AUC from near-random guessing performance to $\geq 0.75$.",https://arxiv.org/abs/2209.03149
Network Intrusion Detection with Limited Labeled Data,"S.Lotfi, M.Modirrousta, S.Shashaani, S. Amini, M. AliyariShoorehdeli",01-sep-22,Cryptography and Security (cs.CR)," With the increasing dependency of daily life over computer networks, the importance of these networks security becomes prominent. Different intrusion attacks to networks have been designed and the attackers are working on improving them. Thus the ability to detect intrusion with limited number of labeled data is desirable to provide networks with higher level of security. In this paper we design an intrusion detection system based on a deep neural network. The proposed system is based on self- supervised contrastive learning where a huge amount of unlabeled data can be used to generate informative representation suitable for various downstream tasks with limited number of labeled data. Using different experiments, we have shown that the proposed system presents an accuracy of 94.05% over the UNSW-NB15 dataset, an improvement of 4.22% in comparison to previous method based on self-supervised learning. Our simulations have also shown impressive results when the size of labeled training data is limited. The performance of the resulting Encoder Block trained on UNSW-NB15 dataset has also been tested on other datasets for representation extraction which shows competitive results in downstream tasks.",https://arxiv.org/abs/2209.03148
A New Heterogeneous Graph Representation in a Social Media Platform:Steemit,"NegarMaleki, BalajiPadamanabhan, KaushikDutta",02-sep-22,Social and Information Networks (cs.SI)," Recently, temporal graphs have substituted dynamic graphs as many real-world problems evolve in continuous time rather than in discrete time, and besides time almost all problems are designed in a heterogeneous format rather than a homogeneous one. However, most existing graph representations do not consider time in their components. To this end, in this paper, we present a new heterogeneous graph representation including time in every single component of the graph, i.e., nodes and edges. We also introduce four time-dependent queries to address machine learning or deep learning problems. Our findings reveal that considering the size of the enormous graphs, our time-dependent queries execute efficiently. In order to show the expressive power of time in graph representation, we construct a graph for a new social media platform (Steemit), and address a DL prediction task using graph neural networks (GNNs). Predicting the payout for a newly published post is one of the most fascinating classification problems in the Steemit setting, and we address this problem with two approaches followed by GNN models.",https://arxiv.org/abs/2209.03147
AudioLM: a Language Modeling Approach to Audio Generation,"ZalÃ¡nBorsos, RaphaÃ«lMarinier, DamienVincent, EugeneKharitonov, OlivierPietquin, MattSharifi, OlivierTeboul, DavidGrangier, MarcoTagliasacchi, NeilZeghidour",07-sep-22,Sound (cs.SD)," We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",https://arxiv.org/abs/2209.03144
RF Fingerprinting Needs Attention: Multi-task Approach for Real-WorldWiFi and Bluetooth,"AnuJagannath, Zackary Kane, JithinJagannath",07-sep-22,Machine Learning (cs.LG)," A novel cross-domain attentional multi-task architecture - xDom - for robust real-world wireless radio frequency (RF) fingerprinting is presented in this work. To the best of our knowledge, this is the first time such comprehensive attention mechanism is applied to solve RF fingerprinting problem. In this paper, we resort to real-world IoT WiFi and Bluetooth (BT) emissions (instead of synthetic waveform generation) in a rich multipath and unavoidable interference environment in an indoor experimental testbed. We show the impact of the time-frame of capture by including waveforms collected over a span of months and demonstrate the same time-frame and multiple time-frame fingerprinting evaluations. The effectiveness of resorting to a multi-task architecture is also experimentally proven by conducting single-task and multi-task model analyses. Finally, we demonstrate the significant gain in performance achieved with the proposed xDom architecture by benchmarking against a well-known state-of-the-art model for fingerprinting. Specifically, we report performance improvements by up to 59.3% and 4.91x under single-task WiFi and BT fingerprinting respectively, and up to 50.5% increase in fingerprinting accuracy under the multi-task setting.",https://arxiv.org/abs/2209.03143
Pixel-Level Equalized Matching for Video Object Segmentation,"SuhwanCho, Woo JinKim, MyeongAh Cho, Seunghoon Lee, Minhyeok Lee, Chaewon Park, Sangyoun Lee",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Feature similarity matching, which transfers the information of the reference frame to the query frame, is a key component in semi- supervised video object segmentation. If surjective matching is adopted, background distractors can easily occur and degrade the performance. Bijective matching mechanisms try to prevent this by restricting the amount of information being transferred to the query frame, but have two limitations: 1) surjective matching cannot be fully leveraged as it is transformed to bijective matching at test time; and 2) test-time manual tuning is required for searching the optimal hyper-parameters. To overcome these limitations while ensuring reliable information transfer, we introduce an equalized matching mechanism. To prevent the reference frame information from being overly referenced, the potential contribution to the query frame is equalized by simply applying a softmax operation along with the query. On public benchmark datasets, our proposed approach achieves a comparable performance to state-of-the-art methods.",https://arxiv.org/abs/2209.03142
Treating Motion as Option to Reduce Motion Dependency in UnsupervisedVideo Object Segmentation,"SuhwanCho, MinhyeokLee, SeunghoonLee, ChaewonPark, DonghyeongKim, SangyounLee",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Unsupervised video object segmentation (VOS) aims to detect the most salient object in a video sequence at the pixel level. In unsupervised VOS, most state-of-the-art methods leverage motion cues obtained from optical flow maps in addition to appearance cues to exploit the property that salient objects usually have distinctive movements compared to the background. However, as they are overly dependent on motion cues, which may be unreliable in some cases, they cannot achieve stable prediction. To reduce this motion dependency of existing two-stream VOS methods, we propose a novel motion-as-option network that optionally utilizes motion cues. Additionally, to fully exploit the property of the proposed network that motion is not always required, we introduce a collaborative network learning strategy. On all the public benchmark datasets, our proposed network affords state-of-the-art performance with real-time inference speed.",https://arxiv.org/abs/2209.03139
Federated Transfer Learning with Multimodal Data,YulianSun,05-sep-22,Machine Learning (cs.LG)," Smart cars, smartphones and other devices in the Internet of Things (IoT), which usually have more than one sensors, produce multimodal data. Federated Learning supports collecting a wealth of multimodal data from different devices without sharing raw data. Transfer Learning methods help transfer knowledge from some devices to others. Federated Transfer Learning methods benefit both Federated Learning and Transfer Learning. This newly proposed Federated Transfer Learning framework aims at connecting data islands with privacy protection. Our construction is based on Federated Learning and Transfer Learning. Compared with previous Federated Transfer Learnings, where each user should have data with identical modalities (either all unimodal or all multimodal), our new framework is more generic, it allows a hybrid distribution of user data. The core strategy is to use two different but inherently connected training methods for our two types of users. Supervised Learning is adopted for users with only unimodal data (Type 1), while Self-Supervised Learning is applied to user with multimodal data (Type 2) for both the feature of each modality and the connection between them. This connection knowledge of Type 2 will help Type 1 in later stages of training. Training in the new framework can be divided in three steps. In the first step, users who have data with the identical modalities are grouped together. For example, user with only sound signals are in group one, and those with only images are in group two, and users with multimodal data are in group three, and so on. In the second step, Federated Learning is executed within the groups, where Supervised Learning and Self-Supervised Learning are used depending on the group's nature. Most of the Transfer Learning happens in the third step, where the related parts in the network obtained from the previous steps are aggregated (federated).",https://arxiv.org/abs/2209.03138
Wavelength-aware 2D Convolutions for Hyperspectral Imaging,"Leon AmadeusVarga, MartinMessmer, NuriBenbarka, Andreas Zell",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Deep Learning could drastically boost the classification accuracy for Hyperspectral Imaging (HSI). Still, the training on the mostly small hyperspectral data sets is not trivial. Two key challenges are the large channel dimension of the recordings and the incompatibility between cameras of different manufacturers. By introducing a suitable model bias and continuously defining the channel dimension, we propose a 2D convolution optimized for these challenges of Hyperspectral Imaging. We evaluate the method based on two different hyperspectral applications (inline inspection and remote sensing). Besides the shown superiority of the model, the modification adds additional explanatory power. In addition, the model learns the necessary camera filters in a data-driven manner. Based on these camera filters, an optimal camera can be designed.",https://arxiv.org/abs/2209.03137
MSSPN: Automatic First Arrival Picking using Multi-Stage SegmentationPicking Network,"HongtaoWang, JiangsheZhang, Xiaoli Wei, ChunxiaZhang, Zhenbo Guo, Li Long, Yicheng Wang",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Picking the first arrival times of prestack gathers is called First Arrival Time (FAT) picking, which is an indispensable step in seismic data processing, and is mainly solved manually in the past. With the current increasing density of seismic data collection, the efficiency of manual picking has been unable to meet the actual needs. Therefore, automatic picking methods have been greatly developed in recent decades, especially those based on deep learning. However, few of the current supervised deep learning-based method can avoid the dependence on labeled samples. Besides, since the gather data is a set of signals which are greatly different from the natural images, it is difficult for the current method to solve the FAT picking problem in case of a low Signal to Noise Ratio (SNR). In this paper, for hard rock seismic gather data, we propose a Multi-Stage Segmentation Pickup Network (MSSPN), which solves the generalization problem across worksites and the picking problem in the case of low SNR. In MSSPN, there are four sub-models to simulate the manually picking processing, which is assumed to four stages from coarse to fine. Experiments on seven field datasets with different qualities show that our MSSPN outperforms benchmarks by a large margin.Particularly, our method can achieve more than 90\% accurate picking across worksites in the case of medium and high SNRs, and even fine-tuned model can achieve 88\% accurate picking of the dataset with low SNR.",https://arxiv.org/abs/2209.03136
DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical ModalityAttention,"ShunsukeKitada, YukiIwazaki, RikuTogashi, HitoshiIyatomi",07-sep-22,Multimedia (cs.MM)," There is increasing interest in the use of multimodal data in various web applications, such as digital advertising and e-commerce. Typical methods for extracting important information from multimodal data rely on a mid-fusion architecture that combines the feature representations from multiple encoders. However, as the number of modalities increases, several potential problems with the mid-fusion model structure arise, such as an increase in the dimensionality of the concatenated multimodal features and missing modalities. To address these problems, we propose a new concept that considers multimodal inputs as a set of sequences, namely, deep multimodal sequence sets (DM$^2$S$^2$). Our set-aware concept consists of three components that capture the relationships among multiple modalities: (a) a BERT-based encoder to handle the inter- and intra-order of elements in the sequences, (b) intra-modality residual attention (IntraMRA) to capture the importance of the elements in a modality, and (c) inter-modality residual attention (InterMRA) to enhance the importance of elements with modality-level granularity further. Our concept exhibits performance that is comparable to or better than the previous set-aware models. Furthermore, we demonstrate that the visualization of the learned InterMRA and IntraMRA weights can provide an interpretation of the prediction results.",https://arxiv.org/abs/2209.03132
SAGE: Software-based Attestation for GPU Execution,"AndreiIvanov, BenjaminRothenberger, ArnaudDethise, MarcoCanini, TorstenHoefler, AdrianPerrig",07-sep-22,Cryptography and Security (cs.CR)," With the application of machine learning to security-critical and sensitive domains, there is a growing need for integrity and privacy in computation using accelerators, such as GPUs. Unfortunately, the support for trusted execution on GPUs is currently very limited - trusted execution on accelerators is particularly challenging since the attestation mechanism should not reduce performance. Although hardware support for trusted execution on GPUs is emerging, we study purely software-based approaches for trusted GPU execution. A software-only approach offers distinct advantages: (1) complement hardware-based approaches, enhancing security especially when vulnerabilities in the hardware implementation degrade security, (2) operate on GPUs without hardware support for trusted execution, and (3) achieve security without reliance on secrets embedded in the hardware, which can be extracted as history has shown. In this work, we present SAGE, a software- based attestation mechanism for GPU execution. SAGE enables secure code execution on NVIDIA GPUs of the Ampere architecture (A100), providing properties of code integrity and secrecy, computation integrity, as well as data integrity and secrecy - all in the presence of malicious code running on the GPU and CPU. Our evaluation demonstrates that SAGE is already practical today for executing code in a trustworthy way on GPUs without specific hardware support.",https://arxiv.org/abs/2209.03126
A Data-driven Reduced Order Modeling Approach Applied In Context OfNumerical Analysis And Optimization Of Plastic Profile Extrusion,"DanielHilger, NorbertHosters",07-sep-22,Numerical Analysis (math.NA)," In course of this work, we examine the process of plastic profile extrusion, where a polymer melt is shaped inside the so-called extrusion die and fixed in its shape by solidification in the downstream calibration unit. More precise, we focus on the development of a data-driven reduced order model (ROM) for the purpose of predicting temperature distributions within the extruded profiles inside the calibration unit. Therein, the ROM functions as a first step to our overall goal of prediction based process control in order to avoid undesired warpage and damages of the final product.",https://arxiv.org/abs/2209.03125
The Ethical Need for Watermarks in Machine-Generated Language,"AlexeiGrinbaum, LaurynasAdomaitis",07-sep-22,Computation and Language (cs.CL)," Watermarks should be introduced in the natural language outputs of AI systems in order to maintain the distinction between human and machine- generated text. The ethical imperative to not blur this distinction arises from the asemantic nature of large language models and from human projections of emotional and cognitive states on machines, possibly leading to manipulation, spreading falsehoods or emotional distress. Enforcing this distinction requires unintrusive, yet easily accessible marks of the machine origin. We propose to implement a code based on equidistant letter sequences. While no such code exists in human-written texts, its appearance in machine-generated ones would prove helpful for ethical reasons.",https://arxiv.org/abs/2209.03121
A New Method for the High-Precision Assessment of Tumor Changes inResponse to Treatment,"P. D.Tar, N. A.Thacker, J.P.B.O'Connor",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Imaging demonstrates that preclinical and human tumors are heterogeneous, i.e. a single tumor can exhibit multiple regions that behave differently during both normal development and also in response to treatment. The large variations observed in control group tumors can obscure detection of significant therapeutic effects due to the ambiguity in attributing causes of change. This can hinder development of effective therapies due to limitations in experimental design, rather than due to therapeutic failure. An improved method to model biological variation and heterogeneity in imaging signals is described. Specifically, Linear Poisson modelling (LPM) evaluates changes in apparent diffusion co-efficient (ADC) before and 72 hours after radiotherapy, in two xenograft models of colorectal cancer. The statistical significance of measured changes are compared to those attainable using a conventional t-test analysis on basic ADC distribution parameters. When LPMs were applied to treated tumors, the LPMs detected highly significant changes. The analyses were significant for all tumors, equating to a gain in power of 4 fold (i.e. equivelent to having a sample size 16 times larger), compared with the conventional approach. In contrast, highly significant changes are only detected at a cohort level using t-tests, restricting their potential use within personalised medicine and increasing the number of animals required during testing. Furthermore, LPM enabled the relative volumes of responding and non-responding tissue to be estimated for each xenograft model. Leave-one-out analysis of the treated xenografts provided quality control and identified potential outliers, raising confidence in LPM data at clinically relevant sample sizes.",https://arxiv.org/abs/2209.03118
Inference and Learning for Generative Capsule Models,"AlfredoNazabal, NikolaosTsagkas, Christopher K. I.Williams",07-sep-22,Machine Learning (cs.LG)," Capsule networks (see e.g. Hinton et al., 2018) aim to encode knowledge of and reason about the relationship between an object and its parts. In this paper we specify a generative model for such data, and derive a variational algorithm for inferring the transformation of each model object in a scene, and the assignments of observed parts to the objects. We derive a learning algorithm for the object models, based on variational expectation maximization (Jordan et al., 1999). We also study an alternative inference algorithm based on the RANSAC method of Fischler and Bolles (1981). We apply these inference methods to (i) data generated from multiple geometric objects like squares and triangles (""constellations""), and (ii) data from a parts-based model of faces. Recent work by Kosiorek et al. (2019) has used amortized inference via stacked capsule autoencoders (SCAEs) to tackle this problem -- our results show that we significantly outperform them where we can make comparisons (on the constellations data).",https://arxiv.org/abs/2209.03116
Inference for Generative Capsule Models,"AlfredoNazabal, NikolaosTsagkas, Christopher K.I.Williams","11 Mar 2021 (v1(https://arxiv.org/abs/2103.06676v1)), lastrevised 14 Mar 2022 (this version, v2)",Machine Learning (cs.LG)," Capsule networks (see e.g. Hinton et al., 2018) aim to encode knowledge and reason about the relationship between an object and its parts. In this paper we specify a \emph{generative} model for such data, and derive a variational algorithm for inferring the transformation of each object and the assignments of observed parts to the objects. We apply this model to (i) data generated from multiple geometric objects like squares and triangles (""constellations""), and (ii) data from a parts-based model of faces. Recent work by Kosiorek et al. [2019] has used amortized inference via stacked capsule autoencoders (SCAEs) to tackle this problem -- our results show that we significantly outperform them where we can make comparisons (on the constellations data).",https://arxiv.org/abs/2209.03115
Multitask Learning via Shared Features: Algorithms and Hardness,"KonstantinaBairaktari, Guy Blanc, Li-Yang Tan, JonathanUllman, LydiaZakynthinou",07-sep-22,Machine Learning (cs.LG)," We investigate the computational efficiency of multitask learning of Boolean functions over the $d$-dimensional hypercube, that are related by means of a feature representation of size $k \ll d$ shared across all tasks. We present a polynomial time multitask learning algorithm for the concept class of halfspaces with margin $\gamma$, which is based on a simultaneous boosting technique and requires only $\textrm{poly}(k/\gamma)$ samples-per- task and $\textrm{poly}(k\log(d)/\gamma)$ samples in total.   In addition, we prove a computational separation, showing that assuming there exists a concept class that cannot be learned in the attribute- efficient model, we can construct another concept class such that can be learned in the attribute-efficient model, but cannot be multitask learned efficiently -- multitask learning this concept class either requires super- polynomial time complexity or a much larger total number of samples.",https://arxiv.org/abs/2103.06676
Open-Ended Evolution for Minecraft Building Generation,"MatthewBarthet, AntoniosLiapis, Georgios N.Yannakakis",07-sep-22,Machine Learning (cs.LG)," This paper proposes a procedural content generator which evolves Minecraft buildings according to an open-ended and intrinsic definition of novelty. To realize this goal we evaluate individuals' novelty in the latent space using a 3D autoencoder, and alternate between phases of exploration and transformation. During exploration the system evolves multiple populations of CPPNs through CPPN-NEAT and constrained novelty search in the latent space (defined by the current autoencoder). We apply a set of repair and constraint functions to ensure candidates adhere to basic structural rules and constraints during evolution. During transformation, we reshape the boundaries of the latent space to identify new interesting areas of the solution space by retraining the autoencoder with novel content. In this study we evaluate five different approaches for training the autoencoder during transformation and its impact on populations' quality and diversity during evolution. Our results show that by retraining the autoencoder we can achieve better open-ended complexity compared to a static model, which is further improved when retraining using larger datasets of individuals with diverse complexities.",https://arxiv.org/abs/2209.03112
MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection,"YangJiao, ZequnJie, ShaoxiangChen, JingjingChen, XiaolinWei, LinMa, Yu-GangJiang",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Fusing LiDAR and camera information is essential for achieving accurate and reliable 3D object detection in autonomous driving systems. However, this is challenging due to the difficulty of combining multi- granularity geometric and semantic features from two drastically different modalities. Recent approaches aim at exploring the semantic densities of camera features through lifting points in 2D camera images (referred to as seeds) into 3D space for fusion, and they can be roughly divided into 1) early fusion of raw points that aims at augmenting the 3D point cloud at the early input stage, and 2) late fusion of BEV (bird-eye view) maps that merges LiDAR and camera BEV features before the detection head. While both have their merits in enhancing the representation power of the combined features, this single-level fusion strategy is a suboptimal solution to the aforementioned challenge. Their major drawbacks are the inability to interact the multi-granularity semantic features from two distinct modalities sufficiently. To this end, we propose a novel framework that focuses on the multi-scale progressive interaction of the multi-granularity LiDAR and camera features. Our proposed method, abbreviated as MDMSFusion, achieves state-of-the-art results in 3D object detection, with 69.1 mAP and 71.8 NDS on nuScenes validation set, and 70.8 mAP and 73.2 NDS on nuScenes test set, which rank 1st and 2nd respectively among single-model non- ensemble approaches by the time of submission.",https://arxiv.org/abs/2209.03108
Effects of Archive Size on Computation Time and Solution Quality forMulti-Objective Optimization,"TianyeShu, KeShang, HisaoIshibuchi, Yang Nan",07-sep-22,Neural and Evolutionary Computing (cs.NE)," An unbounded external archive has been used to store all nondominated solutions found by an evolutionary multi-objective optimization algorithm in some studies. It has been shown that a selected solution subset from the stored solutions is often better than the final population. However, the use of the unbounded archive is not always realistic. When the number of examined solutions is huge, we must pre-specify the archive size. In this study, we examine the effects of the archive size on three aspects: (i) the quality of the selected final solution set, (ii) the total computation time for the archive maintenance and the final solution set selection, and (iii) the required memory size. Unsurprisingly, the increase of the archive size improves the final solution set quality. Interestingly, the total computation time of a medium-size archive is much larger than that of a small-size archive and a huge-size archive (e.g., an unbounded archive). To decrease the computation time, we examine two ideas: periodical archive update and archiving only in later generations. Compared with updating the archive at every generation, the first idea can obtain almost the same final solution set quality using a much shorter computation time at the cost of a slight increase of the memory size. The second idea drastically decreases the computation time at the cost of a slight deterioration of the final solution set quality. Based on our experimental results, some suggestions are given about how to appropriately choose an archiving strategy and an archive size.",https://arxiv.org/abs/2209.03102
Passive and Privacy-preserving Human Localization via mmWave AccessPoints for Social Distancing,"FrancescoDevoti, VincenzoSciancalepore, Xavier Costa-Perez",07-sep-22,Networking and Internet Architecture (cs.NI)," The pandemic outbreak has profoundly changed our life, especially our social habits and communication behaviors. While this dramatic shock has heavily impacted human interaction rules, novel localization techniques are emerging to help society in complying with new policies, such as social distancing. Wireless sensing and machine learning are well suited to alleviate viruses propagation in a privacy-preserving manner. However, its wide deployment requires cost-effective installation and operational solutions. In public environments, individual localization information-such as social distancing-needs to be monitored to avoid safety threats when not properly observed. To this end, the high penetration of wireless devices can be exploited to continuously analyze-and-learn the propagation environment, thereby passively detecting breaches and triggering alerts if required. In this paper, we describe a novel passive and privacy-preserving human localization solution that relies on the directive transmission properties of mmWave communications to monitor social distancing and notify people in the area in case of violations. Thus, addressing the social distancing challenge in a privacy-preserving and cost-efficient manner. Our solution provides an overall accuracy of about 99% in the tested scenarios.",https://arxiv.org/abs/2209.03100
"Physical configurations of a cell doublet with line tension, atheoretical study",FabriceDelbary,07-sep-22,Numerical Analysis (math.NA)," As a first approximation, early embryos may be modeled as foams whose shape depends on the surface tensions of each cell. However it has been remarked that exist line tensions at polarized exterior cellular interfaces (apical). In order to understand the changes it may imply on the usual foam model, a simple case study is considered: a double cell with line tension. Phase diagrams, bifurcations, possible new configurations are studied.",https://arxiv.org/abs/2209.03099
Obtaining Robust Control and Navigation Policies for Multi-RobotNavigation via Deep Reinforcement Learning,"ChristianJestel, HartmutSurmann, JonasStenzel, OliverUrbann, MariusBrehler",07-sep-22,Robotics (cs.RO)," Multi-robot navigation is a challenging task in which multiple robots must be coordinated simultaneously within dynamic environments. We apply deep reinforcement learning (DRL) to learn a decentralized end-to-end policy which maps raw sensor data to the command velocities of the agent. In order to enable the policy to generalize, the training is performed in different environments and scenarios. The learned policy is tested and evaluated in common multi-robot scenarios like switching a place, an intersection and a bottleneck situation. This policy allows the agent to recover from dead ends and to navigate through complex environments.",https://arxiv.org/abs/2209.03098
Hyperloop: A Cybersecurity Perspective,"AlessandroBrighente, Mauro Conti, DenisDonadel, FedericoTurrin",07-sep-22,Cryptography and Security (cs.CR)," Hyperloop is among the most prominent future transportation systems. First introduced by Elon Musk, Hyperloop concept involves novel technologies to allow traveling at a maximum speed of 1220km/h, while guaranteeing sustainability. Due to the system's performance requirements and the critical infrastructure it represents, its safety and security need to be carefully considered. In cyber-physical systems, cyberattacks could lead to safety issues with catastrophic consequences, both on the population and the surrounding environment. Therefore, the cybersecurity of all the components and links in Hyperloop represents a fundamental challenge. To this day, no research investigated the cyber security of the technology used for Hyperloop.   In this paper, we propose the first analysis of the cybersecurity challenges raised by Hyperloop technology. We base our analysis on the related works on Hyperloop, distilling the common features which will be likely to be present in the system. Furthermore, we provide an analysis of possible directions on the Hyperloop infrastructure management, together with their security concerns. Finally, we discuss possible countermeasures and future directions for the security of the future Hyperloop design.",https://arxiv.org/abs/2209.03097
Modular Federated Learning,"Kuo-YunLiang, AbhishekSrinivasan, Juan CarlosAndresen",07-sep-22,Machine Learning (cs.LG)," Federated learning is an approach to train machine learning models on the edge of the networks, as close as possible where the data is produced, motivated by the emerging problem of the inability to stream and centrally store the large amount of data produced by edge devices as well as by data privacy concerns. This learning paradigm is in need of robust algorithms to device heterogeneity and data heterogeneity. This paper proposes ModFL as a federated learning framework that splits the models into a configuration module and an operation module enabling federated learning of the individual modules. This modular approach makes it possible to extract knowlege from a group of heterogeneous devices as well as from non- IID data produced from its users. This approach can be viewed as an extension of the federated learning with personalisation layers FedPer framework that addresses data heterogeneity. We show that ModFL outperforms FedPer for non-IID data partitions of CIFAR-10 and STL-10 using CNNs. Our results on time-series data with HAPT, RWHAR, and WISDM datasets using RNNs remain inconclusive, we argue that the chosen datasets do not highlight the advantages of ModFL, but in the worst case scenario it performs as well as FedPer.",https://arxiv.org/abs/2209.03095
Decoding Demographic un-fairness from Indian Names,"MedidoddiVahini, JalendBantupalli, SouvicChakraborty, AnimeshMukherjee",07-sep-22,Computers and Society (cs.CY)," Demographic classification is essential in fairness assessment in recommender systems or in measuring unintended bias in online networks and voting systems. Important fields like education and politics, which often lay a foundation for the future of equality in society, need scrutiny to design policies that can better foster equality in resource distribution constrained by the unbalanced demographic distribution of people in the country.   We collect three publicly available datasets to train state-of-the-art classifiers in the domain of gender and caste classification. We train the models in the Indian context, where the same name can have different styling conventions (Jolly Abraham/Kumar Abhishikta in one state may be written as Abraham Jolly/Abishikta Kumar in the other). Finally, we also perform cross- testing (training and testing on different datasets) to understand the efficacy of the above models.   We also perform an error analysis of the prediction models. Finally, we attempt to assess the bias in the existing Indian system as case studies and find some intriguing patterns manifesting in the complex demographic layout of the sub-continent across the dimensions of gender and caste.",https://arxiv.org/abs/2209.03090
Autonomous Cooking with Digital Twin Methodology,"MaximilianKannapinn, MichaelSchÃ¤fer",07-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," This work introduces the concept of an autonomous cooking process based on Digital Twin method- ology. It proposes a hybrid approach of physics-based full order simulations followed by a data-driven system identification process with low errors. It makes faster-than-real-time simulations of Digital Twins feasible on a device level, without the need for cloud or high-performance computing. The concept is universally applicable to various physical processes.",https://arxiv.org/abs/2209.03089
Deployment of Aerial Robots during the Flood Disaster in Erftstadt /Blessem in July 2021,"HartmutSurmann, DominikSlomma, RobertGrafe, StefanGrobelny",07-sep-22,Robotics (cs.RO)," Climate change is leading to more and more extreme weather events such as heavy rainfall and flooding. This technical report deals with the question of how rescue commanders can be better and faster provided with current information during flood disasters using Unmanned Aerial Vehicles (UAVs), i.e. during the flood in July 2021 in Central Europe, more specifically in Erftstadt / Blessem. The UAVs were used for live observation and regular inspections of the flood edge on the one hand, and on the other hand for the systematic data acquisition in order to calculate 3D models using Structure from Motion and MultiView Stereo. The 3D models embedded in a GIS application serve as a planning basis for the systematic exploration and decision support for the deployment of additional smaller UAVs but also rescue forces. The systematic data acquisition of the UAVs by means of autonomous meander flights provides high-resolution images which are computed to a georeferenced 3D model of the surrounding area within 15 minutes in a specially equipped robotic command vehicle (RobLW). From the comparison of high-resolution elevation profiles extracted from the 3D model on successive days, changes in the water level become visible. This information enables the emergency management to plan further inspections of the buildings and to search for missing persons on site.",https://arxiv.org/abs/2209.03087
Interactive Visual Analysis of Structure-borne Noise Data,"RainerSplechtna, DenisGracanin, GoranTodorovic, StanislavGoja, BorisBedic, HelwigHauser, KresimirMatkovic",07-sep-22,Graphics (cs.GR)," Numerical simulation has become omnipresent in the automotive domain, posing new challenges such as high-dimensional parameter spaces and large as well as incomplete and multi-faceted data. In this design study, we show how interactive visual exploration and analysis of high-dimensional, spectral data from noise simulation can facilitate design improvements in the context of conflicting criteria. Here, we focus on structure-borne noise, i.e., noise from vibrating mechanical parts. Detecting problematic noise sources early in the design and production process is essential for reducing a product's development costs and its time to market. In a close collaboration of visualization and automotive engineering, we designed a new, interactive approach to quickly identify and analyze critical noise sources, also contributing to an improved understanding of the analyzed system. Several carefully designed, interactive linked views enable the exploration of noises, vibrations, and harshness at multiple levels of detail, both in the frequency and spatial domain. This enables swift and smooth changes of perspective; selections in the frequency domain are immediately reflected in the spatial domain, and vice versa. Noise sources are quickly identified and shown in the context of their neighborhood, both in the frequency and spatial domain. We propose a novel drill-down view, especially tailored to noise data analysis. Split boxplots and synchronized 3D geometry views support comparison tasks. With this solution, engineers iterate over design optimizations much faster, while maintaining a good overview at each iteration. We evaluated the new approach in the automotive industry, studying noise simulation data for an internal combustion engine.",https://arxiv.org/abs/2209.03084
Near-Field Beamforming and Multiplexing Using Extremely Large ApertureArrays,"ParisaRamezani, EmilBjÃ¶rnson",07-sep-22,Information Theory (cs.IT)," The number of users that can be spatially multiplexed by a wireless access point depends on the aperture of its antenna array. When the aperture increases and wavelength shrinks, ""new"" electromagnetic phenomena can be utilized to further enhance network capacity. In this chapter, we describe how extremely large aperture arrays (ELAA) can extend the radiative near-field region to kilometer distances. We demonstrate how this affects the propagation models in line-of-sight (LoS) scenarios and enables finite- depth beamforming. In particular, it becomes possible to simultaneously serve users that are located in the same direction but at different distances.",https://arxiv.org/abs/2209.03083
Plant Species Classification Using Transfer Learning by PretrainedClassifier VGG-19,"ThiruSiddharth, Bhupendra SinghKirar, Dheeraj KumarAgrawal",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Deep learning is currently the most important branch of machine learning, with applications in speech recognition, computer vision, image classification, and medical imaging analysis. Plant recognition is one of the areas where image classification can be used to identify plant species through their leaves. Botanists devote a significant amount of time to recognizing plant species by personally inspecting. This paper describes a method for dissecting color images of Swedish leaves and identifying plant species. To achieve higher accuracy, the task is completed using transfer learning with the help of pre-trained classifier VGG-19. The four primary processes of classification are image preprocessing, image augmentation, feature extraction, and recognition, which are performed as part of the overall model evaluation. The VGG-19 classifier grasps the characteristics of leaves by employing pre-defined hidden layers such as convolutional layers, max pooling layers, and fully connected layers, and finally uses the soft-max layer to generate a feature representation for all plant classes. The model obtains knowledge connected to aspects of the Swedish leaf dataset, which contains fifteen tree classes, and aids in predicting the proper class of an unknown plant with an accuracy of 99.70% which is higher than previous research works reported.",https://arxiv.org/abs/2209.03082
State of Security Awareness in the AM Industry: 2020 Survey,"MarkYampolskiy, Paul Bates, MohsenSeifi, NimaShamsaei",07-sep-22,Cryptography and Security (cs.CR)," Security of Additive Manufacturing (AM) gets increased attention due to the growing proliferation and adoption of AM in a variety of applications and business models. However, there is a significant disconnect between AM community focused on manufacturing and AM Security community focused on securing this highly computerized manufacturing technology. To bridge this gap, we surveyed the America Makes AM community, asking in total eleven AM security-related questions aiming to discover the existing concerns, posture, and expectations. The first set of questions aimed to discover how many of these organizations use AM, outsource AM, or provide AM as a service. Then we asked about biggest security concerns as well as about assessment of who the potential adversaries might be and their motivation for attack. We then proceeded with questions on any experienced security incidents, if any security risk assessment was conducted, and if the participants' organizations were partnering with external experts to secure AM. Lastly, we asked whether security measures are implemented at all and, if yes, whether they fall under the general cyber-security category. Out of 69 participants affiliated with commercial industry, agencies, and academia, 53 have completed the entire survey. This paper presents the results of this survey, as well as provides our assessment of the AM Security posture. The answers are a mixture of what we could label as expected, ""shocking but not surprising,"" and completely unexpected. Assuming that the provided answers are somewhat representative to the current state of the AM industry, we conclude that the industry is not ready to prevent or detect AM-specific attacks that have been demonstrated in the research literature.",https://arxiv.org/abs/2209.03076
On Plane Subgraphs of Complete Topological Drawings,"AlfredoGarcÃ­a, AlexanderPilz, JavierTejel",07-sep-22,Computational Geometry (cs.CG)," Topological drawings are representations of graphs in the plane, where vertices are represented by points, and edges by simple curves connecting the points. A drawing is simple if two edges intersect at most in a single point, either at a common endpoint or at a proper crossing. In this paper we study properties of maximal plane subgraphs of simple drawings $D_n$ of the complete graph $K_n$ on $n$ vertices. Our main structural result is that maximal plane subgraphs are 2-connected and what we call essentially 3-edge-connected. Besides, any maximal plane subgraph contains at least $\lceil 3n/2 \rceil$ edges. We also address the problem of obtaining a plane subgraph of $D_n$ with the maximum number of edges, proving that this problem is NP-complete. However, given a plane spanning connected subgraph of $D_n$, a maximum plane augmentation of this subgraph can be found in $O(n^3)$ time. As a side result, we also show that the problem of finding a largest compatible plane straight-line graph of two labeled point sets is NP-complete.",https://arxiv.org/abs/2209.03073
Legal Detection of AI Products Based on Formal Argumentation and LegalOntology,"Zhe Yu, Yiwei Lu",07-sep-22,Artificial Intelligence (cs.AI)," Ontology is a popular method for knowledge representation in different domains, including the legal domain, and description logics (DL) is commonly used as its description language. To handle reasoning based on inconsistent DL-based legal ontologies, the current paper presents a structured argumentation framework particularly for reasoning in legal contexts on the basis of ASPIC+, and translates the legal ontology into formulas and rules of an argumentation theory. With a particular focus on the design of autonomous vehicles from the perspective of legal AI, we show that using this combined theory of formal argumentation and DL-based legal ontology, acceptable assertions can be obtained based on inconsistent ontologies, and the traditional reasoning tasks of DL ontologies can also be accomplished. In addition, a formal definition of explanations for the result of reasoning is presented.",https://arxiv.org/abs/2209.03072
A Review on the Process of Automated Software Testing,"Durga ShreeN, SreeDharinya S, DasariVijayasree, Nadendla SaiRoopa, Anugu Arun",07-sep-22,Software Engineering (cs.SE)," The requirements in automation, digitalization, and fast computations have loaded the IT sector with expectations of highly reliable, efficient, and cost-effective software. Given that the process of testing, verification, and validation of software products consumes 50-75% of the total revenue if the testing process is ineffective, ""n"" times the expenditure must be invested to mend the havoc caused. A delay in project completion is often attributed to the testing phase because of the numerous cycles of debugging process. The software testing process determines the face of the product released to the user. It sets the standard and reliability of a company's outputs. As the complexity increases, testing gets intense so as to examine all the outliers and various branches of the processing flow. The testing process is automated using software tools to avoid the tedious manual process of test input generation and validation criteria, which certifies the program only to a certain confidence level in the presence of outliers.",https://arxiv.org/abs/2209.03070
A Review of Resource Management in Fog Computing: Machine LearningPerspective,"MuhammadFahimullah, ShohrehAhvar, MariaTrocan",07-sep-22,Networking and Internet Architecture (cs.NI)," Fog computing becomes a promising technology to process user's requests near the proximity of users to reduce response time for latency- sensitive requests. Despite its advantages, the properties such as resource heterogeneity and limitations, and its dynamic and unpredictable nature greatly reduce the efficiency of fog computing. Therefore, predicting the dynamic behavior of the fog and managing resources accordingly is of utmost importance. In this work, we provide a review of machine learning-based predictive resource management approaches in a fog environment. Resource management is classified into six sub-areas: resource provisioning, application placement, scheduling, resource allocation, task offloading, and load balancing. Reviewed resource management approaches are analyzed based on the objective metrics, tools, datasets, and utilized techniques.",https://arxiv.org/abs/2209.03069
MimCo: Masked Image Modeling Pre-training with Contrastive Teacher,"QiangZhou, ChaohuiYu, HaoLuo, ZhibinWang, HaoLi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Recent masked image modeling (MIM) has received much attention in self-supervised learning (SSL), which requires the target model to recover the masked part of the input image. Although MIM-based pre-training methods achieve new state-of-the-art performance when transferred to many downstream tasks, the visualizations show that the learned representations are less separable, especially compared to those based on contrastive learning pre- training. This inspires us to think whether the linear separability of MIM pre-trained representation can be further improved, thereby improving the pre-training performance. Since MIM and contrastive learning tend to utilize different data augmentations and training strategies, combining these two pretext tasks is not trivial. In this work, we propose a novel and flexible pre-training framework, named MimCo, which combines MIM and contrastive learning through two-stage pre-training. Specifically, MimCo takes a pre- trained contrastive learning model as the teacher model and is pre-trained with two types of learning targets: patch-level and image-level reconstruction losses.   Extensive transfer experiments on downstream tasks demonstrate the superior performance of our MimCo pre-training framework. Taking ViT-S as an example, when using the pre-trained MoCov3-ViT-S as the teacher model, MimCo only needs 100 epochs of pre-training to achieve 82.53% top-1 finetuning accuracy on Imagenet-1K, which outperforms the state-of-the-art self-supervised learning counterparts.",https://arxiv.org/abs/2209.03066
"Physics-based Digital Twins for Autonomous Thermal Food Processing:Efficient, Non-intrusive Reduced-order Modeling","MaximilianKannapinn, Minh KhangPham, MichaelSchÃ¤fer",07-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," One possible way of making thermal processing controllable is to gather real-time information on the product's current state. Often, sensory equipment cannot capture all relevant information easily or at all. Digital Twins close this gap with virtual probes in real-time simulations, synchronized with the process. This paper proposes a physics-based, data- driven Digital Twin framework for autonomous food processing. We suggest a lean Digital Twin concept that is executable at the device level, entailing minimal computational load, data storage, and sensor data requirements. This study focuses on a parsimonious experimental design for training non- intrusive reduced-order models (ROMs) of a thermal process. A correlation ($R=-0.76$) between a high standard deviation of the surface temperatures in the training data and a low root mean square error in ROM testing enables efficient selection of training data. The mean test root mean square error of the best ROM is less than 1 Kelvin (0.2 % mean average percentage error) on representative test sets. Simulation speed-ups of Sp $\approx$ 1.8E4 allow on-device model predictive control.   The proposed Digital Twin framework is designed to be applicable within the industry. Typically, non-intrusive reduced-order modeling is required as soon as the modeling of the process is performed in software, where root- level access to the solver is not provided, such as commercial simulation software. The data-driven training of the reduced-order model is achieved with only one data set, as correlations are utilized to predict the training success a priori.",https://arxiv.org/abs/2209.03063
Parallel and Streaming Wavelet Neural Networks for Classification andRegression under Apache Spark,"Eduru HarindraVenkatesh, YelletiVivek, VadlamaniRavi, OrsuShivaShankar",07-sep-22,Neural and Evolutionary Computing (cs.NE)," Wavelet neural networks (WNN) have been applied in many fields to solve regression as well as classification problems. After the advent of big data, as data gets generated at a brisk pace, it is imperative to analyze it as soon as it is generated owing to the fact that the nature of the data may change dramatically in short time intervals. This is necessitated by the fact that big data is all pervasive and throws computational challenges for data scientists. Therefore, in this paper, we built an efficient Scalable, Parallelized Wavelet Neural Network (SPWNN) which employs the parallel stochastic gradient algorithm (SGD) algorithm. SPWNN is designed and developed under both static and streaming environments in the horizontal parallelization framework. SPWNN is implemented by using Morlet and Gaussian functions as activation functions. This study is conducted on big datasets like gas sensor data which has more than 4 million samples and medical research data which has more than 10,000 features, which are high dimensional in nature. The experimental analysis indicates that in the static environment, SPWNN with Morlet activation function outperformed SPWNN with Gaussian on the classification datasets. However, in the case of regression, the opposite was observed. In contrast, in the streaming environment i.e., Gaussian outperformed Morlet on the classification and Morlet outperformed Gaussian on the regression datasets. Overall, the proposed SPWNN architecture achieved a speedup of 1.32-1.40.",https://arxiv.org/abs/2209.03062
A Local Search Algorithm for the Min-Sum Submodular Cover Problem,"LisaHellerstein, ThomasLidbetter, R. TealWitter",07-sep-22,Data Structures and Algorithms (cs.DS)," We consider the problem of solving the Min-Sum Submodular Cover problem using local search. The Min-Sum Submodular Cover problem generalizes the NP-complete Min-Sum Set Cover problem, replacing the input set cover instance with a monotone submodular set function. A simple greedy algorithm achieves an approximation factor of 4, which is tight unless P=NP [Streeter and Golovin, NeurIPS, 2008]. We complement the greedy algorithm with analysis of a local search algorithm. Building on work of Munagala et al. [ICDT, 2005], we show that, using simple initialization, a straightforward local search algorithm achieves a $(4+\epsilon)$-approximate solution in time $O(n^3\log(n/\epsilon))$, provided that the monotone submodular set function is also second-order supermodular. Second-order supermodularity has been shown to hold for a number of submodular functions of practical interest, including functions associated with set cover, matching, and facility location. We present experiments on two special cases of Min-Sum Submodular Cover and find that the local search algorithm can outperform the greedy algorithm on small data sets.",https://arxiv.org/abs/2209.03056
Cerberus: Exploring Federated Prediction of Security Events,"MohammadNaseri, Yufei Han, EnricoMariconti, Yun Shen, GianlucaStringhini, Emiliano DeCristofaro",07-sep-22,Cryptography and Security (cs.CR)," Modern defenses against cyberattacks increasingly rely on proactive approaches, e.g., to predict the adversary's next actions based on past events. Building accurate prediction models requires knowledge from many organizations; alas, this entails disclosing sensitive information, such as network structures, security postures, and policies, which might often be undesirable or outright impossible. In this paper, we explore the feasibility of using Federated Learning (FL) to predict future security events. To this end, we introduce Cerberus, a system enabling collaborative training of Recurrent Neural Network (RNN) models for participating organizations. The intuition is that FL could potentially offer a middle- ground between the non-private approach where the training data is pooled at a central server and the low-utility alternative of only training local models. We instantiate Cerberus on a dataset obtained from a major security company's intrusion prevention product and evaluate it vis-a-vis utility, robustness, and privacy, as well as how participants contribute to and benefit from the system. Overall, our work sheds light on both the positive aspects and the challenges of using FL for this task and paves the way for deploying federated approaches to predictive security.",https://arxiv.org/abs/2209.03054
Numerical integration rules with improved accuracy close tosingularities,"SergioAmat, Zhilin Li, Juan Ruiz-Alvarez, ConcepcionSolano, Juan C.Trillo","7 Sep 2022 (v1(https://arxiv.org/abs/2209.03049v1)), lastrevised 8 Sep 2022 (this version, v2)",Numerical Analysis (math.NA)," Sometimes it is necessary to obtain a numerical integration using only discretised data. In some cases, the data contains singularities which position is known but does not coincide with a discretisation point, and the jumps in the function and its derivatives are available at these positions. The motivation of this paper is to use the previous information to obtain numerical quadrature formulas that allow approximating the integral of the discrete data over certain intervals accurately. This work is devoted to the construction and analysis of a new nonlinear technique that allows to obtain accurate numerical integrations of any order using data that contains singularities, and when the integrand is only known at grid points. The novelty of the technique consists in the inclusion of correction terms with a closed expression that depends on the size of the jumps of the function and its derivatives at the singularities, that are supposed to be known. The addition of these terms allows recovering the accuracy of classical numerical integration formulas even close to the singularities, as these correction terms account for the error that the classical integration formulas commit up to their accuracy at smooth zones. Thus, the correction terms can be added during the integration or as post-processing, which is useful if the main calculation of the integral has been already done using classical formulas. The numerical experiments performed allow us to confirm the theoretical conclusions reached in this paper.",https://arxiv.org/abs/2209.03050
Benchmarking Multimodal Variational Autoencoders: GeBiD Dataset andToolkit,"GabrielaSejnova, MichalVavrecka, KarlaStepanova",07-sep-22,Machine Learning (cs.LG)," Multimodal Variational Autoencoders (VAEs) have been a subject of intense research in the past years as they can integrate multiple modalities into a joint representation and can thus serve as a promising tool for both data classification and generation. Several approaches toward multimodal VAE learning have been proposed so far, their comparison and evaluation have however been rather inconsistent. One reason is that the models differ at the implementation level, another problem is that the datasets commonly used in these cases were not initially designed for the evaluation of multimodal generative models. This paper addresses both mentioned issues. First, we propose a toolkit for systematic multimodal VAE training and comparison. Second, we present a synthetic bimodal dataset designed for a comprehensive evaluation of the joint generation and cross-generation capabilities. We demonstrate the utility of the dataset by comparing state-of-the-art models.",https://arxiv.org/abs/2209.03049
Multi-Scale Attention-based Multiple Instance Learning forClassification of Multi-Gigapixel Histology Images,"Made SatriaWibawa, Kwok-Wai Lo, LawrenceYoung, NasirRajpoot",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Histology images with multi-gigapixel of resolution yield rich information for cancer diagnosis and prognosis. Most of the time, only slide-level label is available because pixel-wise annotation is labour intensive task. In this paper, we propose a deep learning pipeline for classification in histology images. Using multiple instance learning, we attempt to predict the latent membrane protein 1 (LMP1) status of nasopharyngeal carcinoma (NPC) based on haematoxylin and eosin-stain (H&E) histology images. We utilised attention mechanism with residual connection for our aggregation layers. In our 3-fold cross-validation experiment, we achieved average accuracy, AUC and F1-score 0.936, 0.995 and 0.862, respectively. This method also allows us to examine the model interpretability by visualising attention scores. To the best of our knowledge, this is the first attempt to predict LMP1 status on NPC using deep learning.",https://arxiv.org/abs/2209.03048
Not All Instances Contribute Equally: Instance-adaptive ClassRepresentation Learning for Few-Shot Visual Recognition,"MengyaHan, YibingZhan, YongLuo, BoDu, HanHu, YonggangWen, DachengTao",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Few-shot visual recognition refers to recognize novel visual concepts from a few labeled instances. Many few-shot visual recognition methods adopt the metric-based meta-learning paradigm by comparing the query representation with class representations to predict the category of query instance. However, current metric-based methods generally treat all instances equally and consequently often obtain biased class representation, considering not all instances are equally significant when summarizing the instance-level representations for the class-level representation. For example, some instances may contain unrepresentative information, such as too much background and information of unrelated concepts, which skew the results. To address the above issues, we propose a novel metric-based meta- learning framework termed instance-adaptive class representation learning network (ICRL-Net) for few-shot visual recognition. Specifically, we develop an adaptive instance revaluing network with the capability to address the biased representation issue when generating the class representation, by learning and assigning adaptive weights for different instances according to their relative significance in the support set of corresponding class. Additionally, we design an improved bilinear instance representation and incorporate two novel structural losses, i.e., intra-class instance clustering loss and inter-class representation distinguishing loss, to further regulate the instance revaluation process and refine the class representation. We conduct extensive experiments on four commonly adopted few-shot benchmarks: miniImageNet, tieredImageNet, CIFAR-FS, and FC100 datasets. The experimental results compared with the state-of-the-art approaches demonstrate the superiority of our ICRL-Net.",https://arxiv.org/abs/2209.03041
Machine Learning Students Overfit to Overfitting,"Matias Valdenegro-Toro, MatthiaSabatelli",07-sep-22,Machine Learning (cs.LG)," Overfitting and generalization is an important concept in Machine Learning as only models that generalize are interesting for general applications. Yet some students have trouble learning this important concept through lectures and exercises. In this paper we describe common examples of students misunderstanding overfitting, and provide recommendations for possible solutions. We cover student misconceptions about overfitting, about solutions to overfitting, and implementation mistakes that are commonly confused with overfitting issues. We expect that our paper can contribute to improving student understanding and lectures about this important topic.",https://arxiv.org/abs/2209.03034
SIRA: Relightable Avatars from a Single Image,"PolCaselles, EduardRamon, JaimeGarcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer, GilTriginer",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Recovering the geometry of a human head from a single image, while factorizing the materials and illumination is a severely ill-posed problem that requires prior information to be solved. Methods based on 3D Morphable Models (3DMM), and their combination with differentiable renderers, have shown promising results. However, the expressiveness of 3DMMs is limited, and they typically yield over-smoothed and identity-agnostic 3D shapes limited to the face region. Highly accurate full head reconstructions have recently been obtained with neural fields that parameterize the geometry using multilayer perceptrons. The versatility of these representations has also proved effective for disentangling geometry, materials and lighting. However, these methods require several tens of input images. In this paper, we introduce SIRA, a method which, from a single image, reconstructs human head avatars with high fidelity geometry and factorized lights and surface materials. Our key ingredients are two data-driven statistical models based on neural fields that resolve the ambiguities of single-view 3D surface reconstruction and appearance factorization. Experiments show that SIRA obtains state of the art results in 3D head reconstruction while at the same time it successfully disentangles the global illumination, and the diffuse and specular albedos. Furthermore, our reconstructions are amenable to physically-based appearance editing and head model relighting.",https://arxiv.org/abs/2209.03032
Ultra-low-power Range Error Mitigation for Ultra-wideband PreciseLocalization,"SimoneAngarano, FrancescoSalvetti, VittorioMazzia, GiovanniFantin, DarioGandini, MarcelloChiaberge",07-sep-22,Machine Learning (cs.LG)," Precise and accurate localization in outdoor and indoor environments is a challenging problem that currently constitutes a significant limitation for several practical applications. Ultra-wideband (UWB) localization technology represents a valuable low-cost solution to the problem. However, non-line-of-sight (NLOS) conditions and complexity of the specific radio environment can easily introduce a positive bias in the ranging measurement, resulting in highly inaccurate and unsatisfactory position estimation. In the light of this, we leverage the latest advancement in deep neural network optimization techniques and their implementation on ultra-low-power microcontrollers to introduce an effective range error mitigation solution that provides corrections in either NLOS or LOS conditions with a few mW of power. Our extensive experimentation endorses the advantages and improvements of our low-cost and power-efficient methodology.",https://arxiv.org/abs/2209.03027
Multilevel Path Branching for Digital Options,"Michael B.Giles, Abdul-Lateef Haji-Ali",07-sep-22,Numerical Analysis (math.NA), We propose a new Monte Carlo-based estimator for digital options with assets modelled by a stochastic differential equation (SDE). The new estimator is based on repeated path splitting and relies on the correlation of approximate paths of the underlying SDE that share parts of a Brownian path. Combining this new estimator with Multilevel Monte Carlo (MLMC) leads to an estimator with a complexity that is similar to the complexity of a MLMC estimator when applied to options with Lipschitz payoffs.,https://arxiv.org/abs/2209.03021
Text Growing on Leaf,"Chuang.Yang, Mulin.Chen, Yuan.Yuan, Qi.Wang",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Irregular-shaped texts bring challenges to Scene Text Detection (STD). Although existing contour point sequence-based approaches achieve comparable performances, they fail to cover some highly curved ribbon-like text lines. It leads to limited text fitting ability and STD technique application. Considering the above problem, we combine text geometric characteristics and bionics to design a natural leaf vein-based text representation method (LVT). Concretely, it is found that leaf vein is a generally directed graph, which can easily cover various geometries. Inspired by it, we treat text contour as leaf margin and represent it through main, lateral, and thin veins. We further construct a detection framework based on LVT, namely LeafText. In the text reconstruction stage, LeafText simulates the leaf growth process to rebuild text contour. It grows main vein in Cartesian coordinates to locate text roughly at first. Then, lateral and thin veins are generated along the main vein growth direction in polar coordinates. They are responsible for generating coarse contour and refining it, respectively. Considering the deep dependency of lateral and thin veins on main vein, the Multi-Oriented Smoother (MOS) is proposed to enhance the robustness of main vein to ensure a reliable detection result. Additionally, we propose a global incentive loss to accelerate the predictions of lateral and thin veins. Ablation experiments demonstrate LVT is able to depict arbitrary-shaped texts precisely and verify the effectiveness of MOS and global incentive loss. Comparisons show that LeafText is superior to existing state-of-the-art (SOTA) methods on MSRA- TD500, CTW1500, Total-Text, and ICDAR2015 datasets.",https://arxiv.org/abs/2209.03017
Zoom Text Detector,"Chuang.Yang, Mulin.Chen, Yuan.Yuan, Qi.Wang",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," To pursue comprehensive performance, recent text detectors improve detection speed at the expense of accuracy. They adopt shrink-mask based text representation strategies, which leads to a high dependency of detection accuracy on shrink-masks. Unfortunately, three disadvantages cause unreliable shrink-masks. Specifically, these methods try to strengthen the discrimination of shrink-masks from the background by semantic information. However, the feature defocusing phenomenon that coarse layers are optimized by fine-grained objectives limits the extraction of semantic features. Meanwhile, since both shrink-masks and the margins belong to texts, the detail loss phenomenon that the margins are ignored hinders the distinguishment of shrink-masks from the margins, which causes ambiguous shrink-mask edges. Moreover, false-positive samples enjoy similar visual features with shrink-masks. They aggravate the decline of shrink-masks recognition. To avoid the above problems, we propose a Zoom Text Detector (ZTD) inspired by the zoom process of the camera. Specifically, Zoom Out Module (ZOM) is introduced to provide coarse-grained optimization objectives for coarse layers to avoid feature defocusing. Meanwhile, Zoom In Module (ZIM) is presented to enhance the margins recognition to prevent detail loss. Furthermore, Sequential-Visual Discriminator (SVD) is designed to suppress false-positive samples by sequential and visual features. Experiments verify the superior comprehensive performance of ZTD.",https://arxiv.org/abs/2209.03016
Quantitative probing: Validating causal models using quantitativedomain knowledge,"DanielGrÃ¼nbaum, Maike L.Stern, Elmar W.Lang",07-sep-22,Machine Learning (cs.LG)," We present quantitative probing as a model-agnostic framework for validating causal models in the presence of quantitative domain knowledge. The method is constructed as an analogue of the train/test split in correlation-based machine learning and as an enhancement of current causal validation strategies that are consistent with the logic of scientific discovery. The effectiveness of the method is illustrated using Pearl's sprinkler example, before a thorough simulation-based investigation is conducted. Limits of the technique are identified by studying exemplary failing scenarios, which are furthermore used to propose a list of topics for future research and improvements of the presented version of quantitative probing. The code for integrating quantitative probing into causal analysis, as well as the code for the presented simulation-based studies of the effectiveness of quantitative probing is provided in two separate open-source Python packages.",https://arxiv.org/abs/2209.03014
Biblio-Analysis of Cohort Intelligence (CI) Algorithm and its alliedapplications from Scopus and Web of Science Perspective,"IshaanKale, RahulJoshi, KalyaniKadam",07-sep-22,Digital Libraries (cs.DL)," Cohort Intelligence or CI is one of its kind of novel optimization algorithm. Since its inception, in a very short span it is applied successfully in various domains and its results are observed to be effectual in contrast to algorithm of its kind. Till date, there is no such type of bibliometric analysis carried out on CI and its related applications. So, this research paper in a way will be an ice breaker for those who want to take up CI to a new level. In this research papers, CI publications available in Scopus are analyzed through graphs, networked diagrams about authors, source titles, keywords over the years, journals over the time. In a way this bibliometric paper showcase CI, its applications and detail outs systematic review in terms its bibliometric details.",https://arxiv.org/abs/2209.03013
Knowledge-enhanced Iterative Instruction Generation and Reasoning forKnowledge Base Question Answering,"HaoweiDu, QuzheHuang, ChenZhang, Dongyan Zhao",07-sep-22,Computation and Language (cs.CL)," Multi-hop Knowledge Base Question Answering(KBQA) aims to find the answer entity in a knowledge base which is several hops from the topic entity mentioned in the question. Existing Retrieval-based approaches first generate instructions from the question and then use them to guide the multi-hop reasoning on the knowledge graph. As the instructions are fixed during the whole reasoning procedure and the knowledge graph is not considered in instruction generation, the model cannot revise its mistake once it predicts an intermediate entity incorrectly. To handle this, we propose KBIGER(Knowledge Base Iterative Instruction GEnerating and Reasoning), a novel and efficient approach to generate the instructions dynamically with the help of reasoning graph. Instead of generating all the instructions before reasoning, we take the (k-1)-th reasoning graph into consideration to build the k-th instruction. In this way, the model could check the prediction from the graph and generate new instructions to revise the incorrect prediction of intermediate entities. We do experiments on two multi-hop KBQA benchmarks and outperform the existing approaches, becoming the new-state-of-the-art. Further experiments show our method does detect the incorrect prediction of intermediate entities and has the ability to revise such errors.",https://arxiv.org/abs/2209.03009
Flow Straight and Fast: Learning to Generate and Transfer Data withRectified Flow,"XingchaoLiu, ChengyueGong, QiangLiu",07-sep-22,Machine Learning (cs.LG)," We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions \pi_0 and \pi_1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from \pi_0 and \pi_1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of \pi_0 and \pi_1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.",https://arxiv.org/abs/2209.03005
Optimizing Demonstrated Robot Manipulation Skills for Temporal LogicConstraints,"AkshayDhonthi, PhilippSchillinger, Leonel Rozo, DanieleNardi",07-sep-22,Robotics (cs.RO)," For performing robotic manipulation tasks, the core problem is determining suitable trajectories that fulfill the task requirements. Various approaches to compute such trajectories exist, being learning and optimization the main driving techniques. Our work builds on the learning- from-demonstration (LfD) paradigm, where an expert demonstrates motions, and the robot learns to imitate them. However, expert demonstrations are not sufficient to capture all sorts of task specifications, such as the timing to grasp an object. In this paper, we propose a new method that considers formal task specifications within LfD skills. Precisely, we leverage Signal Temporal Logic (STL), an expressive form of temporal properties of systems, to formulate task specifications and use black-box optimization (BBO) to adapt an LfD skill accordingly. We demonstrate our approach in simulation and on a real industrial setting using several tasks that showcase how our approach addresses the LfD limitations using STL and BBO.",https://arxiv.org/abs/2209.03003
On the Transferability of Adversarial Examples between EncryptedModels,"MikiTanaka, IsaoEchizen, Hitoshi Kiya",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In addition, AEs have adversarial transferability, namely, AEs generated for a source model fool other (target) models. In this paper, we investigate the transferability of models encrypted for adversarially robust defense for the first time. To objectively verify the property of transferability, the robustness of models is evaluated by using a benchmark attack method, called AutoAttack. In an image-classification experiment, the use of encrypted models is confirmed not only to be robust against AEs but to also reduce the influence of AEs in terms of the transferability of models.",https://arxiv.org/abs/2209.03001
From Human Walking to Bipedal Robot Locomotion: Reflex InspiredCompensation on Planned and Unplanned Downsteps,"JorisVerhagen, XiaobinXiong, AaronAmes, AjaySeth",07-sep-22,Robotics (cs.RO)," Humans are able to negotiate downstep behaviors -- both planned and unplanned \-- with remarkable agility and ease. The goal of this paper is to systematically study the translation of this human behavior to bipedal walking robots, even if the morphology is inherently different. Concretely, we begin with human data wherein planned and unplanned downsteps are taken. We analyze this data from the perspective of reduced-order modeling of the human, encoding the center of mass (CoM) kinematics and contact forces, which allows for the translation of these behaviors into the corresponding reduced-order model of a bipedal robot. We embed the resulting behaviors into the full-order dynamics of a bipedal robot via nonlinear optimization- based controllers. The end result is the demonstration of planned and unplanned downsteps in simulation on an underactuated walking robot.",https://arxiv.org/abs/2209.02997
Robust Numerical Methods for Singularly Perturbed DifferentialEquations--Supplements,Hans-GÃ¶rgRoos,07-sep-22,Numerical Analysis (math.NA)," The second edition of the book ""Roos, Stynes, Tobiska -- Robust Numerical Methods for Singularly Perturbed Differential Equations"" appeared many years ago and was for many years a reliable guide into the world of numerical methods for singularly perturbed problems. Since then many new results came into the game, we present some selected ones and the related sources.",https://arxiv.org/abs/2209.02995
Layer-adapted meshes: Milestones in 50 years of history,Hans-GoergRoos,18-sep-19,Numerical Analysis (math.NA), 50 years ago the first paper on layer-adapted meshes appeared. We sketch the development in all these years with special emphasis on important ideas.,https://arxiv.org/abs/2209.02994
Remarks on boundary layers in singularly perturbed Caputo fractionalboundary value problems,Hans-GÃ¶rgRoos,07-sep-22,Numerical Analysis (math.NA), Almost nothing is known about the layer structure of solutions to singularly perturbed Caputo fractional boundary value problems. We discuss simple convection-diffusion and reaction-diffusion problems.,https://arxiv.org/abs/1909.08273
Auto-TransRL: Autonomous Composition of Vision Pipelines for RoboticPerception,"AdityaKapoor, NijilGeorge, VartikaSengar, VighneshVatsal, JayavardhanaGubbi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Creating a vision pipeline for different datasets to solve a computer vision task is a complex and time consuming process. Currently, these pipelines are developed with the help of domain experts. Moreover, there is no systematic structure to construct a vision pipeline apart from relying on experience, trial and error or using template-based approaches. As the search space for choosing suitable algorithms for achieving a particular vision task is large, human exploration for finding a good solution requires time and effort. To address the following issues, we propose a dynamic and data-driven way to identify an appropriate set of algorithms that would be fit for building the vision pipeline in order to achieve the goal task. We introduce a Transformer Architecture complemented with Deep Reinforcement Learning to recommend algorithms that can be incorporated at different stages of the vision workflow. This system is both robust and adaptive to dynamic changes in the environment. Experimental results further show that our method also generalizes well to recommend algorithms that have not been used while training and hence alleviates the need of retraining the system on a new set of algorithms introduced during test time.",https://arxiv.org/abs/2209.02993
"Ã•ptimal Vertex Fault-Tolerant Spanners in Ã•ptimal Time: Sequential,Distributed and Parallel",MeravParter,07-sep-22,Data Structures and Algorithms (cs.DS)," We (nearly) settle the time complexity for computing vertex fault- tolerant (VFT) spanners with optimal sparsity (up to polylogarithmic factors). VFT spanners are sparse subgraphs that preserve distance information, up to a small multiplicative stretch, in the presence of vertex failures. These structures were introduced by [Chechik et al., STOC 2009] and have received a lot of attention since then. We provide algorithms for computing nearly optimal $f$-VFT spanners for any $n$-vertex $m$-edge graph, with near optimal running time in several computational models:   \- A randomized sequential algorithm with a runtime of $\widetilde{O}(m)$ (i.e., independent in the number of faults $f$). The state-of-the-art time bound is $\widetilde{O}(f^{1-1/k}\cdot n^{2+1/k}+f^2 m)$ by [Bodwin, Dinitz and Robelle, SODA 2021].   \- A distributed congest algorithm of $\widetilde{O}(1)$ rounds. Improving upon [Dinitz and Robelle, PODC 2020] that obtained FT spanners with near- optimal sparsity in $\widetilde{O}(f^{2})$ rounds.   \- A PRAM (CRCW) algorithm with $\widetilde{O}(m)$ work and $\widetilde{O}(1)$ depth. Prior bounds implied by [Dinitz and Krauthgamer, PODC 2011] obtained sub-optimal FT spanners using $\widetilde{O}(f^3m)$ work and $\widetilde{O}(f^3)$ depth.   An immediate corollary provides the first nearly-optimal PRAM algorithm for computing nearly optimal $\lambda$-\emph{vertex} connectivity certificates using polylogarithmic depth and near-linear work. This improves the state- of-the-art parallel bounds of $\widetilde{O}(1)$ depth and $O(\lambda m)$ work, by [Karger and Motwani, STOC'93].",https://arxiv.org/abs/2209.02991
Propagation Path Loss Models in Forest Scenario at 605 MHz,"ZheXiao, ShuSun, ZhenyuLiu, LianmingXu, WeiHuang, LiWang, AiguoFei",07-sep-22,Information Theory (cs.IT)," When signals propagate through forest areas, they will be affected by environmental factors such as vegetation. Different types of environments have different influences on signal attenuation. This paper analyzes the existing classical propagation path loss models and the model with excess loss caused by forest areas and then proposes a new short-range wireless channel propagation model, which can be applied to different types of forest environments. We conducted continuous-wave measurements at a center frequency of 605 MHz on predetermined routes in distinct types of forest areas and recorded the reference signal received power. Then, we use various path loss models to fit the measured data based on different vegetation types and distributions. Simulation results show that the proposed model has substantially smaller fitting errors with reasonable computational complexity, as compared with representative traditional counterparts.",https://arxiv.org/abs/2209.02990
Multi-access Coded Caching with Optimal Rate and LinearSubpacketization under PDA and Consecutive Cyclic Placement,"JinyuWang, MinquanCheng, Youlong Wu",07-sep-22,Information Theory (cs.IT)," This work considers the multi-access caching system proposed by Hachem et al., where each user has access to L neighboring caches in a cyclic wrap-around fashion. We first propose a placement strategy called the consecutive cyclic placement, which achieves the maximal local caching gain. Then under the consecutive cyclic placement, we derive the optimal coded caching gain from the perspective of Placement Delivery Array (PDA), thus obtaining a lower bound on the rate of PDA. Finally, under the consecutive cyclic placement, we construct a class of PDA, leading to a multi-access coded caching scheme with linear subpacketization, which achieves our derived lower bound for some parameters; while for other parameters, the achieved coded caching gain is only 1 less than the optimal one. Analytical and numerical comparisons of the proposed scheme with existing schemes are provided to validate the performance.",https://arxiv.org/abs/2209.02989
Shifting Perspective to See Difference: A Novel Multi-View Method forSkeleton based Action Recognition,"RuijieHou, YanranLi, NingyuZhang, YulinZhou, XiaosongYang, ZhaoWang",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Skeleton-based human action recognition is a longstanding challenge due to its complex dynamics. Some fine-grain details of the dynamics play a vital role in classification. The existing work largely focuses on designing incremental neural networks with more complicated adjacent matrices to capture the details of joints relationships. However, they still have difficulties distinguishing actions that have broadly similar motion patterns but belong to different categories. Interestingly, we found that the subtle differences in motion patterns can be significantly amplified and become easy for audience to distinct through specified view directions, where this property haven't been fully explored before. Drastically different from previous work, we boost the performance by proposing a conceptually simple yet effective Multi-view strategy that recognizes actions from a collection of dynamic view features. Specifically, we design a novel Skeleton-Anchor Proposal (SAP) module which contains a Multi-head structure to learn a set of views. For feature learning of different views, we introduce a novel Angle Representation to transform the actions under different views and feed the transformations into the baseline model. Our module can work seamlessly with the existing action classification model. Incorporated with baseline models, our SAP module exhibits clear performance gains on many challenging benchmarks. Moreover, comprehensive experiments show that our model consistently beats down the state-of-the-art and remains effective and robust especially when dealing with corrupted data. Related code will be available on [this https URL](https://github.com/ideal-idea/SAP) .",https://arxiv.org/abs/2209.02987
Semantic Interactive Learning for Text Classification: A ConstructiveApproach for Contextual Interactions,"SebastianKiefer, MareikeHoffmann",07-sep-22,Human-Computer Interaction (cs.HC)," Interactive Machine Learning (IML) shall enable intelligent systems to interactively learn from their end-users, and is quickly becoming more and more important. Although it puts the human in the loop, interactions are mostly performed via mutual explanations that miss contextual information. Furthermore, current model-agnostic IML strategies like CAIPI are limited to 'destructive' feedback, meaning they solely allow an expert to prevent a learner from using irrelevant features. In this work, we propose a novel interaction framework called Semantic Interactive Learning for the text domain. We frame the problem of incorporating constructive and contextual feedback into the learner as a task to find an architecture that (a) enables more semantic alignment between humans and machines and (b) at the same time helps to maintain statistical characteristics of the input domain when generating user-defined counterexamples based on meaningful corrections. Therefore, we introduce a technique called SemanticPush that is effective for translating conceptual corrections of humans to non-extrapolating training examples such that the learner's reasoning is pushed towards the desired behavior. In several experiments, we show that our method clearly outperforms CAIPI, a state of the art IML strategy, in terms of Predictive Performance as well as Local Explanation Quality in downstream multi-class classification tasks.",https://arxiv.org/abs/2209.02986
In-Network Computing With Function as a Service at the Edge,"ClaudioCicconetti, Marco Conti, AndreaPassarella",07-sep-22,Networking and Internet Architecture (cs.NI)," Offloading computation from user devices to nodes with processing capabilities at the edge of the network is a major trend in today's network/service architectures. At the same time, serverless computing has gained a huge traction among the cloud computing technologies and has, thus, promoted the adoption of Function-as-a-Service (FaaS). The latter has some characteristics that make it generally suitable to edge applications, except for its cumbersome support of stateful applications. This work is set to provide a broad view on the options available for supporting stateful FaaS, which are distilled into four reference execution models that differ on where the state resides. While further investigation is needed to advance our understanding of the opportunities offered by in-network computing through stateful FaaS, initial insights are provided by means of a qualitative analysis of the four alternatives and their quantitative comparison in a simulator.",https://arxiv.org/abs/2209.02984
Improving the Cross-Lingual Generalisation in Visual QuestionAnswering,"FarhadNooralahzadeh, RicoSennrich",07-sep-22,Computation and Language (cs.CL)," While several benefits were realized for multilingual vision- language pretrained models, recent benchmarks across various tasks and languages showed poor cross-lingual generalisation when multilingually pre- trained vision-language models are applied to non-English data, with a large gap between (supervised) English performance and (zero-shot) cross-lingual transfer. In this work, we explore the poor performance of these models on a zero-shot cross-lingual visual question answering (VQA) task, where models are fine-tuned on English visual-question data and evaluated on 7 typologically diverse languages. We improve cross-lingual transfer with three strategies: (1) we introduce a linguistic prior objective to augment the cross-entropy loss with a similarity-based loss to guide the model during training, (2) we learn a task-specific subnetwork that improves cross-lingual generalisation and reduces variance without model modification, (3) we augment training examples using synthetic code-mixing to promote alignment of embeddings between source and target languages. Our experiments on xGQA using the pretrained multilingual multimodal transformers UC2 and M3P demonstrate the consistent effectiveness of the proposed fine-tuning strategy for 7 languages, outperforming existing transfer methods with sparse models. Code and data to reproduce our findings are publicly available.",https://arxiv.org/abs/2209.02983
VGStore: A Multimodal Extension to SPARQL for Querying RDF Scene Graph,"YanzengLi, ZilongZheng, Wenjuan Han, Lei Zou",07-sep-22,Databases (cs.DB)," Semantic Web technology has successfully facilitated many RDF models with rich data representation methods. It also has the potential ability to represent and store multimodal knowledge bases such as multimodal scene graphs. However, most existing query languages, especially SPARQL, barely explore the implicit multimodal relationships like semantic similarity, spatial relations, etc. We first explored this issue by organizing a large-scale scene graph dataset, namely Visual Genome, in the RDF graph database. Based on the proposed RDF-stored multimodal scene graph, we extended SPARQL queries to answer questions containing relational reasoning about color, spatial, etc. Further demo (i.e., VGStore) shows the effectiveness of customized queries and displaying multimodal data.",https://arxiv.org/abs/2209.02982
Opportunistic Wireless Control Over State-Dependent Fading Channels,"ShulingWang, Peizhe Li, ShanyingZhu, CailianChen",07-sep-22,Systems and Control (eess.SY)," The heterogeneous system consisting of the wireless control system (WCS) and mobile agent system (MAS) is ubiquitous in Industrial Internet of Things (IIoT) systems. Within this system, the positions of mobile agents may lead to shadow fading on the wireless channel that the WCS is controlled over and can significantly compromise the WCS's performance. This paper focuses on the controller design for the MAS to ensure the performance of WCS in the presence of WCS and MAS coupling. Firstly, the constrained finite field network (FFN) with profile-dependent switching topology is adopted to proceed the operational control for the MAS. By virtue of the algebraic state space representation (ASSR) method, an equivalent form is obtained for the WCS and MAS coupling. A necessary and sufficient condition in terms of constrained set stabilization is then established to ensure the Lyapunov- like performance with expected decay rate. Finally, a graphical method together with the breath-first searching is provided to design state feedback controllers for the MAS. With this method, it is easy to check the constrained set stabilization of MAS and to ensure the performance requirements of WCS in the presence of WCS and MAS coupling. The study of an illustrative example shows the effectiveness of the proposed method.",https://arxiv.org/abs/2209.02981
Error Estimates and Physics Informed Augmentation of Neural Networksfor Thermally Coupled Incompressible Navier Stokes Equations,"ShoaibGoraya, Nahil Sobh, Arif Masud",07-sep-22,Numerical Analysis (math.NA)," Physics Informed Neural Networks (PINNs) are shown to be a promising method for the approximation of Partial Differential Equations (PDEs). PINNs approximate the PDE solution by minimizing physics-based loss functions over a given domain. Despite substantial progress in the application of PINNs to a range of problem classes, investigation of error estimation and convergence properties of PINNs, which is important for establishing the rationale behind their good empirical performance, has been lacking. This paper presents convergence analysis and error estimates of PINNs for a multi-physics problem of thermally coupled incompressible Navier-Stokes equations. Through a model problem of Beltrami flow it is shown that a small training error implies a small generalization error. \textit{Posteriori} convergence rates of total error with respect to the training residual and collocation points are presented. This is of practical significance in determining appropriate number of training parameters and training residual thresholds to get good PINNs prediction of thermally coupled steady state laminar flows. These convergence rates are then generalized to different spatial geometries as well as to different flow parameters that lie in the laminar regime. A pressure stabilization term in the form of pressure Poisson equation is added to the PDE residuals for PINNs. This physics informed augmentation is shown to improve accuracy of the pressure field by an order of magnitude as compared to the case without augmentation. Results from PINNs are compared to the ones obtained from stabilized finite element method and good properties of PINNs are highlighted.",https://arxiv.org/abs/2209.02978
YOLOv6: A Single-Stage Object Detection Framework for IndustrialApplications,"ChuyiLi, LuluLi, HongliangJiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, Yiduo Li, BoZhang, YufeiLiang, Linyuan Zhou, Xiaoming Xu, XiangxiangChu, XiaomingWei, XiaolinWei",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," For years, the YOLO series has been the de facto industry-level standard for efficient object detection. The YOLO community has prospered overwhelmingly to enrich its use in a multitude of hardware platforms and abundant scenarios. In this technical report, we strive to push its limits to the next level, stepping forward with an unwavering mindset for industry application.   Considering the diverse requirements for speed and accuracy in the real environment, we extensively examine the up-to-date object detection advancements either from industry or academia. Specifically, we heavily assimilate ideas from recent network design, training strategies, testing techniques, quantization, and optimization methods. On top of this, we integrate our thoughts and practice to build a suite of deployment-ready networks at various scales to accommodate diversified use cases. With the generous permission of YOLO authors, we name it YOLOv6. We also express our warm welcome to users and contributors for further enhancement. For a glimpse of performance, our YOLOv6-N hits 35.9% AP on the COCO dataset at a throughput of 1234 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S strikes 43.5% AP at 495 FPS, outperforming other mainstream detectors at the same scale~(YOLOv5-S, YOLOX-S, and PPYOLOE-S). Our quantized version of YOLOv6-S even brings a new state-of-the-art 43.3% AP at 869 FPS. Furthermore, YOLOv6-M/L also achieves better accuracy performance (i.e., 49.5%/52.3%) than other detectors with a similar inference speed. We carefully conducted experiments to validate the effectiveness of each component. Our code is made available at [this https URL](https://github.com/meituan/YOLOv6).",https://arxiv.org/abs/2209.02977
A multi-chain synchronization protocol that leverage zero knowledgeproof to minimize communication trust base,"SinkaGao, Guo QiangLi, Hong FeiFu, HengZhang",07-sep-22,Networking and Internet Architecture (cs.NI)," Delphinus cross-chain aggregator is a universal firmware which synchronise states between different smart contracts on different block- chains. In the world of block-chains, synchronization challenges are two- folded. Firstly, contracts from different main block-chain can not communicate with each other which makes it hard to establish a trustworthy communication channel for them to share and maintain a universal state between each other. Secondly, transactions on different block-chains can hardly be ordered thus conflicts are common and we need a novel way to avoid and handle these conflicts. Delphinus cross-chain aggregator is a ZKSNARK based multi-block-chain layer on top of which rich cross chain applications can run safely and efficiently.",https://arxiv.org/abs/2209.02976
Foundations of probability-raising causality in Markov decisionprocesses,"ChristelBaier, JakobPiribauer, RobinZiemek",07-sep-22,Logic in Computer Science (cs.LO)," This work introduces a novel cause-effect relation in Markov decision processes using the probability-raising principle. Initially, sets of states as causes and effects are considered, which is subsequently extended to regular path properties as effects and then as causes. The paper lays the mathematical foundations and analyzes the algorithmic properties of these cause-effect relations. This includes algorithms for checking cause conditions given an effect and deciding the existence of probability-raising causes. As the definition allows for sub-optimal coverage properties, quality measures for causes inspired by concepts of statistical analysis are studied. These include recall, coverage ratio and f-score. The computational complexity for finding optimal causes with respect to these measures is analyzed.",https://arxiv.org/abs/2209.02974
On probability-raising causality in Markov decision processes,"ChristelBaier, FlorianFunke, JakobPiribauer, RobinZiemek",21 Jan 2022,Logic in Computer Science (cs.LO)," The purpose of this paper is to introduce a notion of causality in Markov decision processes based on the probability-raising principle and to analyze its algorithmic properties. The latter includes algorithms for checking cause-effect relationships and the existence of probability-raising causes for given effect scenarios. Inspired by concepts of statistical analysis, we study quality measures (recall, coverage ratio and f-score) for causes and develop algorithms for their computation. Finally, the computational complexity for finding optimal causes with respect to these measures is analyzed.",https://arxiv.org/abs/2209.02973
Non-Standard Vietnamese Word Detection and Normalization for Text-to-Speech,"Huu-TienDang, Thi-Hai-YenVuong, Xuan-Hieu Phan",07-sep-22,Computation and Language (cs.CL)," Converting written texts into their spoken forms is an essential problem in any text-to-speech (TTS) systems. However, building an effective text normalization solution for a real-world TTS system face two main challenges: (1) the semantic ambiguity of non-standard words (NSWs), e.g., numbers, dates, ranges, scores, abbreviations, and (2) transforming NSWs into pronounceable syllables, such as URL, email address, hashtag, and contact name. In this paper, we propose a new two-phase normalization approach to deal with these challenges. First, a model-based tagger is designed to detect NSWs. Then, depending on NSW types, a rule-based normalizer expands those NSWs into their final verbal forms. We conducted three empirical experiments for NSW detection using Conditional Random Fields (CRFs), BiLSTM-CNN-CRF, and BERT-BiGRU-CRF models on a manually annotated dataset including 5819 sentences extracted from Vietnamese news articles. In the second phase, we propose a forward lexicon-based maximum matching algorithm to split down the hashtag, email, URL, and contact name. The experimental results of the tagging phase show that the average F1 scores of the BiLSTM-CNN-CRF and CRF models are above 90.00%, reaching the highest F1 of 95.00% with the BERT-BiGRU-CRF model. Overall, our approach has low sentence error rates, at 8.15% with CRF and 7.11% with BiLSTM-CNN- CRF taggers, and only 6.67% with BERT-BiGRU-CRF tagger.",https://arxiv.org/abs/2201.08768
Fengshenbang 1.0: Being the Foundation of Chinese CognitiveIntelligence,"JunjieWang, YuxiangZhang, LinZhang, PingYang, XinyuGao, ZiweiWu, XiaoqunDong, JunqingHe, JianhengZhuo, QiYang, YongfengHuang, XiayuLi, YanghanWu, JunyuLu, XinyuZhu, WeifengChen, TingHan, KunhaoPan, RuiWang, HaoWang, XiaojunWu, ZhongshenZeng, ChongpeiChen, RuyiGan, JiaxingZhang",07-sep-22,Computation and Language (cs.CL)," Nowadays, foundation models become one of fundamental infrastructures in artificial intelligence, paving ways to the general intelligence. However, the reality presents two urgent challenges: existing foundation models are dominated by the English-language community; users are often given limited resources and thus cannot always use foundation models. To support the development of the Chinese-language community, we introduce an open-source project, called Fengshenbang, which leads by the research center for Cognitive Computing and Natural Language (CCNL). Our project has comprehensive capabilities, including large pre-trained models, user- friendly APIs, benchmarks, datasets, and others. We wrap all these in three sub-projects: the Fengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An open-source roadmap, Fengshenbang, aims to re- evaluate the open-source community of Chinese pre-trained large-scale models, prompting the development of the entire Chinese large-scale model community. We also want to build a user-centered open-source ecosystem to allow individuals to access the desired models to match their computing resources. Furthermore, we invite companies, colleges, and research institutions to collaborate with us to build the large-scale open-source model-based ecosystem. We hope that this project will be the foundation of Chinese cognitive intelligence.",https://arxiv.org/abs/2209.02971
That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentationwith Switch-memory,"XuemeiTang, QiSu, JunWang",07-sep-22,Computation and Language (cs.CL)," The evolution of language follows the rule of gradual change. Grammar, vocabulary, and lexical semantic shifts take place over time, resulting in a diachronic linguistic gap. As such, a considerable amount of texts are written in languages of different eras, which creates obstacles for natural language processing tasks, such as word segmentation and machine translation. Although the Chinese language has a long history, previous Chinese natural language processing research has primarily focused on tasks within a specific era. Therefore, we propose a cross-era learning framework for Chinese word segmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to incorporate era-specific linguistic knowledge. Experiments on four corpora from different eras show that the performance of each corpus significantly improves. Further analyses also demonstrate that the SM can effectively integrate the knowledge of the eras into the neural network.",https://arxiv.org/abs/2209.02970
ExpTrialMng: A Universal Experiment Trial Manager for AR/VR/MRExperiments based on Unity,"JinwookKim, Yee JoonKim, JeongmiLee",07-sep-22,Human-Computer Interaction (cs.HC)," Based on the improvement of recent virtual and augmented reality (VR and AR) Head Mounted Display (HMD), there have been attempts to adopt VR and AR in various fields. Since VR and AR could provide more immersive experimental environments and stimuli than 2D settings in a cost-efficient way, psychological and cognitive researchers are particularly interested in using these platforms. However, there is still an entry barrier for researchers who are not familiar with Unity programming, and current VR/AR HMDs could also cause unexpected errors during the experiment. Therefore, we developed a Unity library that can be adopted in various experiments universally and assist researchers in developing their own. Our library provides functions related to trial assignment and results saving. That way, researchers can easily implement the essential functions of their psychological experiments. We also made a function that enables proceeding with the experiment from a specific trial point to handle unexpected errors caused by HMD tracking loss issues during the experiment. We expect our library could invite researchers from various disciplines and help them acquire valuable insights in VR/AR environments.",https://arxiv.org/abs/2209.02967
Risk of Bias in Chest X-ray Foundation Models,"BenGlocker, CharlesJones, MelanieBernhardt, StefanWinzeck",07-sep-22,Machine Learning (cs.LG)," Foundation models are considered a breakthrough in all applications of AI, promising robust and reusable mechanisms for feature extraction, alleviating the need for large amounts of high quality training data for task-specific prediction models. However, foundation models may potentially encode and even reinforce existing biases present in historic datasets. Given the limited ability to scrutinize foundation models, it remains unclear whether the opportunities outweigh the risks in safety critical applications such as clinical decision making. In our statistical bias analysis of a recently published, and publicly available chest X-ray foundation model, we found reasons for concern as the model seems to encode protected characteristics including biological sex and racial identity, which may lead to disparate performance across subgroups in downstream applications. While research into foundation models for healthcare applications is in an early stage, we believe it is important to make the community aware of these risks to avoid harm.",https://arxiv.org/abs/2209.02966
Adam Mickiewicz University at WMT 2022: NER-Assisted and Quality-AwareNeural Machine Translation,"ArturNowakowski, GabrielaPaÅ‚ka, KamilGuttmann, MikoÅ‚ajPokrywka",07-sep-22,Computation and Language (cs.CL)," This paper presents Adam Mickiewicz University's (AMU) submissions to the constrained track of the WMT 2022 General MT Task. We participated in the Ukrainian $\leftrightarrow$ Czech translation directions. The systems are a weighted ensemble of four models based on the Transformer (big) architecture. The models use source factors to utilize the information about named entities present in the input. Each of the models in the ensemble was trained using only the data provided by the shared task organizers. A noisy back-translation technique was used to augment the training corpora. One of the models in the ensemble is a document-level model, trained on parallel and synthetic longer sequences. During the sentence-level decoding process, the ensemble generated the n-best list. The n-best list was merged with the n-best list generated by a single document-level model which translated multiple sentences at a time. Finally, existing quality estimation models and minimum Bayes risk decoding were used to rerank the n-best list so that the best hypothesis was chosen according to the COMET evaluation metric. According to the automatic evaluation results, our systems rank first in both translation directions.",https://arxiv.org/abs/2209.02965
Difficulty-Net: Learning to Predict Difficulty for Long-TailedRecognition,"SaptarshiSinha, HirokiOhashi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Long-tailed datasets, where head classes comprise much more training samples than tail classes, cause recognition models to get biased towards the head classes. Weighted loss is one of the most popular ways of mitigating this issue, and a recent work has suggested that class-difficulty might be a better clue than conventionally used class-frequency to decide the distribution of weights. A heuristic formulation was used in the previous work for quantifying the difficulty, but we empirically find that the optimal formulation varies depending on the characteristics of datasets. Therefore, we propose Difficulty-Net, which learns to predict the difficulty of classes using the model's performance in a meta-learning framework. To make it learn reasonable difficulty of a class within the context of other classes, we newly introduce two key concepts, namely the relative difficulty and the driver loss. The former helps Difficulty-Net take other classes into account when calculating difficulty of a class, while the latter is indispensable for guiding the learning to a meaningful direction. Extensive experiments on popular long-tailed datasets demonstrated the effectiveness of the proposed method, and it achieved state-of-the-art performance on multiple long-tailed datasets.",https://arxiv.org/abs/2209.02962
A Weakly Supervised Learning Framework for Salient Object Detectionvia Hybrid Labels,"RunminCong, QiQin, ChenZhang, QiupingJiang, ShiqiWang, YaoZhao, SamKwong",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Fully-supervised salient object detection (SOD) methods have made great progress, but such methods often rely on a large number of pixel-level annotations, which are time-consuming and labour-intensive. In this paper, we focus on a new weakly-supervised SOD task under hybrid labels, where the supervision labels include a large number of coarse labels generated by the traditional unsupervised method and a small number of real labels. To address the issues of label noise and quantity imbalance in this task, we design a new pipeline framework with three sophisticated training strategies. In terms of model framework, we decouple the task into label refinement sub-task and salient object detection sub-task, which cooperate with each other and train alternately. Specifically, the R-Net is designed as a two-stream encoder-decoder model equipped with Blender with Guidance and Aggregation Mechanisms (BGA), aiming to rectify the coarse labels for more reliable pseudo-labels, while the S-Net is a replaceable SOD network supervised by the pseudo labels generated by the current R-Net. Note that, we only need to use the trained S-Net for testing. Moreover, in order to guarantee the effectiveness and efficiency of network training, we design three training strategies, including alternate iteration mechanism, group- wise incremental mechanism, and credibility verification mechanism. Experiments on five SOD benchmarks show that our method achieves competitive performance against weakly-supervised/unsupervised methods both qualitatively and quantitatively.",https://arxiv.org/abs/2209.02960
Semi-supervised Crowd Counting via Density Agency,"HuiLin, ZhihengMa, XiaopengHong, YaoweiWang, ZhouSu",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In this paper, we propose a new agency-guided semi-supervised counting approach. First, we build a learnable auxiliary structure, namely the density agency to bring the recognized foreground regional features close to corresponding density sub-classes (agents) and push away background ones. Second, we propose a density-guided contrastive learning loss to consolidate the backbone feature extractor. Third, we build a regression head by using a transformer structure to refine the foreground features further. Finally, an efficient noise depression loss is provided to minimize the negative influence of annotation noises. Extensive experiments on four challenging crowd counting datasets demonstrate that our method achieves superior performance to the state-of-the-art semi-supervised counting methods by a large margin. Code is available.",https://arxiv.org/abs/2209.02957
A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on aPlatform,"Z.Jiang, G.Song",07-sep-22,Robotics (cs.RO)," With the development of industry, drones are appearing in various field. In recent years, deep reinforcement learning has made impressive gains in games, and we are committed to applying deep reinforcement learning algorithms to the field of robotics, moving reinforcement learning algorithms from game scenarios to real-world application scenarios. We are inspired by the LunarLander of OpenAI Gym, we decided to make a bold attempt in the field of reinforcement learning to control drones. At present, there is still a lack of work applying reinforcement learning algorithms to robot control, the physical simulation platform related to robot control is only suitable for the verification of classical algorithms, and is not suitable for accessing reinforcement learning algorithms for the training. In this paper, we will face this problem, bridging the gap between physical simulation platforms and intelligent agent, connecting intelligent agents to a physical simulation platform, allowing agents to learn and complete drone flight tasks in a simulator that approximates the real world. We proposed a reinforcement learning framework based on Gazebo that is a kind of physical simulation platform (ROS-RL), and used three continuous action space reinforcement learning algorithms in the framework to dealing with the problem of autonomous landing of drones. Experiments show the effectiveness of the algorithm, the task of autonomous landing of drones based on reinforcement learning achieved full success.",https://arxiv.org/abs/2209.02955
BiFuse++: Self-supervised and Efficient Bi-projection Fusion for 360Depth Estimation,"Fu-EnWang, Yu-Hsuan Yeh, Yi-Hsuan Tsai, Wei-ChenChiu, MinSun",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Due to the rise of spherical cameras, monocular 360 depth estimation becomes an important technique for many applications (e.g., autonomous systems). Thus, state-of-the-art frameworks for monocular 360 depth estimation such as bi-projection fusion in BiFuse are proposed. To train such a framework, a large number of panoramas along with the corresponding depth ground truths captured by laser sensors are required, which highly increases the cost of data collection. Moreover, since such a data collection procedure is time-consuming, the scalability of extending these methods to different scenes becomes a challenge. To this end, self- training a network for monocular depth estimation from 360 videos is one way to alleviate this issue. However, there are no existing frameworks that incorporate bi-projection fusion into the self-training scheme, which highly limits the self-supervised performance since bi-projection fusion can leverage information from different projection types. In this paper, we propose BiFuse++ to explore the combination of bi-projection fusion and the self-training scenario. To be specific, we propose a new fusion module and Contrast-Aware Photometric Loss to improve the performance of BiFuse and increase the stability of self-training on real-world videos. We conduct both supervised and self-supervised experiments on benchmark datasets and achieve state-of-the-art performance.",https://arxiv.org/abs/2209.02954
Democratizing Domain-Specific Computing,"YuzeChi, WeikangQiao, AtefehSohrabizadeh, Jie Wang, Jason Cong",07-sep-22,Hardware Architecture (cs.AR)," In the past few years, domain-specific accelerators (DSAs), such as Google's Tensor Processing Units, have shown to offer significant performance and energy efficiency over general-purpose CPUs. An important question is whether typical software developers can design and implement their own customized DSAs, with affordability and efficiency, to accelerate their applications. This article presents our answer to this question.",https://arxiv.org/abs/2209.02952
Visual Transformer for Soil Classification,"AaryanJagetia, UmangGoenka, PriyadarshiniKumari, Mary Samuel",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Our food security is built on the foundation of soil. Farmers would be unable to feed us with fiber, food, and fuel if the soils were not healthy. Accurately predicting the type of soil helps in planning the usage of the soil and thus increasing productivity. This research employs state- of-the-art Visual Transformers and also compares performance with different models such as SVM, Alexnet, Resnet, and CNN. Furthermore, this study also focuses on differentiating different Visual Transformers architectures. For the classification of soil type, the dataset consists of 4 different types of soil samples such as alluvial, red, black, and clay. The Visual Transformer model outperforms other models in terms of both test and train accuracies by attaining 98.13% on training and 93.62% while testing. The performance of the Visual Transformer exceeds the performance of other models by at least 2%. Hence, the novel Visual Transformers can be used for Computer Vision tasks including Soil Classification.",https://arxiv.org/abs/2209.02951
"Nonoverlapping (delta, gamma)-approximate pattern matching","YouxiWu, BojingJian, YanLi, HeJiang, Xindong Wu",07-sep-22,Databases (cs.DB)," Pattern matching can be used to calculate the support of patterns, and is a key issue in sequential pattern mining (or sequence pattern mining). Nonoverlapping pattern matching means that two occurrences cannot use the same character in the sequence at the same position. Approximate pattern matching allows for some data noise, and is more general than exact pattern matching. At present, nonoverlapping approximate pattern matching is based on Hamming distance, which cannot be used to measure the local approximation between the subsequence and pattern, resulting in large deviations in matching results. To tackle this issue, we present a Nonoverlapping Delta and gamma approximate Pattern matching (NDP) scheme that employs the (delta, gamma)-distance to give an approximate pattern matching, where the local and the global distances do not exceed delta and gamma, respectively. We first transform the NDP problem into a local approximate Nettree and then construct an efficient algorithm, called the local approximate Nettree for NDP (NetNDP). We propose a new approach called the Minimal Root Distance which allows us to determine whether or not a node has root paths that satisfy the global constraint and to prune invalid nodes and parent-child relationships. NetNDP finds the rightmost absolute leaf of the max root, searches for the rightmost occurrence from the rightmost absolute leaf, and deletes this occurrence. We iterate the above steps until there are no new occurrences. Numerous experiments are used to verify the performance of the proposed algorithm.",https://arxiv.org/abs/2209.02950
Assessing Software Privacy using the Privacy Flow-Graph,"FeiyangTang, BjarteM.Ã˜stvold",07-sep-22,Cryptography and Security (cs.CR)," We increasingly rely on digital services and the conveniences they provide. Processing of personal data is integral to such services and thus privacy and data protection are a growing concern, and governments have responded with regulations such as the EU's GDPR. Following this, organisations that make software have legal obligations to document the privacy and data protection of their software. This work must involve both software developers that understand the code and the organisation's data protection officer or legal department that understands privacy and the requirements of a Data Protection and Impact Assessment (DPIA).   To help developers and non-technical people such as lawyers document the privacy and data protection behaviour of software, we have developed an automatic software analysis technique. This technique is based on static program analysis to characterise the flow of privacy-related data. The results of the analysis can be presented as a graph of privacy flows and operations -- that is understandable also for non-technical people. We argue that our technique facilitates collaboration between technical and non- technical people in documenting the privacy behaviour of the software. We explain how to use the results produced by our technique to answer a series of privacy-relevant questions needed for a DPIA. To illustrate our work, we show both detailed and abstract analysis results from applying our analysis technique to the secure messaging service Signal and to the client of the cloud service NextCloud and show how their privacy flow-graphs inform the writing of a DPIA.",https://arxiv.org/abs/2209.02949
Architecture-Algorithmic Trade-offs in Multi-path Channel Estimationfor mmWAVE Systems,"LyutianyangZhang, SumitRoy, LiuCao",07-sep-22,Information Theory (cs.IT)," 5G mmWave massive MIMO systems are likely to be deployed in dense urban scenarios, where increasing network capacity is the primary objective. A key component in mmWave transceiver design is channel estimation which is challenging due to the very large signal bandwidths (order of GHz) implying significant resolved spatial multipath, coupled with large # of Tx/Rx antennas for large-scale MIMO. This results in significantly increased training overhead that in turn leads to unacceptably high computational complexity and power cost. Our work thus highlights the interplay of transceiver architecture and receiver signal processing algorithm choices that fundamentally address (mobile) handset power consumption, with minimal degradation in performance. We investigate trade-offs enabled by conjunction of hybrid beamforming mmWave receiver and channel estimation algorithms that exploit available sparsity in such wideband scenarios. A compressive sensing (CS) framework for sparse channel estimation -- Binary Iterative Hard Thresholding (BIHT) \cite{jacques2013robust} followed by linear reconstruction method with varying quantization (ADC) levels -- is explored to compare the trade-offs between bit-depth and sampling rate for a given ADC power budget. Performance analysis of the BIHT+ linear reconstruction method is conducted via simulation studies for 5G specified multi-path channel models and compared to oracle-assisted bounds for validation.",https://arxiv.org/abs/2209.02948
Can GAN-induced Attribute Manipulations Impact Face Recognition?,"SudiptaBanerjee, AditiAggarwal, Arun Ross",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Impact due to demographic factors such as age, sex, race, etc., has been studied extensively in automated face recognition systems. However, the impact of \textit{digitally modified} demographic and facial attributes on face recognition is relatively under-explored. In this work, we study the effect of attribute manipulations induced via generative adversarial networks (GANs) on face recognition performance. We conduct experiments on the CelebA dataset by intentionally modifying thirteen attributes using AttGAN and STGAN and evaluating their impact on two deep learning-based face verification methods, ArcFace and VGGFace. Our findings indicate that some attribute manipulations involving eyeglasses and digital alteration of sex cues can significantly impair face recognition by up to 73% and need further analysis.",https://arxiv.org/abs/2209.02944
Grouping-matrix based Graph Pooling with Adaptive Number of Clusters,"Sung MoonKo, SungjunCho, Dae-WoongJeong, SehuiHan, MoontaeLee, HonglakLee",07-sep-22,Artificial Intelligence (cs.AI)," Graph pooling is a crucial operation for encoding hierarchical structures within graphs. Most existing graph pooling approaches formulate the problem as a node clustering task which effectively captures the graph topology. Conventional methods ask users to specify an appropriate number of clusters as a hyperparameter, then assume that all input graphs share the same number of clusters. In inductive settings where the number of clusters can vary, however, the model should be able to represent this variation in its pooling layers in order to learn suitable clusters. Thus we propose GMPool, a novel differentiable graph pooling architecture that automatically determines the appropriate number of clusters based on the input data. The main intuition involves a grouping matrix defined as a quadratic form of the pooling operator, which induces use of binary classification probabilities of pairwise combinations of nodes. GMPool obtains the pooling operator by first computing the grouping matrix, then decomposing it. Extensive evaluations on molecular property prediction tasks demonstrate that our method outperforms conventional methods.",https://arxiv.org/abs/2209.02941
Adjusted Asymmetric Accuracy: A Well-Behaving External ClusterValidity Measure,MarekGagolewski,07-sep-22,Machine Learning (cs.LG)," There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to pinpoint those which are well-performing on certain task types and filter out the systematically disappointing ones. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. The commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, might not possess all the desirable properties, e.g., they do not identify pathological edge cases correctly. Furthermore, they are not nicely interpretable: it is hard to say what a score of 0.8 really means. Its behaviour might also vary as the number of true clusters changes. This makes comparing clustering algorithms across many benchmark datasets difficult. To remedy this, we propose and analyse a new measure: an asymmetric version of the optimal set-matching accuracy. It is corrected for chance and the imbalancedness of cluster sizes.",https://arxiv.org/abs/2209.02939
Facial De-morphing: Extracting Component Faces from a Single Morph,"SudiptaBanerjee, PrateekJaiswal, Arun Ross",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," A face morph is created by strategically combining two or more face images corresponding to multiple identities. The intention is for the morphed image to match with multiple identities. Current morph attack detection strategies can detect morphs but cannot recover the images or identities used in creating them. The task of deducing the individual face images from a morphed face image is known as \textit{de-morphing}. Existing work in de-morphing assume the availability of a reference image pertaining to one identity in order to recover the image of the accomplice - i.e., the other identity. In this work, we propose a novel de-morphing method that can recover images of both identities simultaneously from a single morphed face image without needing a reference image or prior information about the morphing process. We propose a generative adversarial network that achieves single image-based de-morphing with a surprisingly high degree of visual realism and biometric similarity with the original face images. We demonstrate the performance of our method on landmark-based morphs and generative model-based morphs with promising results.",https://arxiv.org/abs/2209.02935
Solving Elliptic Problems with Singular Sources using SingularitySplitting Deep Ritz Method,"TianhaoHu, BangtiJin, ZhiZhou",07-sep-22,Numerical Analysis (math.NA)," In this work, we develop an efficient solver based on deep neural networks for the Poisson equation with variable coefficients and singular sources expressed by the Dirac delta function $\delta(\mathbf{x})$. This class of problems covers general point sources, line sources and point-line combinations, and has a broad range of practical applications. The proposed approach is based on decomposing the true solution into a singular part that is known analytically using the fundamental solution of the Laplace equation and a regular part that satisfies a suitable elliptic PDE with smoother sources, and then solving for the regular part using the deep Ritz method. A path-following strategy is suggested to select the penalty parameter for penalizing the Dirichlet boundary condition. Extensive numerical experiments in two- and multi-dimensional spaces with point sources, line sources or their combinations are presented to illustrate the efficiency of the proposed approach, and a comparative study with several existing approaches is also given, which shows clearly its competitiveness for the specific class of problems. In addition, we briefly discuss the error analysis of the approach.",https://arxiv.org/abs/2209.02933
Reflections on Software Failure Analysis,Paschal C.Amusuo(1) AishwaryaSharma (1)Siddharth R.Rao (1)AbbeyVincent(1) James C.Davis (1)(,07-sep-22,Software Engineering (cs.SE)," Failure studies are important in revealing the root causes, behaviors, and life cycle of defects in software systems. These studies either focus on understanding the characteristics of defects in specific classes of systems or the characteristics of a specific type of defect in the systems it manifests in. Failure studies have influenced various software engineering research directions, especially in the area of software evolution, defect detection, and program repair.   In this paper, we reflect on the conduct of failure studies in software engineering. We reviewed a sample of 52 failure study papers. We identified several recurring problems in these studies, some of which hinder the ability of the engineering community to trust or replicate the results. Based on our findings, we suggest future research directions, including identifying and analyzing failure causal chains, standardizing the conduct of failure studies, and tool support for faster defect analysis.",https://arxiv.org/abs/2209.02931
Network-aware Prefetching Method for Short-Form Video Streaming,"DucNguyen, PhongNguyen, VuLong, TruongThu Huong, Pham NgocNam",07-sep-22,Multimedia (cs.MM)," Recent years have witnessed the rising of short-form video platforms such as TikTok. Apart from conventional videos, short-form videos are much shorter and users frequently change the content to watch. Thus, it is crucial to have an effective streaming method for this new type of video. In this paper, we propose a resource-efficient prefetching method for short- form video streaming. Taking into account network throughput conditions and user viewing behaviors, the proposed method dynamically adapts the amount of prefetched video data. Experiment results show that our method can reduce the data waste by 37~52% compared to other existing methods.",https://arxiv.org/abs/2209.02930
A SUMO Framework for Deep Reinforcement Learning Experiments SolvingElectric Vehicle Charging Dispatching Problem,"YaofengSong, HanZhao, Ruikang Luo, LipingHuang, YichengZhang, Rong Su",07-sep-22,Systems and Control (eess.SY)," In modern cities, the number of Electric vehicles (EV) is increasing rapidly for their low emission and better dynamic performance, leading to increasing demand for EV charging. However, due to the limited number of EV charging facilities, catering to the huge demand for time- consuming EV charging becomes a critical problem. It is quite a challenge to dispatch EVs in the dynamic traffic environment and coordinate interaction among agents. To better serve further research on various related Deep Reinforcment Learning (DRL) EV dispatching algorithms, an efficient simulation environment is necessary to ensure success. As simulator Simulation Urban Mobility (SUMO) is one of the most widely used open-source simulators, it has great significance in creating an environment that satisfies research requirements on SUMO. We aim to improve the efficiency of EV charging station usage and save time for EV users in further work. As a result, we design an EV navigation system on the basis of the traffic simulator SUMO using Jurong Area, Singapore in this paper. Various state-of- the-art DRL algorithms are deployed on the designed testbed to validate the feasibility of the framework in terms of EV charging dispatching problems. Besides EV dispatching problems, the environment can also serve for other reinforcement learning (RL) traffic control problems",https://arxiv.org/abs/2209.02927
Hardware Acceleration of Sampling Algorithms in Sample and AggregateGraph Neural Networks,"YuchenGui, BoyiWei, WeiYuan, XiJin",07-sep-22,Machine Learning (cs.LG)," Sampling is an important process in many GNN structures in order to train larger datasets with a smaller computational complexity. However, compared to other processes in GNN (such as aggregate, backward propagation), the sampling process still costs tremendous time, which limits the speed of training. To reduce the time of sampling, hardware acceleration is an ideal choice. However, state of the art GNN acceleration proposal did not specify how to accelerate the sampling process. What's more, directly accelerating traditional sampling algorithms will make the structure of the accelerator very complicated.   In this work, we made two contributions: (1) Proposed a new neighbor sampler: CONCAT Sampler, which can be easily accelerated on hardware level while guaranteeing the test accuracy. (2) Designed a CONCAT-sampler- accelerator based on FPGA, with which the neighbor sampling process boosted to about 300-1000 times faster compared to the sampling process without it.",https://arxiv.org/abs/2209.02921
Convergence analysis of an implicit finite difference method for theinertial Landau-Lifshitz-Gilbert equation,"JingrunChen, Panchi Li, Cheng Wang",07-sep-22,Numerical Analysis (math.NA)," The Landau-Lifshitz-Gilbert (LLG) equation is a widely used model for fast magnetization dynamics in ferromagnetic materials. Recently, the inertial LLG equation, which contains an inertial term, has been proposed to capture the ultra-fast magnetization dynamics at the sub-picosecond timescale. Mathematically, this generalized model contains the first temporal derivative and a newly introduced second temporal derivative of magnetization. Consequently, it produces extra difficulties in numerical analysis due to the mixed hyperbolic-parabolic type of this equation with degeneracy. In this work, we propose an implicit finite difference scheme based on the central difference in both time and space. A fixed point iteration method is applied to solve the implicit nonlinear system. With the help of a second order accurate constructed solution, we provide a convergence analysis in $H^1$ for this numerical scheme, in the $\ell^\infty (0, T; H_h^1)$ norm. It is shown that the proposed method is second order accurate in both time and space, with unconditional stability and a natural preservation of the magnetization length. In the hyperbolic regime, significant damping wave behaviors of magnetization at a shorter timescale are observed through numerical simulations.",https://arxiv.org/abs/2209.02916
Optimal Sensor Placement in Body Surface Networks using GaussianProcesses,"EmadAlenany, ChangqingCheng",07-sep-22,Machine Learning (cs.LG)," This paper explores a new sequential selection framework for the optimal sensor placement (OSP) in Electrocardiography imaging networks (ECGI). The proposed methodology incorporates the use a recent experimental design method for the sequential selection of landmarkings on biological objects, namely, Gaussian process landmarking (GPLMK) for better exploration of the candidate sensors. The two experimental design methods work as a source of the training and the validation locations which is fitted using a spatiotemporal Gaussian process (STGP). The STGP is fitted using the training set to predict for the current validation set generated using GPLMK, and the sensor with the largest prediction absolute error is selected from the current validation set and added to the selected sensors. Next, a new validation set is generated and predicted using the current training set. The process continues until selecting a specific number of sensor locations. The study is conducted on a dataset of body surface potential mapping (BSPM) of 352 electrodes of four human subjects. A number of 30 sensor locations is selected using the proposed algorithm. The selected sensor locations achieved average $R^2 = 94.40 \%$ for estimating the whole- body QRS segment. The proposed method adds to design efforts for a more clinically practical ECGI system by improving its wearability and reduce the design cost as well.",https://arxiv.org/abs/2209.02914
Social Media Engagement and Cryptocurrency Performance,"KhizarQureshi, Tauhid Zaman",07-sep-22,Social and Information Networks (cs.SI)," We study the problem of predicting the future performance of cryptocurrencies using social media data. We propose a new model to measure the engagement of users with topics discussed on social media based on interactions with social media posts. This model overcomes the limitations of previous volume and sentiment based approaches. We use this model to estimate engagement coefficients for 48 cryptocurrencies created between 2019 and 2021 using data from Twitter from the first month of the cryptocurrencies' existence. We find that the future returns of the cryptocurrencies are dependent on the engagement coefficients. Cryptocurrencies whose engagement coefficients are too low or too high have lower returns. Low engagement coefficients signal a lack of interest, while high engagement coefficients signal artificial activity which is likely from automated accounts known as bots. We measure the amount of bot posts for the cryptocurrencies and find that generally, cryptocurrencies with more bot posts have lower future returns. While future returns are dependent on both the bot activity and engagement coefficient, the dependence is strongest for the engagement coefficient, especially for short-term returns. We show that simple investment strategies which select cryptocurrencies with engagement coefficients exceeding a fixed threshold perform well for holding times of a few months.",https://arxiv.org/abs/2209.02912
PERFECT: A Hyperbolic Embedding for Joint User and Community Alignment,"LiSun, ZhongbaoZhang, JiaweiZhang, Feiyang Wang, Yang Du, SenSu, Philip S.Yu",07-sep-22,Social and Information Networks (cs.SI)," Social network alignment shows fundamental importance in a wide spectrum of applications. To the best of our knowledge, existing studies mainly focus on network alignment at the individual user level, requiring abundant common information between shared individual users. For the networks that cannot meet such requirements, social community structures actually provide complementary and critical information at a slightly coarse-grained level, alignment of which will provide additional information for user alignment. In turn, user alignment also reveals more clues for community alignment. Hence, in this paper, we introduce the problem of joint social network alignment, which aims to align users and communities across social networks simultaneously. Key challenges lie in that 1) how to learn the representations of both users and communities, and 2) how to make user alignment and community alignment benefit from each other. To address these challenges, we first elaborate on the characteristics of real-world networks with the notion of delta-hyperbolicity, and show the superiority of hyperbolic space for representing social networks. Then, we present a novel hyperbolic embedding approach for the joint social network alignment, referred to as PERFECT, in a unified optimization. Extensive experiments on real-world datasets show the superiority of PERFECT in both user alignment and community alignment.",https://arxiv.org/abs/2209.02911
Facilitating Global Team Meetings Between Language-Based Subgroups:When and How Can Machine Translation Help?,"YongleZhang, Dennis AsamoahOwusu, MarineCarpuat, Ge Gao",07-sep-22,Computation and Language (cs.CL)," Global teams frequently consist of language-based subgroups who put together complementary information to achieve common goals. Previous research outlines a two-step work communication flow in these teams. There are team meetings using a required common language (i.e., English); in preparation for those meetings, people have subgroup conversations in their native languages. Work communication at team meetings is often less effective than in subgroup conversations. In the current study, we investigate the idea of leveraging machine translation (MT) to facilitate global team meetings. We hypothesize that exchanging subgroup conversation logs before a team meeting offers contextual information that benefits teamwork at the meeting. MT can translate these logs, which enables comprehension at a low cost. To test our hypothesis, we conducted a between- subjects experiment where twenty quartets of participants performed a personnel selection task. Each quartet included two English native speakers (NS) and two non-native speakers (NNS) whose native language was Mandarin. All participants began the task with subgroup conversations in their native languages, then proceeded to team meetings in English. We manipulated the exchange of subgroup conversation logs prior to team meetings: with MT- mediated exchanges versus without. Analysis of participants' subjective experience, task performance, and depth of discussions as reflected through their conversational moves jointly indicates that team meeting quality improved when there were MT-mediated exchanges of subgroup conversation logs as opposed to no exchanges. We conclude with reflections on when and how MT could be applied to enhance global teamwork across a language barrier.",https://arxiv.org/abs/2209.02908
A Data-dependent Approach for High Dimensional (Robust) WassersteinAlignment,"HuDing, WenjieLiu, MingquanYe",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Many real-world problems can be formulated as the alignment between two geometric patterns. Previously, a great amount of research focus on the alignment of 2D or 3D patterns in the field of computer vision. Recently, the alignment problem in high dimensions finds several novel applications in practice. However, the research is still rather limited in the algorithmic aspect. To the best of our knowledge, most existing approaches are just simple extensions of their counterparts for 2D and 3D cases, and often suffer from the issues such as high computational complexities. In this paper, we propose an effective framework to compress the high dimensional geometric patterns. Any existing alignment method can be applied to the compressed geometric patterns and the time complexity can be significantly reduced. Our idea is inspired by the observation that high dimensional data often has a low intrinsic dimension. Our framework is a ""data-dependent"" approach that has the complexity depending on the intrinsic dimension of the input data. Our experimental results reveal that running the alignment algorithm on compressed patterns can achieve similar qualities, comparing with the results on the original patterns, but the runtimes (including the times cost for compression) are substantially lower.",https://arxiv.org/abs/2209.02906
On Geometric Alignment in Low Doubling Dimension,"HuDing, Mingquan Ye",19-nov-18,Machine Learning (cs.LG)," In real-world, many problems can be formulated as the alignment between two geometric patterns. Previously, a great amount of research focus on the alignment of 2D or 3D patterns, especially in the field of computer vision. Recently, the alignment of geometric patterns in high dimension finds several novel applications, and has attracted more and more attentions. However, the research is still rather limited in terms of algorithms. To the best of our knowledge, most existing approaches for high dimensional alignment are just simple extensions of their counterparts for 2D and 3D cases, and often suffer from the issues such as high complexities. In this paper, we propose an effective framework to compress the high dimensional geometric patterns and approximately preserve the alignment quality. As a consequence, existing alignment approach can be applied to the compressed geometric patterns and thus the time complexity is significantly reduced. Our idea is inspired by the observation that high dimensional data often has a low intrinsic dimension. We adopt the widely used notion ""doubling dimension"" to measure the extents of our compression and the resulting approximation. Finally, we test our method on both random and real datasets, the experimental results reveal that running the alignment algorithm on compressed patterns can achieve similar qualities, comparing with the results on the original patterns, but the running times (including the times cost for compression) are substantially lower.",https://arxiv.org/abs/2209.02905
Taking a Language Detour: How International Migrants Speaking aMinority Language Seek COVID-Related Information in Their Host Countries,"GeGao, JianZheng, EunKyoung Choe, NaomiYamashita",07-sep-22,Computers and Society (cs.CY)," Information seeking is crucial for people's self-care and wellbeing in times of public crises. Extensive research has investigated empirical understandings as well as technical solutions to facilitate information seeking by domestic citizens of affected regions. However, limited knowledge is established to support international migrants who need to survive a crisis in their host countries. The current paper presents an interview study with two cohorts of Chinese migrants living in Japan (N=14) and the United States (N=14). Participants reflected on their information seeking experiences during the COVID pandemic. The reflection was supplemented by two weeks of self-tracking where participants maintained records of their COVIDrelated information seeking practice. Our data indicated that participants often took language detours, or visits to Mandarin resources for information about the COVID outbreak in their host countries. They also made strategic use of the Mandarin information to perform selective reading, cross-checking, and contextualized interpretation of COVID-related information in Japanese or English. While such practices enhanced participants' perceived effectiveness of COVID-related information gathering and sensemaking, they disadvantaged people through sometimes incognizant ways. Further, participants lacked the awareness or preference to review migrant-oriented information that was issued by the host country's public authorities despite its availability. Building upon these findings, we discussed solutions to improve international migrants' COVID-related information seeking in their non-native language and cultural environment. We advocated inclusive crisis infrastructures that would engage people with diverse levels of local language fluency, information literacy, and experience in leveraging public services.",https://arxiv.org/abs/1811.07455
Defending Against Backdoor Attack on Graph Nerual Network byExplainability,"BingchenJiang, ZhaoLi",07-sep-22,Artificial Intelligence (cs.AI)," Backdoor attack is a powerful attack algorithm to deep learning model. Recently, GNN's vulnerability to backdoor attack has been proved especially on graph classification task. In this paper, we propose the first backdoor detection and defense method on GNN. Most backdoor attack depends on injecting small but influential trigger to the clean sample. For graph data, current backdoor attack focus on manipulating the graph structure to inject the trigger. We find that there are apparent differences between benign samples and malicious samples in some explanatory evaluation metrics, such as fidelity and infidelity. After identifying the malicious sample, the explainability of the GNN model can help us capture the most significant subgraph which is probably the trigger in a trojan graph. We use various dataset and different attack settings to prove the effectiveness of our defense method. The attack success rate all turns out to decrease considerably.",https://arxiv.org/abs/2209.02903
Context Recovery and Knowledge Retrieval: A Novel Two-Stream Frameworkfor Video Anomaly Detection,"CongqiCao, YueLu, YanningZhang",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Video anomaly detection aims to find the events in a video that do not conform to the expected behavior. The prevalent methods mainly detect anomalies by snippet reconstruction or future frame prediction error. However, the error is highly dependent on the local context of the current snippet and lacks the understanding of normality. To address this issue, we propose to detect anomalous events not only by the local context, but also according to the consistency between the testing event and the knowledge about normality from the training data. Concretely, we propose a novel two- stream framework based on context recovery and knowledge retrieval, where the two streams can complement each other. For the context recovery stream, we propose a spatiotemporal U-Net which can fully utilize the motion information to predict the future frame. Furthermore, we propose a maximum local error mechanism to alleviate the problem of large recovery errors caused by complex foreground objects. For the knowledge retrieval stream, we propose an improved learnable locality-sensitive hashing, which optimizes hash functions via a Siamese network and a mutual difference loss. The knowledge about normality is encoded and stored in hash tables, and the distance between the testing event and the knowledge representation is used to reveal the probability of anomaly. Finally, we fuse the anomaly scores from the two streams to detect anomalies. Extensive experiments demonstrate the effectiveness and complementarity of the two streams, whereby the proposed two-stream framework achieves state-of-the-art performance on four datasets.",https://arxiv.org/abs/2209.02902
An augmented fully-mixed formulation for the quasistatic Navier--Stokes--Biot model,"TongtongLi, SergioCaucao, Ivan Yotov",07-sep-22,Numerical Analysis (math.NA)," We introduce and analyze a partially augmented fully-mixed formulation and a mixed finite element method for the coupled problem arising in the interaction between a free fluid and a poroelastic medium. The flows in the free fluid and poroelastic regions are governed by the Navier-Stokes and Biot equations, respectively, and the transmission conditions are given by mass conservation, balance of fluid force, conservation of momentum, and the Beavers-Joseph-Saffman condition. We apply dual-mixed formulations in both domains, where the symmetry of the Navier- Stokes and poroelastic stress tensors is imposed in an ultra-weak and weak sense. In turn, since the transmission conditions are essential in the fully mixed formulation, they are imposed weakly by introducing the traces of the structure velocity and the poroelastic medium pressure on the interface as the associated Lagrange multipliers. Furthermore, since the fluid convective term requires the velocity to live in a smaller space than usual, we augment the variational formulation with suitable Galerkin type terms. Existence and uniqueness of a solution are established for the continuous weak formulation, as well as a semidiscrete continuous-in-time formulation with non-matching grids, together with the corresponding stability bounds and error analysis with rates of convergence. Several numerical experiments are presented to verify the theoretical results and illustrate the performance of the method for applications to arterial flow and flow through a filter.",https://arxiv.org/abs/2209.02899
A Systematical Evaluation for Next-Basket Recommendation Algorithms,"ZhufengShao, ShoujinWang, QianZhang, Wenpeng Lu, Zhao Li, Xueping Peng",07-sep-22,Information Retrieval (cs.IR)," Next basket recommender systems (NBRs) aim to recommend a user's next (shopping) basket of items via modeling the user's preferences towards items based on the user's purchase history, usually a sequence of historical baskets. Due to its wide applicability in the real-world E-commerce industry, the studies NBR have attracted increasing attention in recent years. NBRs have been widely studied and much progress has been achieved in this area with a variety of NBR approaches having been proposed. However, an important issue is that there is a lack of a systematic and unified evaluation over the various NBR approaches. Different studies often evaluate NBR approaches on different datasets, under different experimental settings, making it hard to fairly and effectively compare the performance of different NBR approaches. To bridge this gap, in this work, we conduct a systematical empirical study in NBR area. Specifically, we review the representative work in NBR and analyze their cons and pros. Then, we run the selected NBR algorithms on the same datasets, under the same experimental setting and evaluate their performances using the same measurements. This provides a unified framework to fairly compare different NBR approaches. We hope this study can provide a valuable reference for the future research in this vibrant area.",https://arxiv.org/abs/2209.02894
Toward Data-Driven Radar STAP,"ShyamVenkatasubramanian, SandeepGogineni, Bosung Kang, AliPezeshki, MuralidharRangaswamy, VahidTarokh",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Catalyzed by the recent emergence of site-specific, high-fidelity radio frequency (RF) modeling and simulation tools purposed for radar, data- driven formulations of classical methods in radar have rapidly grown in popularity over the past decade. Despite this surge, limited focus has been directed toward the theoretical foundations of these classical methods. In this regard, as part of our ongoing data-driven approach to radar space-time adaptive processing (STAP), we analyze the asymptotic performance guarantees of select subspace separation methods in the context of radar target localization, and augment this analysis through a proposed deep learning framework for target location estimation. In our approach, we generate comprehensive datasets by randomly placing targets of variable strengths in predetermined constrained areas using RFView, a site-specific RF modeling and simulation tool developed by ISL Inc. For each radar return signal from these constrained areas, we generate heatmap tensors in range, azimuth, and elevation of the normalized adaptive matched filter (NAMF) test statistic, and of the output power of a generalized sidelobe canceller (GSC). Using our deep learning framework, we estimate target locations from these heatmap tensors to demonstrate the feasibility of and significant improvements provided by our data-driven approach in matched and mismatched settings.",https://arxiv.org/abs/2209.02892
Toward Data-Driven STAP Radar,"ShyamVenkatasubramanian, ChayutWongkamthong, MohammadrezaSoltani, Bosung Kang, SandeepGogineni, AliPezeshki, MuralidharRangaswamy, VahidTarokh","26 Jan 2022 (v1(https://arxiv.org/abs/2201.10712v1)), lastrevised 10 Mar 2022 (this version, v2)",Computer Vision and Pattern Recognition (cs.CV)," Using an amalgamation of techniques from classical radar, computer vision, and deep learning, we characterize our ongoing data-driven approach to space-time adaptive processing (STAP) radar. We generate a rich example dataset of received radar signals by randomly placing targets of variable strengths in a predetermined region using RFView, a site-specific radio frequency modeling and simulation tool developed by ISL Inc. For each data sample within this region, we generate heatmap tensors in range, azimuth, and elevation of the output power of a minimum variance distortionless response (MVDR) beamformer, which can be replaced with a desired test statistic. These heatmap tensors can be thought of as stacked images, and in an airborne scenario, the moving radar creates a sequence of these time- indexed image stacks, resembling a video. Our goal is to use these images and videos to detect targets and estimate their locations, a procedure reminiscent of computer vision algorithms for object detection$-$namely, the Faster Region-Based Convolutional Neural Network (Faster R-CNN). The Faster R-CNN consists of a proposal generating network for determining regions of interest (ROI), a regression network for positioning anchor boxes around targets, and an object classification algorithm; it is developed and optimized for natural images. Our ongoing research will develop analogous tools for heatmap images of radar data. In this regard, we will generate a large, representative adaptive radar signal processing database for training and testing, analogous in spirit to the COCO dataset for natural images. As a preliminary example, we present a regression network in this paper for estimating target locations to demonstrate the feasibility of and significant improvements provided by our data-driven approach.",https://arxiv.org/abs/2209.02890
KT-BT: A Framework for Knowledge Transfer Through Behavior Trees inMulti-Robot Systems,"Sanjay Sarma OrugantiVenkata, RamviyasParasuraman, RamanaPidaparti",07-sep-22,Robotics (cs.RO)," Multi-Robot and Multi-Agent Systems demonstrate collective (swarm) intelligence through systematic and distributed integration of local behaviors in a group. Agents sharing knowledge about the mission and environment can enhance performance at individual and mission levels. However, this is difficult to achieve, partly due to the lack of a generic framework for transferring part of the known knowledge (behaviors) between agents. This paper presents a new knowledge representation framework and a transfer strategy called KT-BT: Knowledge Transfer through Behavior Trees. The KT-BT framework follows a query-response-update mechanism through an online Behavior Tree framework, where agents broadcast queries for unknown conditions and respond with appropriate knowledge using a condition-action- control sub-flow. We embed a novel grammar structure called stringBT that encodes knowledge, enabling behavior sharing. We theoretically investigate the properties of the KT-BT framework in achieving homogeneity of high knowledge across the entire group compared to a heterogeneous system without the capability of sharing their knowledge. We extensively verify our framework in a simulated multi-robot search and rescue problem. The results show successful knowledge transfers and improved group performance in various scenarios. We further study the effects of opportunities and communication range on group performance, knowledge spread, and functional heterogeneity in a group of agents, presenting interesting insights.",https://arxiv.org/abs/2201.10712
Multi-Grained Angle Representation for Remote Sensing Object Detection,"HaoWang, ZhanchaoHuang, ZhengchaoChen, YingSong, WeiLi",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Arbitrary-oriented object detection (AOOD) plays a significant role for image understanding in remote sensing scenarios. The existing AOOD methods face the challenges of ambiguity and high costs in angle representation. To this end, a multi-grained angle representation (MGAR) method, consisting of coarse-grained angle classification (CAC) and fine- grained angle regression (FAR), is proposed. Specifically, the designed CAC avoids the ambiguity of angle prediction by discrete angular encoding (DAE) and reduces complexity by coarsening the granularity of DAE. Based on CAC, FAR is developed to refine the angle prediction with much lower costs than narrowing the granularity of DAE. Furthermore, an Intersection over Union (IoU) aware FAR-Loss (IFL) is designed to improve accuracy of angle prediction using an adaptive re-weighting mechanism guided by IoU. Extensive experiments are performed on several public remote sensing datasets, which demonstrate the effectiveness of the proposed MGAR. Moreover, experiments on embedded devices demonstrate that the proposed MGAR is also friendly for lightweight deployments.",https://arxiv.org/abs/2209.02886
Sgap: Towards Efficient Sparse Tensor Algebra Compilation for GPU,"GenghanZhang, Yuetong Zhao, Yanting Tao, Zhongming Yu, Guohao Dai, Sitao Huang, Yuan Wen, PavlosPetoumenos, Yu Wang",07-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," Sparse compiler is a promising solution for sparse tensor algebra optimization. In compiler implementation, reduction in sparse-dense hybrid algebra plays a key role in performance. Though GPU provides various reduction semantics that can better utilize the parallel computing and memory bandwidth capacity, the central question is: how to elevate the flexible reduction semantics to sparse compilation theory that assumes serial execution. Specifically, we have to tackle two main challenges: (1) there are wasted parallelism by adopting static synchronization granularity (2) static reduction strategy limits optimization space exploration. We propose Sgap: segment group and atomic parallelism to solve these problems. Atomic parallelism captures the flexible reduction semantics to systematically analyze the optimization space of sparse-dense hybrid algebra on GPU. It is a new optimization technique beyond current compiler-based and open-source runtime libraries. Segment group elevates the flexible reduction semantics to suitable levels of abstraction in the sparse compilation theory. It adopts changeable group size and user-defined reduction strategy to solve challenge (1) and (2), respectively. Finally, we use GPU sparse matrix-matrix multiplication (SpMM) on the TACO compiler as a use case to demonstrate the effectiveness of segment group in reduction semantics elevation. We achieve up to 1.2x speedup over the original TACO's SpMM kernels. We also apply new optimization techniques found by atomic parallelism to an open-source state-of-the-art SpMM library dgSPARSE. We achieve 1.6x - 2.3x speedup on the algorithm tuned with atomic parallelism.",https://arxiv.org/abs/2209.02884
GPU implementation of a ray-surface intersection algorithm in CUDA(Compute Unified Device Architecture),RaymondLeung,07-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," These notes accompany the open-source code published in GitHub which implements a GPU-based line-segment, surface-triangle intersection algorithm in CUDA. It mentions some relevant works and discusses issues specific to this implementation. The goal is to provide software documentation and greater clarity on collision buffer management which is sometimes omitted in online literature. For real-world applications, CPU- based implementations of the test are often deemed too slow to be useful. In contrast, the code described here targets Nvidia GPU devices and offers a solution that is vastly more efficient and scalable. The main API is also wrapped in Python. This geometry test is applied in various engineering problems, so the software developed can be reused in new situations.",https://arxiv.org/abs/2209.02882
SUNet: Scale-aware Unified Network for Panoptic Segmentation,"WeihaoYan, YeqiangQian, ChunxiangWang, MingYang",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Panoptic segmentation combines the advantages of semantic and instance segmentation, which can provide both pixel-level and instance-level environmental perception information for intelligent vehicles. However, it is challenged with segmenting objects of various scales, especially on extremely large and small ones. In this work, we propose two lightweight modules to mitigate this problem. First, Pixel-relation Block is designed to model global context information for large-scale things, which is based on a query-independent formulation and brings small parameter increments. Then, Convectional Network is constructed to collect extra high-resolution information for small-scale stuff, supplying more appropriate semantic features for the downstream segmentation branches. Based on these two modules, we present an end-to-end Scale-aware Unified Network (SUNet), which is more adaptable to multi-scale objects. Extensive experiments on Cityscapes and COCO demonstrate the effectiveness of the proposed methods.",https://arxiv.org/abs/2209.02878
Self-supervised multimodal neuroimaging yields predictiverepresentations for a spectrum of Alzheimer's phenotypes,"AlexFedorov, EloyGeenjaar, Lei Wu, TristanSylvain, Thomas P.DeRamus, Margaux Luck, MariaMisiura, RDevonHjelm, Sergey M.Plis, VinceD.Calhoun",07-sep-22,Machine Learning (cs.LG)," Recent neuroimaging studies that focus on predicting brain disorders via modern machine learning approaches commonly include a single modality and rely on supervised over-parameterized models.However, a single modality provides only a limited view of the highly complex brain. Critically, supervised models in clinical settings lack accurate diagnostic labels for training. Coarse labels do not capture the long-tailed spectrum of brain disorder phenotypes, which leads to a loss of generalizability of the model that makes them less useful in diagnostic settings. This work presents a novel multi-scale coordinated framework for learning multiple representations from multimodal neuroimaging data. We propose a general taxonomy of informative inductive biases to capture unique and joint information in multimodal self-supervised fusion. The taxonomy forms a family of decoder-free models with reduced computational complexity and a propensity to capture multi-scale relationships between local and global representations of the multimodal inputs. We conduct a comprehensive evaluation of the taxonomy using functional and structural magnetic resonance imaging (MRI) data across a spectrum of Alzheimer's disease phenotypes and show that self-supervised models reveal disorder-relevant brain regions and multimodal links without access to the labels during pre- training. The proposed multimodal self-supervised learning yields representations with improved classification performance for both modalities. The concomitant rich and flexible unsupervised deep learning framework captures complex multimodal relationships and provides predictive performance that meets or exceeds that of a more narrow supervised classification analysis. We present elaborate quantitative evidence of how this framework can significantly advance our search for missing links in complex brain disorders.",https://arxiv.org/abs/2209.02877
Compact schemes for variable coefficient convection-diffusionequations,"AnindyaGoswami, Kuldip SinghPatel, Pradeep KumarSahu",07-sep-22,Numerical Analysis (math.NA)," Fourth order accurate compact schemes for variable coefficient convection-diffusion equations are considered. A sufficient condition for stability of the schemes have been derived using a difference equation based approach. The constant coefficient problems are considered as a special case, and the unconditional stability of compact schemes for such case is proved theoretically. The condition number of the amplification matrix is also analysed, and an estimate for the same is derived. In order to verify the derived conditions numerically, MATLAB codes are provided in Appendix of the manuscript. An example is provided to support the assumption taken to assure stability.",https://arxiv.org/abs/2209.02876
Improving Choral Music Separation through Expressive Synthesized Datafrom Sampled Instruments,"KeChen, Hao-WenDong, YiLuo, JulianMcAuley, Taylor Berg-Kirkpatrick, MillerPuckette, ShlomoDubnov",07-sep-22,Sound (cs.SD)," Choral music separation refers to the task of extracting tracks of voice parts (e.g., soprano, alto, tenor, and bass) from mixed audio. The lack of datasets has impeded research on this topic as previous work has only been able to train and evaluate models on a few minutes of choral music data due to copyright issues and dataset collection difficulties. In this paper, we investigate the use of synthesized training data for the source separation task on real choral music. We make three contributions: first, we provide an automated pipeline for synthesizing choral music data from sampled instrument plugins within controllable options for instrument expressiveness. This produces an 8.2-hour-long choral music dataset from the JSB Chorales Dataset and one can easily synthesize additional data. Second, we conduct an experiment to evaluate multiple separation models on available choral music separation datasets from previous work. To the best of our knowledge, this is the first experiment to comprehensively evaluate choral music separation. Third, experiments demonstrate that the synthesized choral data is of sufficient quality to improve the model's performance on real choral music datasets. This provides additional experimental statistics and data support for the choral music separation study.",https://arxiv.org/abs/2209.02873
Interpretations Steered Network Pruning via Amortized InferredSaliency Maps,"AlirezaGanjdanesh, Shangqian Gao, Heng Huang",07-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Convolutional Neural Networks (CNNs) compression is crucial to deploying these models in edge devices with limited resources. Existing channel pruning algorithms for CNNs have achieved plenty of success on complex models. They approach the pruning problem from various perspectives and use different metrics to guide the pruning process. However, these metrics mainly focus on the model's `outputs' or `weights' and neglect its `interpretations' information. To fill in this gap, we propose to address the channel pruning problem from a novel perspective by leveraging the interpretations of a model to steer the pruning process, thereby utilizing information from both inputs and outputs of the model. However, existing interpretation methods cannot get deployed to achieve our goal as either they are inefficient for pruning or may predict non-coherent explanations. We tackle this challenge by introducing a selector model that predicts real- time smooth saliency masks for pruned models. We parameterize the distribution of explanatory masks by Radial Basis Function (RBF)-like functions to incorporate geometric prior of natural images in our selector model's inductive bias. Thus, we can obtain compact representations of explanations to reduce the computational costs of our pruning method. We leverage our selector model to steer the network pruning by maximizing the similarity of explanatory representations for the pruned and original models. Extensive experiments on CIFAR-10 and ImageNet benchmark datasets demonstrate the efficacy of our proposed method. Our implementations are available at \url{[this https URL](https://github.com/Alii- Ganjj/InterpretationsSteeredPruning)}",https://arxiv.org/abs/2209.02871
Numerical investigation and factor analysis of the spatial-temporalmulti-species competition problem,"MariaVasilyeva, YouwenWang, SergeiStepanov, AlexeySadovski",07-sep-22,Numerical Analysis (math.NA)," In this work, we consider the spatial-temporal multi-species competition model. A mathematical model is described by a coupled system of nonlinear diffusion-reaction equations. We use a finite volume approximation with semi-implicit time approximation for the numerical solution of the model with corresponding boundary and initial conditions. To understand the effect of the diffusion to solution in one and two-dimensional formulations, we present numerical results for several cases of the parameters related to the survival scenarios. The random initial conditions' effect on the time to reach equilibrium is investigated. The influence of diffusion on the survival scenarios is presented. In real-world problems, values of the parameters are usually unknown and vary in some range. In order to evaluate the impact of parameters on the system stability, we simulate a spatial- temporal model with random parameters and perform factor analysis for two and three-species competition models.",https://arxiv.org/abs/2209.02869
Algorithmic Learning Foundations for Common Law,"JasonHartline, Daniel W. LinnaJr., LirenShan, AlexTang",07-sep-22,Computers and Society (cs.CY)," This paper looks at a common law legal system as a learning algorithm, models specific features of legal proceedings, and asks whether this system learns efficiently. A particular feature of our model is explicitly viewing various aspects of court proceedings as learning algorithms. This viewpoint enables directly pointing out that when the costs of going to court are not commensurate with the benefits of going to court, there is a failure of learning and inaccurate outcomes will persist in cases that settle. Specifically, cases are brought to court at an insufficient rate. On the other hand, when individuals can be compelled or incentivized to bring their cases to court, the system can learn and inaccuracy vanishes over time.",https://arxiv.org/abs/2209.02867
DC-MRTA: Decentralized Multi-Robot Task Allocation and Navigation inComplex Environments,"AakritiAgrawal, SenthilHariharan, Amrit SinghBedi, DineshManocha",07-sep-22,Robotics (cs.RO)," We present a novel reinforcement learning (RL) based task allocation and decentralized navigation algorithm for mobile robots in warehouse environments. Our approach is designed for scenarios in which multiple robots are used to perform various pick up and delivery tasks. We consider the problem of joint decentralized task allocation and navigation and present a two level approach to solve it. At the higher level, we solve the task allocation by formulating it in terms of Markov Decision Processes and choosing the appropriate rewards to minimize the Total Travel Delay (TTD). At the lower level, we use a decentralized navigation scheme based on ORCA that enables each robot to perform these tasks in an independent manner, and avoid collisions with other robots and dynamic obstacles. We combine these lower and upper levels by defining rewards for the higher level as the feedback from the lower level navigation algorithm. We perform extensive evaluation in complex warehouse layouts with large number of agents and highlight the benefits over state-of-the-art algorithms based on myopic pickup distance minimization and regret-based task selection. We observe improvement up to 14% in terms of task completion time and up-to 40% improvement in terms of computing collision-free trajectories for the robots.",https://arxiv.org/abs/2209.02866
On the Convergence of Monte Carlo UCB for Random-Length Episodic MDPs,"ZixuanDong, CheWang, KeithRoss",07-sep-22,Machine Learning (cs.LG)," In reinforcement learning, Monte Carlo algorithms update the Q function by averaging the episodic returns. In the Monte Carlo UCB (MC-UCB) algorithm, the action taken in each state is the action that maximizes the Q function plus a UCB exploration term, which biases the choice of actions to those that have been chosen less frequently. Although there has been significant work on establishing regret bounds for MC-UCB, most of that work has been focused on finite-horizon versions of the problem, for which each episode terminates after a constant number of steps. For such finite-horizon problems, the optimal policy depends both on the current state and the time within the episode. However, for many natural episodic problems, such as games like Go and Chess and robotic tasks, the episode is of random length and the optimal policy is stationary. For such environments, it is an open question whether the Q-function in MC-UCB will converge to the optimal Q function; we conjecture that, unlike Q-learning, it does not converge for all MDPs. We nevertheless show that for a large class of MDPs, which includes stochastic MDPs such as blackjack and deterministic MDPs such as Go, the Q-function in MC-UCB converges almost surely to the optimal Q function. An immediate corollary of this result is that it also converges almost surely for all finite-horizon MDPs. We also provide numerical experiments, providing further insights into MC-UCB.",https://arxiv.org/abs/2209.02865
DAVE Aquatic Virtual Environment: Toward a General Underwater RoboticsSimulator,"Mabel M.Zhang, Woen-SugChoi, JessicaHerman, Duane Davis, Carson Vogt, MichaelMcCarrin, YadunundVijay, DhariniDutia, William Lew, StevenPeters, BrianBingham",06-sep-22,Robotics (cs.RO)," We present DAVE Aquatic Virtual Environment (DAVE), an open source simulation stack for underwater robots, sensors, and environments. Conventional robotics simulators are not designed to address unique challenges that come with the marine environment, including but not limited to environment conditions that vary spatially and temporally, impaired or challenging perception, and the unavailability of data in a generally unexplored environment. Given the variety of sensors and platforms, wheels are often reinvented for specific use cases that inevitably resist wider adoption.   Building on existing simulators, we provide a framework to help speed up the development and evaluation of algorithms that would otherwise require expensive and time-consuming operations at sea. The framework includes basic building blocks (e.g., new vehicles, water-tracking Doppler Velocity Logger, physics-based multibeam sonar) as well as development tools (e.g., dynamic bathymetry spawning, ocean currents), which allows the user to focus on methodology rather than software infrastructure. We demonstrate usage through example scenarios, bathymetric data import, user interfaces for data inspection and motion planning for manipulation, and visualizations.",https://arxiv.org/abs/2209.02864
The Role of Voice Persona in Expressive Communication:An Argument forRelevance in Speech Synthesis Design,"CamilleNoufi, LloydMay, JonathanBerger",06-sep-22,Sound (cs.SD)," We present an approach to imbuing expressivity in a synthesized voice by acquiring a thematic analysis of 10 interviews with vocal studies and performance experts to inform the design framework for a real-time, interactive vocal persona that would generate compelling and appropriate contextually-dependent expression. The resultant tone of voice is defined as a point existing within a continuous, contextually-dependent probability space. The inclusion of voice persona in synthesized voice can be significant in a broad range of applications. Of particular interest is the potential impact in augmentative and assistive communication (AAC) community. Finally, we conclude with an introduction to our ongoing research investigating the themes of vocal persona and how they may continue to inform proposed expressive speech synthesis design frameworks.",https://arxiv.org/abs/2209.02862
"Second order, unconditionally stable, linear ensemble algorithms forthe magnetohydrodynamics equations","JohnCarter, Daozhi Han, Nan Jiang",06-sep-22,Numerical Analysis (math.NA)," We propose two unconditionally stable, linear ensemble algorithms with pre-computable shared coefficient matrices across different realizations for the magnetohydrodynamics equations. The viscous terms are treated by a standard perturbative discretization. The nonlinear terms are discretized fully explicitly within the framework of the generalized positive auxiliary variable approach (GPAV). Artificial viscosity stabilization that modifies the kinetic energy is introduced to improve accuracy of the GPAV ensemble methods. Numerical results are presented to demonstrate the accuracy and robustness of the ensemble algorithms.",https://arxiv.org/abs/2209.02855
Code Code Evolution: Understanding How People Change Data ScienceNotebooks Over Time,"DeepthiRaghunandan, Aayushi Roy, Shenzhi Shi, NiklasElmqvist, LeilaniBattle","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02851v1)), lastrevised 8 Sep 2022 (this version, v2)",Human-Computer Interaction (cs.HC)," Sensemaking is the iterative process of identifying, extracting, and explaining insights from data, where each iteration is referred to as the ""sensemaking loop."" Although recent work observes snapshots of the sensemaking loop within computational notebooks, none measure shifts in sensemaking behaviors over time -- between exploration and explanation. This gap limits our ability to understand the full scope of the sensemaking process and thus our ability to design tools to fully support sensemaking. We contribute the first quantitative method to characterize how sensemaking evolves within data science computational notebooks. To this end, we conducted a quantitative study of 2,574 Jupyter notebooks mined from GitHub. First, we identify data science-focused notebooks that have undergone significant iterations. Second, we present regression models that automatically characterize sensemaking activity within individual notebooks by assigning them a score representing their position within the sensemaking spectrum. Finally, we use our regression models to calculate and analyze shifts in notebook scores across GitHub versions. Our results show that notebook authors participate in a diverse range of sensemaking tasks over time, such as annotation, branching analysis, and documentation. Finally, we propose design recommendations for extending notebook environments to support the sensemaking behaviors we observed.",https://arxiv.org/abs/2209.02853
Inversion of Time-Lapse Surface Gravity Data for Detection of 3DCO$_2$ Plumes via Deep Learning,"AdrianCelaya, BertrandDenel, YenSun, MauricioAraya-Polo, AntonyPrice",06-sep-22,Machine Learning (cs.LG)," We introduce three algorithms that invert simulated gravity data to 3D subsurface rock/flow properties. The first algorithm is a data-driven, deep learning-based approach, the second mixes a deep learning approach with physical modeling into a single workflow, and the third considers the time dependence of surface gravity monitoring. The target application of these proposed algorithms is the prediction of subsurface CO$_2$ plumes as a complementary tool for monitoring CO$_2$ sequestration deployments. Each proposed algorithm outperforms traditional inversion methods and produces high-resolution, 3D subsurface reconstructions in near real-time. Our proposed methods achieve Dice scores of up to 0.8 for predicted plume geometry and near perfect data misfit in terms of $\mu$Gals. These results indicate that combining 4D surface gravity monitoring with deep learning techniques represents a low-cost, rapid, and non-intrusive method for monitoring CO$_2$ storage sites.",https://arxiv.org/abs/2209.02851
Adaptive Complexity Model Predictive Control,"JosephNorby, ArdalanTajbakhsh, Yanhao Yang, Aaron M.Johnson",06-sep-22,Robotics (cs.RO)," This work introduces a formulation of model predictive control (MPC) which adaptively reasons about the complexity of the model based on the task while maintaining feasibility and stability guarantees. Existing MPC implementations often handle computational complexity by shortening prediction horizons or simplifying models, both of which can result in instability. Inspired by related approaches in behavioral economics, motion planning, and biomechanics, our method solves MPC problems with a simple model for dynamics and constraints over regions of the horizon where such a model is feasible and a complex model where it is not. The approach leverages an interleaving of planning and execution to iteratively identify these regions, which can be safely simplified if they satisfy an exact template/anchor relationship. We show that this method does not compromise the stability and feasibility properties of the system, and measure performance in simulation experiments on a quadrupedal robot executing agile behaviors over terrains of interest. We find that this adaptive method enables more agile motion and expands the range of executable tasks compared to fixed-complexity implementations.",https://arxiv.org/abs/2209.02850
Traffic State Estimation for Connected Vehicles using the Second-OrderAw-Rascle-Zhang Traffic Model,"Suyash C.Vishnoi, Sebastian A.Nugroho, Ahmad F.Taha, Christian G.Claudel",06-sep-22,Systems and Control (eess.SY)," This paper addresses the problem of traffic state estimation (TSE) in the presence of heterogeneous sensors which include both fixed and moving sensors. Traditional fixed sensors are expensive and cannot be installed throughout the highway. Moving sensors such as Connected Vehicles (CVs) offer a relatively cheap alternative to measure traffic states across the network. Moving forward it is thus important to develop such models that effectively use the data from CVs. One such model is the nonlinear second- order Aw-Rascle-Zhang (ARZ) model which is a realistic traffic model, reliable for TSE and control. A state-space formulation is presented for the ARZ model considering junctions in the formulation which is important to model real highways with ramps. Linear approximation of the state-space model is investigated with respect to two techniques, first-order Taylor series approximation and Carleman linearization. A Moving Horizon Estimation (MHE) implementation is presented for TSE using a linearized ARZ model. Various state-estimation techniques used for TSE in the literature along with the presented approach are compared with regard to accuracy, computational tractability and parameter tuning with the help of a case study using the VISSIM traffic simulation software. Several research questions are posed and addressed with thorough analysis of the results.",https://arxiv.org/abs/2209.02849
DC-Art-GAN: Stable Procedural Content Generation using DC-GANs forDigital Art,"RohitGandikota, Nik BearBrown",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Art is an artistic method of using digital technologies as a part of the generative or creative process. With the advent of digital currency and NFTs (Non-Fungible Token), the demand for digital art is growing aggressively. In this manuscript, we advocate the concept of using deep generative networks with adversarial training for a stable and variant art generation. The work mainly focuses on using the Deep Convolutional Generative Adversarial Network (DC-GAN) and explores the techniques to address the common pitfalls in GAN training. We compare various architectures and designs of DC-GANs to arrive at a recommendable design choice for a stable and realistic generation. The main focus of the work is to generate realistic images that do not exist in reality but are synthesised from random noise by the proposed model. We provide visual results of generated animal face images (some pieces of evidence showing a blend of species) along with recommendations for training, architecture and design choices. We also show how training image preprocessing plays a massive role in GAN training.",https://arxiv.org/abs/2209.02848
Building up Cyber Resilience by Better Grasping Cyber Risk Via a NewAlgorithm for Modelling Heavy-Tailed Data,"MichelDacorogna, NehlaDebbabi, Marie Kratz",06-sep-22,Cryptography and Security (cs.CR)," Cyber security and resilience are major challenges in our modern economies; this is why they are top priorities on the agenda of governments, security and defense forces, management of companies and organizations. Hence, the need of a deep understanding of cyber risks to improve resilience. We propose here an analysis of the database of the cyber complaints filed at the {\it Gendarmerie Nationale}. We perform this analysis with a new algorithm developed for non-negative asymmetric heavy- tailed data, which could become a handy tool in applied fields. This method gives a good estimation of the full distribution including the tail. Our study confirms the finiteness of the loss expectation, necessary condition for insurability. Finally, we draw the consequences of this model for risk management, compare its results to other standard EVT models, and lay the ground for a classification of attacks based on the fatness of the tail.",https://arxiv.org/abs/2209.02847
ASR2K: Speech Recognition for Around 2000 Languages without Audio,"XinjianLi, FlorianMetze, DavidRMortensen, Alan WBlack, ShinjiWatanabe",06-sep-22,Computation and Language (cs.CL)," Most recent speech recognition models rely on large supervised datasets, which are unavailable for many low-resource languages. In this work, we present a speech recognition pipeline that does not require any audio for the target language. The only assumption is that we have access to raw text datasets or a set of n-gram statistics. Our speech pipeline consists of three components: acoustic, pronunciation, and language models. Unlike the standard pipeline, our acoustic and pronunciation models use multilingual models without any supervision. The language model is built using n-gram statistics or the raw text dataset. We build speech recognition for 1909 languages by combining it with Crubadan: a large endangered languages n-gram database. Furthermore, we test our approach on 129 languages across two datasets: Common Voice and CMU Wilderness dataset. We achieve 50% CER and 74% WER on the Wilderness dataset with Crubadan statistics only and improve them to 45% CER and 69% WER when using 10000 raw text utterances.",https://arxiv.org/abs/2209.02845
A Fourth-Order Embedded Boundary Finite Volume Method for the UnsteadyStokes Equations with Complex Geometries,"Nathaniel Overton-Katz, XinfengGao, StephenGuzik, OscarAntepara, Daniel T.Graves, HansJohansen",06-sep-22,Numerical Analysis (math.NA)," A fourth-order finite volume embedded boundary (EB) method is presented for the unsteady Stokes equations. The algorithm represents complex geometries on a Cartesian grid using EB, employing a technique to mitigate the ""small cut-cell"" problem without mesh modifications, cell merging, or state redistribution. Spatial discretizations are based on a weighted least-squares technique that has been extended to fourth-order operators and boundary conditions, including an approximate projection to enforce the divergence-free constraint. Solutions are advanced in time using a fourth-order additive implicit-explicit Runge-Kutta method, with the viscous and source terms treated implicitly and explicitly, respectively. Formal accuracy of the method is demonstrated with several grid convergence studies, and results are shown for an application with a complex bio- inspired material. The developed method achieves fourth-order accuracy and is stable despite the pervasive small cells arising from complex geometries.",https://arxiv.org/abs/2209.02842
A Zeroth-Order Momentum Method for Risk-Averse Online Convex Games,"ZifanWang, YiShen, ZacharyI. Bell, ScottNivison, Michael M.Zavlanos, Karl H.Johansson",06-sep-22,Machine Learning (cs.LG)," We consider risk-averse learning in repeated unknown games where the goal of the agents is to minimize their individual risk of incurring significantly high cost. Specifically, the agents use the conditional value at risk (CVaR) as a risk measure and rely on bandit feedback in the form of the cost values of the selected actions at every episode to estimate their CVaR values and update their actions. A major challenge in using bandit feedback to estimate CVaR is that the agents can only access their own cost values, which, however, depend on the actions of all agents. To address this challenge, we propose a new risk-averse learning algorithm with momentum that utilizes the full historical information on the cost values. We show that this algorithm achieves sub-linear regret and matches the best known algorithms in the literature. We provide numerical experiments for a Cournot game that show that our method outperforms existing methods.",https://arxiv.org/abs/2209.02840
Studying Bias in GANs through the Lens of Race,"Vongani H.Maluleke, NeerjaThakkar, Tim Brooks, Ethan Weber, TrevorDarrell, Alexei A.Efros, AngjooKanazawa, DevinGuillory",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In this work, we study how the performance and evaluation of generative image models are impacted by the racial composition of their training datasets. By examining and controlling the racial distributions in various training datasets, we are able to observe the impacts of different training distributions on generated image quality and the racial distributions of the generated images. Our results show that the racial compositions of generated images successfully preserve that of the training data. However, we observe that truncation, a technique used to generate higher quality images during inference, exacerbates racial imbalances in the data. Lastly, when examining the relationship between image quality and race, we find that the highest perceived visual quality images of a given race come from a distribution where that race is well-represented, and that annotators consistently prefer generated images of white people over those of Black people.",https://arxiv.org/abs/2209.02838
Security and Privacy of IP-ICN Coexistence: A Comprehensive Survey,"EnkeledaBardhi, Mauro Conti, RiccardoLazzeretti, EleonoraLosiouk",06-sep-22,Cryptography and Security (cs.CR)," Internet usage has changed from its first design. Hence, the current Internet must cope with some limitations, including performance degradation, availability of IP addresses, and multiple security and privacy issues. Nevertheless, to unsettle the current Internet's network layer i.e., Internet Protocol with ICN is a challenging, expensive task. It also requires worldwide coordination among Internet Service Providers , backbone, and Autonomous Services. Additionally, history showed that technology changes e.g., from 3G to 4G, from IPv4 to IPv6 are not immediate, and usually, the replacement includes a long coexistence period between the old and new technology. Similarly, we believe that the process of replacement of the current Internet will surely transition through the coexistence of IP and ICN. Although the tremendous amount of security and privacy issues of the current Internet taught us the importance of securely designing the architectures, only a few of the proposed architectures place the security- by-design. Therefore, this article aims to provide the first comprehensive Security and Privacy analysis of the state-of-the-art coexistence architectures. Additionally, it yields a horizontal comparison of security and privacy among three deployment approaches of IP and ICN protocol i.e., overlay, underlay, and hybrid and a vertical comparison among ten considered security and privacy features. As a result of our analysis, emerges that most of the architectures utterly fail to provide several SP features including data and traffic flow confidentiality, availability and communication anonymity. We believe this article draws a picture of the secure combination of current and future protocol stacks during the coexistence phase that the Internet will definitely walk across.",https://arxiv.org/abs/2209.02836
Unsupervised Scene Sketch to Photo Synthesis,"JiayunWang, SangryulJeon, StellaX. Yu, XiZhang, HimanshuArora, YuLou",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Sketches make an intuitive and powerful visual expression as they are fast executed freehand drawings. We present a method for synthesizing realistic photos from scene sketches. Without the need for sketch and photo pairs, our framework directly learns from readily available large-scale photo datasets in an unsupervised manner. To this end, we introduce a standardization module that provides pseudo sketch-photo pairs during training by converting photos and sketches to a standardized domain, i.e. the edge map. The reduced domain gap between sketch and photo also allows us to disentangle them into two components: holistic scene structures and low- level visual styles such as color and texture. Taking this advantage, we synthesize a photo-realistic image by combining the structure of a sketch and the visual style of a reference photo. Extensive experimental results on perceptual similarity metrics and human perceptual studies show the proposed method could generate realistic photos with high fidelity from scene sketches and outperform state-of-the-art photo synthesis baselines. We also demonstrate that our framework facilitates a controllable manipulation of photo synthesis by editing strokes of corresponding sketches, delivering more fine-grained details than previous approaches that rely on region-level editing.",https://arxiv.org/abs/2209.02835
Impact of Colour Variation on Robustness of Deep Neural Networks,"ChengyinHu, WeiwenShi",02-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Deep neural networks (DNNs) have have shown state-of-the-art performance for computer vision applications like image classification, segmentation and object detection. Whereas recent advances have shown their vulnerability to manual digital perturbations in the input data, namely adversarial attacks. The accuracy of the networks is significantly affected by the data distribution of their training dataset. Distortions or perturbations on color space of input images generates out-of-distribution data, which make networks more likely to misclassify them. In this work, we propose a color-variation dataset by distorting their RGB color on a subset of the ImageNet with 27 different combinations. The aim of our work is to study the impact of color variation on the performance of DNNs. We perform experiments on several state-of-the-art DNN architectures on the proposed dataset, and the result shows a significant correlation between color variation and loss of accuracy. Furthermore, based on the ResNet50 architecture, we demonstrate some experiments of the performance of recently proposed robust training techniques and strategies, such as Augmix, revisit, and free normalizer, on our proposed dataset. Experimental results indicate that these robust training techniques can improve the robustness of deep networks to color variation.",https://arxiv.org/abs/2209.02834
Impact of Scaled Image on Robustness of Deep Neural Networks,"ChengyinHu, WeiwenShi",02-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Deep neural networks (DNNs) have been widely used in computer vision tasks like image classification, object detection and segmentation. Whereas recent studies have shown their vulnerability to manual digital perturbations or distortion in the input images. The accuracy of the networks is remarkably influenced by the data distribution of their training dataset. Scaling the raw images creates out-of-distribution data, which makes it a possible adversarial attack to fool the networks. In this work, we propose a Scaling-distortion dataset ImageNet-CS by Scaling a subset of the ImageNet Challenge dataset by different multiples. The aim of our work is to study the impact of scaled images on the performance of advanced DNNs. We perform experiments on several state-of-the-art deep neural network architectures on the proposed ImageNet-CS, and the results show a significant positive correlation between scaling size and accuracy decline. Moreover, based on ResNet50 architecture, we demonstrate some tests on the performance of recent proposed robust training techniques and strategies like Augmix, Revisiting and Normalizer Free on our proposed ImageNet-CS. Experiment results have shown that these robust training techniques can improve networks' robustness to scaling transformation.",https://arxiv.org/abs/2209.02832
Annealing Optimization for Progressive Learning with StochasticApproximation,"ChristosMavridis, John Baras",06-sep-22,Systems and Control (eess.SY)," In this work, we introduce a learning model designed to meet the needs of applications in which computational resources are limited, and robustness and interpretability are prioritized. Learning problems can be formulated as constrained stochastic optimization problems, with the constraints originating mainly from model assumptions that define a trade- off between complexity and performance. This trade-off is closely related to over-fitting, generalization capacity, and robustness to noise and adversarial attacks, and depends on both the structure and complexity of the model, as well as the properties of the optimization methods used. We develop an online prototype-based learning algorithm based on annealing optimization that is formulated as an online gradient-free stochastic approximation algorithm. The learning model can be viewed as an interpretable and progressively growing competitive-learning neural network model to be used for supervised, unsupervised, and reinforcement learning. The annealing nature of the algorithm contributes to minimal hyper-parameter tuning requirements, poor local minima prevention, and robustness with respect to the initial conditions. At the same time, it provides online control over the performance-complexity trade-off by progressively increasing the complexity of the learning model as needed, through an intuitive bifurcation phenomenon. Finally, the use of stochastic approximation enables the study of the convergence of the learning algorithm through mathematical tools from dynamical systems and control, and allows for its integration with reinforcement learning algorithms, constructing an adaptive state-action aggregation scheme.",https://arxiv.org/abs/2209.02132
Online Deterministic Annealing for Classification and Clustering,"ChristosMavridis, John Baras","11 Feb 2021 (v1(https://arxiv.org/abs/2102.05836v1)), lastrevised 10 Jan 2022 (this version, v4)",Machine Learning (cs.LG)," Inherent in virtually every iterative machine learning algorithm is the problem of hyper-parameter tuning, which includes three major design parameters: (a) the complexity of the model, e.g., the number of neurons in a neural network, (b) the initial conditions, which heavily affect the behavior of the algorithm, and (c) the dissimilarity measure used to quantify its performance. We introduce an online prototype-based learning algorithm that can be viewed as a progressively growing competitive-learning neural network architecture for classification and clustering. The learning rule of the proposed approach is formulated as an online gradient-free stochastic approximation algorithm that solves a sequence of appropriately defined optimization problems, simulating an annealing process. The annealing nature of the algorithm contributes to avoiding poor local minima, offers robustness with respect to the initial conditions, and provides a means to progressively increase the complexity of the learning model, through an intuitive bifurcation phenomenon. The proposed approach is interpretable, requires minimal hyper-parameter tuning, and allows online control over the performance-complexity trade-off. Finally, we show that Bregman divergences appear naturally as a family of dissimilarity measures that play a central role in both the performance and the computational complexity of the learning algorithm.",https://arxiv.org/abs/2209.02826
CP-AGCN: Pytorch-based Attention Informed Graph Convolutional Networkfor Identifying Infants at Risk of Cerebral Palsy,"HaozhengZhang, Edmond S. L.Ho, HubertP. H. Shum",06-sep-22,Computer Vision and Pattern Recognition (cs.CV), Early prediction is clinically considered one of the essential parts of cerebral palsy (CP) treatment. We propose to implement a low-cost and interpretable classification system for supporting CP prediction based on General Movement Assessment (GMA). We design a Pytorch-based attention- informed graph convolutional network to early identify infants at risk of CP from skeletal data extracted from RGB videos. We also design a frequency- binning module for learning the CP movements in the frequency domain while filtering noise. Our system only requires consumer-grade RGB videos for training to support interactive-time CP prediction by providing an interpretable CP classification result.,https://arxiv.org/abs/2102.05836
Multilingual Bidirectional Unsupervised Translation ThroughMultilingual Finetuning and Back-Translation,"BryanLi, AjayPatel, ChrisCallison-Burch, Mohammad SadeghRasooli",06-sep-22,Computation and Language (cs.CL)," We propose a two-stage training approach for developing a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 25 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then train with successive rounds of back-translation. The final model extends to the English-to-Many direction, while retaining Many-to- English performance. We term our approach EcXTra (English-centric Crosslingual (X) Transfer). Our approach sequentially leverages auxiliary parallel data and monolingual data, and is conceptually simple, only using a standard cross-entropy objective in both stages. The final EcXTra model is evaluated on unsupervised NMT on 8 low-resource languages achieving a new state-of-the-art for English-to-Kazakh (22.3  10.4 BLEU), and competitive performance for the other 15 translation directions.",https://arxiv.org/abs/2209.02824
Efficient solution of parameter identification problems with $H^1$regularization,"JanBlechta, Oliver G.Ernst",06-sep-22,Numerical Analysis (math.NA)," We consider the identification of spatially distributed parameters under $H^1$ regularization. Solving the associated minimization problem by Gauss-Newton iteration results in linearized problems to be solved in each step that can be cast as boundary value problems involving a low-rank modification of the Laplacian. Using algebraic multigrid as a fast Laplace solver, the Sherman-Morrison-Woodbury formula can be employed to construct a preconditioner for these linear problems which exhibits excellent scaling w.r.t. the relevant problem parameters. We first develop this approach in the functional setting, thus obtaining a consistent methodology for selecting boundary conditions that arise from the $H^1$ regularization. We then construct a method for solving the discrete linear systems based on combining any fast Poisson solver with the Woodbury formula. The efficacy of this method is then demonstrated with scaling experiments. These are carried out for a common nonlinear parameter identification problem arising in electrical resistivity tomography.",https://arxiv.org/abs/2209.02821
A Subexponential Quantum Algorithm for the Semdirect DiscreteLogarithm Problem,"ChristopherBattarbee, DelaramKahrobaei, LudovicPerret, Siamak F.Shahandashti",06-sep-22,Cryptography and Security (cs.CR)," Group-based cryptography is a relatively young family in post- quantum cryptography. In this paper we give the first dedicated security analysis of a central problem in group-based cryptography: the so-called Semidirect Product Key Exchange (SDPKE). We present a subexponential quantum algorithm for solving SDPKE. To do this we reduce SDPKE to the Abelian Hidden Shift Problem (for which there are known quantum subexponential algorithms). We stress that this does not per se constitute a break of SDPKE; rather, the purpose of the paper is to provide a connection to known problems.",https://arxiv.org/abs/2209.02815
Increasing Adverse Drug Events extraction robustness on social media:case study on negation and speculation,"SimoneScaboro, BeatricePortelli, EmmanueleChersoni, EnricoSantus, GiuseppeSerra",06-sep-22,Computation and Language (cs.CL)," In the last decade, an increasing number of users have started reporting Adverse Drug Events (ADE) on social media platforms, blogs, and health forums. Given the large volume of reports, pharmacovigilance has focused on ways to use Natural Language Processing (NLP) techniques to rapidly examine these large collections of text, detecting mentions of drug- related adverse reactions to trigger medical investigations. However, despite the growing interest in the task and the advances in NLP, the robustness of these models in face of linguistic phenomena such as negations and speculations is an open research question. Negations and speculations are pervasive phenomena in natural language, and can severely hamper the ability of an automated system to discriminate between factual and nonfactual statements in text. In this paper we take into consideration four state-of-the-art systems for ADE detection on social media texts. We introduce SNAX, a benchmark to test their performance against samples containing negated and speculated ADEs, showing their fragility against these phenomena. We then introduce two possible strategies to increase the robustness of these models, showing that both of them bring significant increases in performance, lowering the number of spurious entities predicted by the models by 60% for negation and 80% for speculations.",https://arxiv.org/abs/2209.02814
Use and Misuse of Machine Learning in Anthropology,"JeffCalder, Reed Coil, AnnieMelton, Peter J.Olver, GilbertTostevin, Katrina Yezzi-Woodley",06-sep-22,Machine Learning (cs.LG)," Machine learning (ML), being now widely accessible to the research community at large, has fostered a proliferation of new and striking applications of these emergent mathematical techniques across a wide range of disciplines. In this paper, we will focus on a particular case study: the field of paleoanthropology, which seeks to understand the evolution of the human species based on biological and cultural evidence. As we will show, the easy availability of ML algorithms and lack of expertise on their proper use among the anthropological research community has led to foundational misapplications that have appeared throughout the literature. The resulting unreliable results not only undermine efforts to legitimately incorporate ML into anthropological research, but produce potentially faulty understandings about our human evolutionary and behavioral past.   The aim of this paper is to provide a brief introduction to some of the ways in which ML has been applied within paleoanthropology; we also include a survey of some basic ML algorithms for those who are not fully conversant with the field, which remains under active development. We discuss a series of missteps, errors, and violations of correct protocols of ML methods that appear disconcertingly often within the accumulating body of anthropological literature. These mistakes include use of outdated algorithms and practices; inappropriate train/test splits, sample composition, and textual explanations; as well as an absence of transparency due to the lack of data/code sharing, and the subsequent limitations imposed on independent replication. We assert that expanding samples, sharing data and code, re- evaluating approaches to peer review, and, most importantly, developing interdisciplinary teams that include experts in ML are all necessary for progress in future research incorporating ML within anthropology.",https://arxiv.org/abs/2209.02812
Localizing Load-Altering Attacks Against Power Grids Using DeepCapsule Nets,"HamidrezaJahangir, SubhashLakshminarayana, CarstenMaple",06-sep-22,Cryptography and Security (cs.CR)," Recent research has shown that the security of power grids can be seriously threatened by botnet-type cyber attacks that target a large number of high-wattage smart electrical appliances owned by end-users. Accurate detection and localization of such attacks is of critical importance in limiting the damage. To this end, the paper proposes a novel technique using capsule networks (CNs) tailored to the power grid security application that uses the frequency and phase angle data monitored by phasor measurement units (PMUs). With the benefit of vector output from capsules and dynamic routing agreements between them, CNs can obtain accurate detection and localization performance. To demonstrate the efficiency of the suggested technique, we compare the developed CN with benchmark data-driven methodologies, including two-dimensional convolutional neural networks (2D-CNN), one-dimensional CNN (1D-CNN), deep multi-layer perceptrons (MLP), and support vector machines (SVM). Simulations are performed on IEEE 14-, 39-, and 57-bus systems, considering various real-world issues such as PMU delays, noisy data, and missing data points. The results show that CNs significantly outperform other techniques, thus making them suitable for the aforementioned cyber security applications.",https://arxiv.org/abs/2209.02811
Fusion of Satellite Images and Weather Data with Transformer Networksfor Downy Mildew Disease Detection,"WilliamMaillet, MaryamOuhami, AdelHafiane",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Crop diseases significantly affect the quantity and quality of agricultural production. In a context where the goal of precision agriculture is to minimize or even avoid the use of pesticides, weather and remote sensing data with deep learning can play a pivotal role in detecting crop diseases, allowing localized treatment of crops. However, combining heterogeneous data such as weather and images remains a hot topic and challenging task. Recent developments in transformer architectures have shown the possibility of fusion of data from different domains, for instance text-image. The current trend is to custom only one transformer to create a multimodal fusion model. Conversely, we propose a new approach to realize data fusion using three transformers. In this paper, we first solved the missing satellite images problem, by interpolating them with a ConvLSTM model. Then, proposed a multimodal fusion architecture that jointly learns to process visual and weather information. The architecture is built from three main components, a Vision Transformer and two transformer-encoders, allowing to fuse both image and weather modalities. The results of the proposed method are promising achieving 97\% overall accuracy.",https://arxiv.org/abs/2209.02809
"""Es geht um Respekt, nicht um Technologie"": Erkenntnisse aus einemInteressensgruppen-Ã¼bergreifenden Workshop zu genderfairer Sprache undSprachtechnologie","SabrinaBurtscher, Katta Spiel, Lukas DanielKlausner, ManuelLardelli, DagmarGromann",06-sep-22,Computation and Language (cs.CL)," With the increasing attention non-binary people receive in Western societies, strategies of gender-fair language have started to move away from binary (only female/male) concepts of gender. Nevertheless, hardly any approaches to take these identities into account into machine translation models exist so far. A lack of understanding of the socio-technical implications of such technologies risks further reproducing linguistic mechanisms of oppression and mislabelling. In this paper, we describe the methods and results of a workshop on gender-fair language and language technologies, which was led and organised by ten researchers from TU Wien, St. PÃ¶lten UAS, FH Campus Wien and the University of Vienna and took place in Vienna in autumn 2021. A wide range of interest groups and their representatives were invited to ensure that the topic could be dealt with holistically. Accordingly, we aimed to include translators, machine translation experts and non-binary individuals (as ""community experts"") on an equal footing. Our analysis shows that gender in machine translation requires a high degree of context sensitivity, that developers of such technologies need to position themselves cautiously in a process still under social negotiation, and that flexible approaches seem most adequate at present. We then illustrate steps that follow from our results for the field of gender-fair language technologies so that technological developments can adequately line up with social advancements.   \----   Mit zunehmender gesamtgesellschaftlicher Wahrnehmung nicht-binÃ¤rer Personen haben sich in den letzten Jahren auch Konzepte von genderfairer Sprache von der bisher verwendeten BinaritÃ¤t (weiblich/mÃ¤nnlich) entfernt. Trotzdem gibt es bislang nur wenige AnsÃ¤tze dazu, diese IdentitÃ¤ten in maschineller Ãœbersetzung abzubilden. Ein fehlendes VerstÃ¤ndnis unterschiedlicher sozio- technischer Implikationen derartiger Technologien birgt in sich die Gefahr, fehlerhafte Ansprachen und Bezeichnungen sowie sprachliche UnterdrÃ¼ckungsmechanismen zu reproduzieren. In diesem Beitrag beschreiben wir die Methoden und Ergebnisse eines Workshops zu genderfairer Sprache in technologischen ZusammenhÃ¤ngen, der im Herbst 2021 in Wien stattgefunden hat. Zehn Forscher*innen der TU Wien, FH St. PÃ¶lten, FH Campus Wien und UniversitÃ¤t Wien organisierten und leiteten den Workshop. Dabei wurden unterschiedlichste Interessensgruppen und deren Vertreter*innen breit gestreut eingeladen, um sicherzustellen, dass das Thema holistisch behandelt werden kann. Dementsprechend setzten wir uns zum Ziel, Machine-Translation- Entwickler*innen, Ãœbersetzer*innen, und nicht-binÃ¤re Privatpersonen (als ""Lebenswelt-Expert*innen"") gleichberechtigt einzubinden. Unsere Analyse zeigt, dass Geschlecht in maschineller Ãœbersetzung eine maÃŸgeblich kontextsensible Herangehensweise erfordert, die Entwicklung von Sprachtechnologien sich vorsichtig in einem sich noch in Aushandlung befindlichen gesellschaftlichen Prozess positionieren muss, und flexible AnsÃ¤tze derzeit am adÃ¤quatesten erscheinen. Wir zeigen auf, welche nÃ¤chsten Schritte im Bereich genderfairer Technologien notwendig sind, damit technische mit sozialen Entwicklungen mithalten kÃ¶nnen.",https://arxiv.org/abs/2209.02797
Side-channel attack analysis on in-memory computing architectures,"ZiyuWang, Fan-hsuan Meng, Yongmo Park, Jason K.Eshraghian, Wei D. Lu",06-sep-22,Cryptography and Security (cs.CR)," In-memory computing (IMC) systems have great potential for accelerating data-intensive tasks such as deep neural networks (DNNs). As DNN models are generally highly proprietary, the neural network architectures become valuable targets for attacks. In IMC systems, since the whole model is mapped on chip and weight memory read can be restricted, the system acts as a ""black box"" for customers. However, the localized and stationary weight and data patterns may subject IMC systems to other attacks. In this paper, we propose a side-channel attack methodology on IMC architectures. We show that it is possible to extract model architectural information from power trace measurements without any prior knowledge of the neural network. We first developed a simulation framework that can emulate the dynamic power traces of the IMC macros. We then performed side-channel attacks to extract information such as the stored layer type, layer sequence, output channel/feature size and convolution kernel size from power traces of the IMC macros. Based on the extracted information, full networks can potentially be reconstructed without any knowledge of the neural network. Finally, we discuss potential countermeasures for building IMC systems that offer resistance to these model extraction attack.",https://arxiv.org/abs/2209.02793
Bioinspired Smooth Neuromorphic Control for Robotic Arms,"IoannisPolykretis, Lazar Supic, AndreeaDanielescu",06-sep-22,Robotics (cs.RO)," Replicating natural human movements is a long-standing goal of robotics control theory. Drawing inspiration from biology, where reaching control networks give rise to smooth and precise movements, can narrow the performance gap between human and robot control. Neuromorphic processors, which mimic the brain's computational principles, are an ideal platform to approximate the accuracy and smoothness of such controllers while maximizing their energy efficiency and robustness. However, the incompatibility of conventional control methods with neuromorphic hardware limits the computational efficiency and explainability of their existing adaptations. In contrast, the neuronal connectome underlying smooth and accurate reaching movements is effective, minimal, and inherently compatible with neuromorphic processors. In this work, we emulate these networks and propose a biologically realistic spiking neural network for motor control. Our controller incorporates adaptive feedback to provide smooth and accurate motor control while inheriting the minimal complexity of its biological counterpart that controls reaching movements, allowing for direct deployment on Intel's neuromorphic processor. Using our controller as a building block and inspired by joint coordination in human arms, we scaled up our approach to control real-world robot arms. The trajectories and smooth, minimum-jerk velocity profiles of the resulting motions resembled those of humans, verifying the biological relevance of our controller. Notably, our method achieved state-of-the-art control performance while decreasing the motion jerk by 19\% to improve motion smoothness. Our work suggests that exploiting both the computational units of the brain and their connectivity may lead to the design of effective, efficient, and explainable neuromorphic controllers, paving the way for neuromorphic solutions in fully autonomous systems.",https://arxiv.org/abs/2209.02792
Read it to me: An emotionally aware Speech Narration Application,RishibhaBansal,06-sep-22,Sound (cs.SD)," In this work we try to perform emotional style transfer on audios. In particular, MelGAN-VC architecture is explored for various emotion-pair transfers. The generated audio is then classified using an LSTM-based emotion classifier for audio. We find that ""sad"" audio is generated well as compared to ""happy"" or ""anger"" as people have similar expressions of sadness.",https://arxiv.org/abs/2209.02787
Unifying Effects of Direct and Relational Associations for VisualCommunication,"Melissa A.Schoenlein, JohnnyCampos, Kevin J.Lande, LaurentLessard, Karen B.Schloss",06-sep-22,Human-Computer Interaction (cs.HC)," People have expectations about how colors map to concepts in visualizations, and they are better at interpreting visualizations that match their expectations. Traditionally, studies on these expectations (inferred mappings) distinguished distinct factors relevant for visualizations of categorical vs. continuous information. Studies on categorical information focused on direct associations (e.g., mangos are associated with yellows) whereas studies on continuous information focused on relational associations (e.g., darker colors map to larger quantities; dark-is-more bias). We unite these two areas within a single framework of assignment inference. Assignment inference is the process by which people infer mappings between perceptual features and concepts represented in encoding systems. Observers infer globally optimal assignments by maximizing the ""merit,"" or ""goodness,"" of each possible assignment. Previous work on assignment inference focused on visualizations of categorical information. We extend this approach to visualizations of continuous data by (a) broadening the notion of merit to include relational associations and (b) developing a method for combining multiple (sometimes conflicting) sources of merit to predict people's inferred mappings. We developed and tested our model on data from experiments in which participants interpreted colormap data visualizations, representing fictitious data about environmental concepts (sunshine, shade, wild fire, ocean water, glacial ice). We found both direct and relational associations contribute independently to inferred mappings. These results can be used to optimize visualization design to facilitate visual communication.",https://arxiv.org/abs/2209.02785
Multi-skill Mobile Manipulation for Object Rearrangement,"JiayuanGu, DevendraSinghChaplot, Hao Su, JitendraMalik",06-sep-22,Robotics (cs.RO)," We study a modular approach to tackle long-horizon mobile manipulation tasks for object rearrangement, which decomposes a full task into a sequence of subtasks. To tackle the entire task, prior work chains multiple stationary manipulation skills with a point-goal navigation skill, which are learned individually on subtasks. Although more effective than monolithic end-to-end RL policies, this framework suffers from compounding errors in skill chaining, e.g., navigating to a bad location where a stationary manipulation skill can not reach its target to manipulate. To this end, we propose that the manipulation skills should include mobility to have flexibility in interacting with the target object from multiple locations and at the same time the navigation skill could have multiple end points which lead to successful manipulation. We operationalize these ideas by implementing mobile manipulation skills rather than stationary ones and training a navigation skill trained with region goal instead of point goal. We evaluate our multi-skill mobile manipulation method M3 on 3 challenging long-horizon mobile manipulation tasks in the Home Assistant Benchmark (HAB), and show superior performance as compared to the baselines.",https://arxiv.org/abs/2209.02782
Impact of Mobility on Downlink Cell-Free Massive MIMO Systems,"AbhinavAnand, Chandra R.Murthy, RibhuChopra",06-sep-22,Information Theory (cs.IT)," In this paper, we analyze the achievable downlink spectral efficiency of cell-free massive multiple input multiple output (CF-mMIMO) systems, accounting for the effects of channel aging (caused by user mobility) and pilot contamination. We consider two cases, one where user equipments (UEs) rely on downlink pilots beamformed by the access points (APs) to estimate downlink channel, and another where UEs utilize statistical channel state information (CSI) for data decoding. For comparison, we also consider cellular mMIMO and derive its achievable spectral efficiency with channel aging and pilot contamination in the above two cases. Our results show that, in CF-mMIMO, downlink training is preferable over statistical CSI when the length of the data sequence is chosen optimally to maximize the spectral efficiency. In cellular mMIMO, however, either one of the two schemes may be better depending on whether user fairness or sum spectral efficiency is prioritized. Furthermore, the CF-mMIMO system generally outperforms cellular mMIMO even after accounting for the effects of channel aging and pilot contamination. Through numerical results, we illustrate the effect of various system parameters such as the maximum user velocity, uplink/downlink pilot lengths, data duration, network densification, and provide interesting insights into the key differences between cell-free and cellular mMIMO systems.",https://arxiv.org/abs/2209.02778
Separators in Continuous Petri Nets,"MichaelBlondin, JavierEsparza",06-sep-22,Logic in Computer Science (cs.LO)," Leroux has proved that unreachability in Petri nets can be witnessed by a Presburger separator, i.e. if a marking $\vec{m}_\text{src}$ cannot reach a marking $\vec{m}_\text{tgt}$, then there is a formula $\varphi$ of Presburger arithmetic such that: $\varphi(\vec{m}_\text{src})$ holds; $\varphi$ is forward invariant, i.e., $\varphi(\vec{m})$ and $\vec{m} \rightarrow \vec{m}'$ imply $\varphi(\vec{m}'$); and $\neg \varphi(\vec{m}_\text{tgt})$ holds. While these separators could be used as explanations and as formal certificates of unreachability, this has not yet been the case due to their (super-)Ackermannian worst-case size and the (super-)exponential complexity of checking that a formula is a separator.   We show that, in continuous Petri nets, these two problems can be overcome. We introduce locally closed separators, and prove that: (a) unreachability can be witnessed by a locally closed separator computable in polynomial time; (b) checking whether a formula is a locally closed separator is in NC (so, simpler than unreachablity, which is P-complete).   We further consider the more general problem of (existential) set-to-set reachability, where two sets of markings are given as convex polytopes. We show that, while our approach does not extend directly, we can still efficiently certify unreachability via an altered Petri.",https://arxiv.org/abs/2209.02777
Depression Symptoms Modelling from Social Media Text: An ActiveLearning Approach,"NawshadFarruque, RandyGoebel, SudhakarSivapalan, OsmarZaiane","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02765v1)), lastrevised 8 Sep 2022 (this version, v2)",Computation and Language (cs.CL)," A fundamental component of user-level social media language based clinical depression modelling is depression symptoms detection (DSD). Unfortunately, there does not exist any DSD dataset that reflects both the clinical insights and the distribution of depression symptoms from the samples of self-disclosed depressed population. In our work, we describe an Active Learning (AL) framework which uses an initial supervised learning model that leverages 1) a state-of-the-art large mental health forum text pre-trained language model further fine-tuned on a clinician annotated DSD dataset, 2) a Zero-Shot learning model for DSD, and couples them together to harvest depression symptoms related samples from our large self-curated Depression Tweets Repository (DTR). Our clinician annotated dataset is the largest of its kind. Furthermore, DTR is created from the samples of tweets in self-disclosed depressed users Twitter timeline from two datasets, including one of the largest benchmark datasets for user-level depression detection from Twitter. This further helps preserve the depression symptoms distribution of self-disclosed Twitter users tweets. Subsequently, we iteratively retrain our initial DSD model with the harvested data. We discuss the stopping criteria and limitations of this AL process, and elaborate the underlying constructs which play a vital role in the overall AL process. We show that we can produce a final dataset which is the largest of its kind. Furthermore, a DSD and a Depression Post Detection (DPD) model trained on it achieves significantly better accuracy than their initial version.",https://arxiv.org/abs/2209.02767
Change Detection for Local Explainability in Evolving Data Streams,"JohannesHaug, AlexanderBraun, StefanZÃ¼rn, GjergjiKasneci",06-sep-22,Machine Learning (cs.LG)," As complex machine learning models are increasingly used in sensitive applications like banking, trading or credit scoring, there is a growing demand for reliable explanation mechanisms. Local feature attribution methods have become a popular technique for post-hoc and model- agnostic explanations. However, attribution methods typically assume a stationary environment in which the predictive model has been trained and remains stable. As a result, it is often unclear how local attributions behave in realistic, constantly evolving settings such as streaming and online applications. In this paper, we discuss the impact of temporal change on local feature attributions. In particular, we show that local attributions can become obsolete each time the predictive model is updated or concept drift alters the data generating distribution. Consequently, local feature attributions in data streams provide high explanatory power only when combined with a mechanism that allows us to detect and respond to local changes over time. To this end, we present CDLEEDS, a flexible and model-agnostic framework for detecting local change and concept drift. CDLEEDS serves as an intuitive extension of attribution-based explanation techniques to identify outdated local attributions and enable more targeted recalculations. In experiments, we also show that the proposed framework can reliably detect both local and global concept drift. Accordingly, our work contributes to a more meaningful and robust explainability in online machine learning.",https://arxiv.org/abs/2209.02765
A Data Science Approach to Risk Assessment for Automobile InsurancePolicies,PatrickHosein,06-sep-22,Machine Learning (cs.LG)," In order to determine a suitable automobile insurance policy premium one needs to take into account three factors, the risk associated with the drivers and cars on the policy, the operational costs associated with management of the policy and the desired profit margin. The premium should then be some function of these three values. We focus on risk assessment using a Data Science approach. Instead of using the traditional frequency and severity metrics we instead predict the total claims that will be made by a new customer using historical data of current and past policies. Given multiple features of the policy (age and gender of drivers, value of car, previous accidents, etc.) one can potentially try to provide personalized insurance policies based specifically on these features as follows. We can compute the average claims made per year of all past and current policies with identical features and then take an average over these claim rates. Unfortunately there may not be sufficient samples to obtain a robust average. We can instead try to include policies that are ""similar"" to obtain sufficient samples for a robust average. We therefore face a trade- off between personalization (only using closely similar policies) and robustness (extending the domain far enough to capture sufficient samples). This is known as the Bias-Variance Trade-off. We model this problem and determine the optimal trade-off between the two (i.e. the balance that provides the highest prediction accuracy) and apply it to the claim rate prediction problem. We demonstrate our approach using real data.",https://arxiv.org/abs/2209.02764
An End-to-End Solution for Enabling Urban Cyclability: The Bike2WorkExperience,"AntonioBucchiarone, SimoneBassanelli, MassimilianoLuca, SimoneCentellegher, PiergiorgioCipriano, LucaGiovannini, Bruno Lepri, AnnapaolaMarconi",06-sep-22,Computers and Society (cs.CY)," Mobility plays a fundamental role in modern cities. How citizens experience the city, access its core services, and participate in city life, strongly depends on its mobility organization and efficiency. The challenges that municipalities face are very ambitious: on the one hand, administrators must guarantee their citizens the right to mobility and to easily access local services; on the other hand, they need to minimize the economic, social, and environmental costs of the mobility system. Municipalities are increasingly facing problems of traffic congestion, road safety, energy dependency and air pollution, and therefore encouraging a shift towards sustainable mobility habits based on active mobility is of central importance. Active modes, such as cycling, should be particularly encouraged, especially for local recurrent journeys (i.e., home-to-school, home-to-work). In this context, addressing and mitigating commuter-generated traffic requires engaging public and private stakeholders through innovative and collaborative approaches that focus not only on supply (e.g., roads and vehicles) but also on transportation demand management. In this paper, we propose an end-to-end solution for enabling urban cyclability. It supports the companies' Mobility Managers (MMs) acting on the promotion of active mobility for home-to-work commuting, helps the city administrators to understand the needed urban planning interventions, and motivates the citizens to sustainable mobility. To evaluate the effectiveness of the proposed solution we developed two analyses: the first to accurately analyze the user experience and any behaviour change related to the BIKE2WORK initiative, and the second to demonstrate how exploiting the collected data we can inform and possible guide the involved municipality (i.e., Ferrara, a city in Northern Italy) in improving the urban cyclability.",https://arxiv.org/abs/2209.02762
Specification-Guided Component-Based Synthesis from EffectfulLibraries,"AshishMishra, SureshJagannathan",06-sep-22,Programming Languages (cs.PL)," Component-based synthesis seeks to build programs using the APIs provided by a set of libraries. Oftentimes, these APIs have effects, which make it challenging to reason about the correctness of potential synthesis candidates. This is because changes to global state made by effectful library procedures affect how they may be composed together, yielding an intractably large search space that can confound typical enumerative synthesis techniques. If the nature of these effects are exposed as part of their specification, however, deductive synthesis approaches can be used to help guide the search for components. In this paper, we present a new specification-guided synthesis procedure that uses Hoare-style pre- and post-conditions to express fine-grained effects of potential library component candidates to drive a bi-directional synthesis search strategy. The procedure alternates between a forward search process that seeks to build larger terms given an existing context but which is otherwise unaware of the actual goal, alongside a backward search mechanism that seeks terms consistent with the desired goal but which is otherwise unaware of the context from which these terms must be synthesized. To further improve efficiency and scalability, we integrate a conflict-driven learning procedure into the synthesis algorithm that provides a semantic characterization of previously encountered unsuccessful search paths that is used to prune the space of possible candidates as synthesis proceeds. We have implemented our ideas in a tool called Cobalt and demonstrate its effectiveness on a number of challenging synthesis problems defined over OCaml libraries equipped with effectful specifications.",https://arxiv.org/abs/2209.02755
Scalable Regularization of Scene Graph Generation Models usingSymbolic Theories,"DavideBuffelli, EfthymiaTsamoura",06-sep-22,Machine Learning (cs.LG)," Several techniques have recently aimed to improve the performance of deep learning models for Scene Graph Generation (SGG) by incorporating background knowledge. State-of-the-art techniques can be divided into two families: one where the background knowledge is incorporated into the model in a subsymbolic fashion, and another in which the background knowledge is maintained in symbolic form. Despite promising results, both families of techniques face several shortcomings: the first one requires ad-hoc, more complex neural architectures increasing the training or inference cost; the second one suffers from limited scalability w.r.t. the size of the background knowledge. Our work introduces a regularization technique for injecting symbolic background knowledge into neural SGG models that overcomes the limitations of prior art. Our technique is model-agnostic, does not incur any cost at inference time, and scales to previously unmanageable background knowledge sizes. We demonstrate that our technique can improve the accuracy of state-of-the-art SGG models, by up to 33%.",https://arxiv.org/abs/2209.02752
Educating Educators to Integrate Inclusive Design Across a 4-Year CSDegree Program,"LaraLetaw, RosalindaGarcia, PatriciaMorreale, Gail Verdi, HeatherGarcia, Geraldine JimenaNoa, SpencerP. Madsen, Maria Jesus Alzugaray-Orellana, MargaretBurnett",06-sep-22,Human-Computer Interaction (cs.HC)," How can an entire CS faculty, who together have been teaching the ACM standard CS curricula, shift to teaching elements of inclusive design across a 4-year undergraduate CS program? And will they even want to try? To investigate these questions, we developed an educate-the-educators curriculum to support this shift. The overall goal of the educate-the- educators curriculum was to enable CS faculty to creatively engage with embedding inclusive design into their courses in ""minimally invasive"" ways. GenderMag, an inclusive design evaluation method, was selected for use. The curriculum targeted the following learning outcomes: to enable CS faculty: (1) to analyze the costs and benefits of integrating inclusive design into their own course(s); (2) to evaluate software using the GenderMag method, and recognize its use to identify meaningful issues in software; (3) to integrate inclusive design into existing course materials with provided resources and collaboration; and (4) to prepare to engage and guide students on learning GenderMag concepts. We conducted a field study over a spring/summer followed by end-of-fall interviews, during which we worked with 18 faculty members to integrate inclusive design into 13 courses. Ten of these faculty then taught 7 of these courses that were on the Fall 2021 schedule, across 16 sections. We present the new educate-the-educators curriculum and report on the faculty's experiences acting upon it over the three-month field study and subsequent interviews. Our results showed that, of the 18 faculty we worked with, 83% chose to modify their courses; by Fall 2021, faculty across all four years of a CS degree program had begun teaching inclusive design concepts. When we followed up with the 10 Fall 2021 faculty, 91% of their reported outcomes indicated that the incorporations of inclusive design concepts in their courses went as well as or better than expected.",https://arxiv.org/abs/2209.02749
Handcrafted Feature Selection Techniques for Pattern Recognition: ASurvey,"Alysson Ribeiro daSilva, Camila GuedesSilveira",06-sep-22,Robotics (cs.RO)," The accuracy of a classifier, when performing Pattern recognition, is mostly tied to the quality and representativeness of the input feature vector. Feature Selection is a process that allows for representing information properly and may increase the accuracy of a classifier. This process is responsible for finding the best possible features, thus allowing us to identify to which class a pattern belongs. Feature selection methods can be categorized as Filters, Wrappers, and Embed. This paper presents a survey on some Filters and Wrapper methods for handcrafted feature selection. Some discussions, with regard to the data structure, processing time, and ability to well represent a feature vector, are also provided in order to explicitly show how appropriate some methods are in order to perform feature selection. Therefore, the presented feature selection methods can be accurate and efficient if applied considering their positives and negatives, finding which one fits best the problem's domain may be the hardest task.",https://arxiv.org/abs/2209.02748
Stochastic Data-Driven Variational Multiscale Reduced Order Models,"FeiLu, ChanghongMou, HonghuLiu, TraianIliescu",06-sep-22,Numerical Analysis (math.NA)," Trajectory-wise data-driven reduced order models (ROMs) tend to be sensitive to training data, and thus lack robustness. We propose to construct a robust stochastic ROM closure (S-ROM) from data consisting of multiple trajectories from random initial conditions. The S-ROM is a low- dimensional time series model for the coefficients of the dominating proper orthogonal decomposition (POD) modes inferred from data. Thus, it achieves reduction both space and time, leading to simulations orders of magnitude faster than the full order model. We show that both the estimated POD modes and parameters in the S-ROM converge when the number of trajectories increases. Thus, the S-ROM is robust when the training data size increases. We demonstrate the S-ROM on a 1D Burgers equation with a viscosity $\nu= 0.002$ and with random initial conditions. The numerical results verify the convergence. Furthermore, the S-ROM makes accurate trajectory-wise predictions from new initial conditions and with a prediction time far beyond the training range, and it quantifies the spread of uncertainties due to the unresolved scales.",https://arxiv.org/abs/2209.02746
Mixed approximation of nonlinear acoustic equations: Well-posednessand a priori error analysis,"MostafaMeliani, VanjaNikoliÄ‡",06-sep-22,Numerical Analysis (math.NA)," Accurate simulation of nonlinear acoustic waves is essential for the continued development of a wide range of (high-intensity) focused ultrasound applications. This article explores mixed finite element formulations of classical strongly damped quasilinear models of ultrasonic wave propagation; the Kuznetsov and Westervelt equations. Such formulations allow simultaneous retrieval of the acoustic particle velocity and either the pressure or acoustic velocity potential, thus characterizing the entire ultrasonic field at once. Using non-standard energy analysis and a fixed- point technique, we establish sufficient conditions for the well-posedness, stability, and optimal a priori errors in the energy norm for the semi- discrete equations. For the Westervelt equation, we also determine the conditions under which the error bounds can be made uniform with respect to the involved strong dissipation parameter. A byproduct of this analysis is the convergence rate for the inviscid (undamped) Westervelt equation in mixed form. Additionally, we discuss convergence in the $L^q(\Omega)$ norm for the involved scalar quantities, where $q$ depends on the spatial dimension. Finally, computer experiments for the Raviart--Thomas (RT) and Brezzi--Douglas--Marini (BDM) elements are performed to confirm the theoretical findings.",https://arxiv.org/abs/2209.02739
Spatiotemporal Cardiac Statistical Shape Modeling: A Data-DrivenApproach,"JadieAdams, NawazishKhan, AlanMorris, ShireenElhabian",06-sep-22,Machine Learning (cs.LG)," Clinical investigations of anatomy's structural changes over time could greatly benefit from population-level quantification of shape, or spatiotemporal statistic shape modeling (SSM). Such a tool enables characterizing patient organ cycles or disease progression in relation to a cohort of interest. Constructing shape models requires establishing a quantitative shape representation (e.g., corresponding landmarks). Particle- based shape modeling (PSM) is a data-driven SSM approach that captures population-level shape variations by optimizing landmark placement. However, it assumes cross-sectional study designs and hence has limited statistical power in representing shape changes over time. Existing methods for modeling spatiotemporal or longitudinal shape changes require predefined shape atlases and pre-built shape models that are typically constructed cross- sectionally. This paper proposes a data-driven approach inspired by the PSM method to learn population-level spatiotemporal shape changes directly from shape data. We introduce a novel SSM optimization scheme that produces landmarks that are in correspondence both across the population (inter- subject) and across time-series (intra-subject). We apply the proposed method to 4D cardiac data from atrial-fibrillation patients and demonstrate its efficacy in representing the dynamic change of the left atrium. Furthermore, we show that our method outperforms an image-based approach for spatiotemporal SSM with respect to a generative time-series model, the Linear Dynamical System (LDS). LDS fit using a spatiotemporal shape model optimized via our approach provides better generalization and specificity, indicating it accurately captures the underlying time-dependency.",https://arxiv.org/abs/2209.02737
An IoT-Enriched Event Log for Process Mining in Smart Factories,"LukasMalburg, JoschaGrÃ¼ger, RalphBergmann",06-sep-22,Databases (cs.DB)," Modern technologies such as the Internet of Things (IoT) are becoming increasingly important in various domains, including Business Process Management (BPM) research. One main research area in BPM is process mining, which can be used to analyze event logs, e.g., for checking the conformance of running processes. However, there are only a few IoT-based event logs available for research purposes. Some of them are artificially generated and the problem occurs that they do not always completely reflect the actual physical properties of smart environments. In this paper, we present an IoT-enriched XES event log that is generated by a physical smart factory. For this purpose, we create the SensorStream XES extension for representing IoT-data in event logs. Finally, we present some preliminary analysis and properties of the log.",https://arxiv.org/abs/2209.02736
Language-aware Domain Generalization Network for Cross-SceneHyperspectral Image Classification,"YuxiangZhang, MengmengZhang, WeiLi, ShuaiWang, RanTao",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Text information including extensive prior knowledge about land cover classes has been ignored in hyperspectral image classification (HSI) tasks. It is necessary to explore the effectiveness of linguistic mode in assisting HSI classification. In addition, the large-scale pre-training image-text foundation models have demonstrated great performance in a variety of downstream applications, including zero-shot transfer. However, most domain generalization methods have never addressed mining linguistic modal knowledge to improve the generalization performance of model. To compensate for the inadequacies listed above, a Language-aware Domain Generalization Network (LDGnet) is proposed to learn cross-domain invariant representation from cross-domain shared prior knowledge. The proposed method only trains on the source domain (SD) and then transfers the model to the target domain (TD). The dual-stream architecture including image encoder and text encoder is used to extract visual and linguistic features, in which coarse-grained and fine-grained text representations are designed to extract two levels of linguistic features. Furthermore, linguistic features are used as cross-domain shared semantic space, and visual-linguistic alignment is completed by supervised contrastive learning in semantic space. Extensive experiments on three datasets demonstrate the superiority of the proposed method when compared with state-of-the-art techniques.",https://arxiv.org/abs/2209.02702
Single-source Domain Expansion Network for Cross-Scene HyperspectralImage Classification,"YuxiangZhang, WeiLi, WeidongSun, RanTao, QianDu",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Currently, cross-scene hyperspectral image (HSI) classification has drawn increasing attention. It is necessary to train a model only on source domain (SD) and directly transferring the model to target domain (TD), when TD needs to be processed in real time and cannot be reused for training. Based on the idea of domain generalization, a Single-source Domain Expansion Network (SDEnet) is developed to ensure the reliability and effectiveness of domain extension. The method uses generative adversarial learning to train in SD and test in TD. A generator including semantic encoder and morph encoder is designed to generate the extended domain (ED) based on encoder-randomization-decoder architecture, where spatial and spectral randomization are specifically used to generate variable spatial and spectral information, and the morphological knowledge is implicitly applied as domain invariant information during domain expansion. Furthermore, the supervised contrastive learning is employed in the discriminator to learn class-wise domain invariant representation, which drives intra-class samples of SD and ED. Meanwhile, adversarial training is designed to optimize the generator to drive intra-class samples of SD and ED to be separated. Extensive experiments on two public HSI datasets and one additional multispectral image (MSI) dataset demonstrate the superiority of the proposed method when compared with state-of-the-art techniques.",https://arxiv.org/abs/2209.02700
Instrument Separation of Symbolic Music by Explicitly Guided DiffusionModel,"SangjunHan, HyeongraeIhm, DaeHanAhn, WoohyungLim",05-sep-22,Sound (cs.SD)," Similar to colorization in computer vision, instrument separation is to assign instrument labels (e.g. piano, guitar...) to notes from unlabeled mixtures which contain only performance information. To address the problem, we adopt diffusion models and explicitly guide them to preserve consistency between mixtures and music. The quantitative results show that our proposed model can generate high-fidelity samples for multitrack symbolic music with creativity.",https://arxiv.org/abs/2209.01634
A Self-Similar Sine-Cosine Fractal Architecture for MultiportInterferometers,"Jasvith RajBasani, Sri KrishnaVadlamani, SaumilBandyopadhyay, Dirk R.Englund, RyanHamerly",07-sep-22,Optics (physics.optics)," Multiport interferometers based on integrated beamsplitter meshes have recently captured interest as a platform for many emerging technologies. In this paper, we present a novel architecture for multiport interferometers based on the Sine-Cosine fractal decomposition of a unitary matrix. Our architecture is unique in that it is self-similar, enabling the construction of modular multi-chiplet devices. Due to this modularity, our design enjoys improved resilience to hardware imperfections as compared to conventional multiport interferometers. Additionally, the structure of our circuit enables systematic truncation, which is key in reducing the hardware footprint of the chip as well as compute time in training optical neural networks, while maintaining full connectivity. Numerical simulations show that truncation of these meshes gives robust performance even under large fabrication errors. This design is a step forward in the construction of large-scale programmable photonics, removing a major hurdle in scaling up to practical machine learning and quantum computing applications.",https://arxiv.org/abs/2209.02696
Minimum-entropy causal inference and its application in brain networkanalysis,LipengNing,07-sep-22,Methodology (stat.ME)," Identification of the causal relationship between multivariate time series is a ubiquitous problem in data science. Granger causality measure (GCM) and conditional Granger causality measure (cGCM) are widely used statistical methods for causal inference and effective connectivity analysis in neuroimaging research. Both GCM and cGCM have frequency-domain formulations that are developed based on a heuristic algorithm for matrix decompositions. The goal of this work is to generalize GCM and cGCM measures and their frequency-domain formulations by using a theoretic framework for minimum entropy (ME) estimation. The proposed ME-estimation method extends the classical theory of minimum mean squared error (MMSE) estimation for stochastic processes. It provides three formulations of cGCM that include Geweke's original time-domain cGCM as a special case. But all three frequency-domain formulations of cGCM are different from previous methods. Experimental results based on simulations have shown that one of the proposed frequency-domain cGCM has enhanced sensitivity and specificity in detecting network connections compared to other methods. In an example based on in vivo functional magnetic resonance imaging, the proposed frequency- domain measure cGCM can significantly enhance the consistency between the structural and effective connectivity of human brain networks.",https://arxiv.org/abs/2209.03335
On the Second Kahn--Kalai Conjecture,"ElchananMossel, Jonathan Niles-Weed, Nike Sun, IliasZadik",07-sep-22,Combinatorics (math.CO)," For any given graph $H$, we are interested in $p_\mathrm{crit}(H)$, the minimal $p$ such that the ErdÅ‘s-RÃ©nyi graph $G(n,p)$ contains a copy of $H$ with probability at least $1/2$. Kahn and Kalai (2007) conjectured that $p_\mathrm{crit}(H)$ is given up to a logarithmic factor by a simpler ""subgraph expectation threshold"" $p_\mathrm{E}(H)$, which is the minimal $p$ such that for every subgraph $H'\subseteq H$, the ErdÅ‘s-RÃ©nyi graph $G(n,p)$ contains \emph{in expectation} at least $1/2$ copies of $H'$. It is trivial that $p_\mathrm{E}(H) \le p_\mathrm{crit}(H)$, and the so-called ""second Kahn- Kalai conjecture"" states that $p_\mathrm{crit}(H) \lesssim p_\mathrm{E}(H) \log e(H)$ where $e(H)$ is the number of edges in $H$.   In this article, we present a natural modification $p_\mathrm{E, new}(H)$ of the Kahn--Kalai subgraph expectation threshold, which we show is sandwiched between $p_\mathrm{E}(H)$ and $p_\mathrm{crit}(H)$. The new definition $p_\mathrm{E, new}(H)$ is based on the simple observation that if $G(n,p)$ contains a copy of $H$ and $H$ contains \emph{many} copies of $H'$, then $G(n,p)$ must also contain \emph{many} copies of $H'$. We then show that $p_\mathrm{crit}(H) \lesssim p_\mathrm{E, new}(H) \log e(H)$, thus proving a modification of the second Kahn--Kalai conjecture. The bound follows by a direct application of the set-theoretic ""spread"" property, which led to recent breakthroughs in the sunflower conjecture by Alweiss, Lovett, Wu and Zhang and the first fractional Kahn--Kalai conjecture by Frankston, Kahn, Narayanan and Park.",https://arxiv.org/abs/2209.03329
Spach Transformer: Spatial and Channel-wise Transformer Based on Localand Global Self-attentions for PET Image Denoising,"Se-InJang, TinsuPan, YeLi, PedramHeidari, Junyu Chen, Quanzheng Li, Kuang Gong",07-sep-22,Image and Video Processing (eess.IV)," Position emission tomography (PET) is widely used in clinics and research due to its quantitative merits and high sensitivity, but suffers from low signal-to-noise ratio (SNR). Recently convolutional neural networks (CNNs) have been widely used to improve PET image quality. Though successful and efficient in local feature extraction, CNN cannot capture long-range dependencies well due to its limited receptive field. Global multi-head self-attention (MSA) is a popular approach to capture long-range information. However, the calculation of global MSA for 3D images has high computational costs. In this work, we proposed an efficient spatial and channel-wise encoder-decoder transformer, Spach Transformer, that can leverage spatial and channel information based on local and global MSAs. Experiments based on datasets of different PET tracers, i.e., $^{18}$F-FDG, $^{18}$F-ACBC, $^{18}$F-DCFPyL, and $^{68}$Ga-DOTATATE, were conducted to evaluate the proposed framework. Quantitative results show that the proposed Spach Transformer can achieve better performance than other reference methods.",https://arxiv.org/abs/2209.03326
Quadratic Gradient: Uniting Gradient Algorithm and Newton Method asOne,JohnChiang,03-sep-22,Optimization and Control (math.OC)," It might be inadequate for the line search technique for Newton's method to use only one floating point number. A column vector of the same size as the gradient might be better than a mere float number to accelerate each of the gradient elements with different rates. Moreover, a square matrix of the same order as the Hessian matrix might be helpful to correct the Hessian matrix. Chiang applied something between a column vector and a square matrix, namely a diagonal matrix, to accelerate the gradient and further proposed a faster gradient variant called quadratic gradient. In this paper, we present a new way to build a new version of the quadratic gradient. This new quadratic gradient doesn't satisfy the convergence conditions of the fixed Hessian Newton's method. However, experimental results show that it sometimes has a better performance than the original one in convergence rate. Also, Chiang speculates that there might be a relation between the Hessian matrix and the learning rate for the first- order gradient descent method. We prove that the floating number $\frac{1}{\epsilon + \max \\{| \lambda_i | \\}}$ can be a good learning rate of the gradient methods, where $\epsilon$ is a number to avoid division by zero and $\lambda_i$ the eigenvalues of the Hessian matrix.",https://arxiv.org/abs/2209.03300
Manifold Free Riemannian Optimization,"BorisShustin, HaimAvron, BarakSober",07-sep-22,Optimization and Control (math.OC)," Riemannian optimization is a principled framework for solving optimization problems where the desired optimum is constrained to a smooth manifold $\mathcal{M}$. Algorithms designed in this framework usually require some geometrical description of the manifold, which typically includes tangent spaces, retractions, and gradients of the cost function. However, in many cases, only a subset (or none at all) of these elements can be accessed due to lack of information or intractability. In this paper, we propose a novel approach that can perform approximate Riemannian optimization in such cases, where the constraining manifold is a submanifold of $\R^{D}$. At the bare minimum, our method requires only a noiseless sample set of the cost function $(\x_{i}, y_{i})\in {\mathcal{M}} \times \mathbb{R}$ and the intrinsic dimension of the manifold $\mathcal{M}$. Using the samples, and utilizing the Manifold-MLS framework (Sober and Levin 2020), we construct approximations of the missing components entertaining provable guarantees and analyze their computational costs. In case some of the components are given analytically (e.g., if the cost function and its gradient are given explicitly, or if the tangent spaces can be computed), the algorithm can be easily adapted to use the accurate expressions instead of the approximations. We analyze the global convergence of Riemannian gradient-based methods using our approach, and we demonstrate empirically the strength of this method, together with a conjugate-gradients type method based upon similar principles.",https://arxiv.org/abs/2209.03282
Quadratic sequences with prime power discriminators,SajedHaque,07-sep-22,Number Theory (math.NT)," The discriminator of an integer sequence $\textbf{s} = (s(i))_{i \geq 0}$, introduced by Arnold, Benkoski, and McCabe in 1985, is the function $D_{\textbf{s}} (n)$ that sends $n$ to the least integer $m$ such that the numbers $s(0), s(1), \ldots, s(n - 1)$ are pairwise incongruent modulo $m$. In this note, we explore the quadratic sequences whose discriminator is given by $p^{\lceil \log_p n \rceil}$ for prime $p$, i.e., the smallest power of $p$ which is $\geq n$. We provide a complete characterization of such sequences for $p = 2$, show that this is impossible for $p \geq 5$, and provide some partial results for $p = 3$.",https://arxiv.org/abs/2209.03269
Neuromorphic computing using wavelength-division multiplexing,"XingyuanXu, WeiweiHan, MengxiTan, YangSun, YangLi, JiayangWu, RobertoMorandotti, ArnanMitchell, Kun Xu, David J.Moss",07-sep-22,Optics (physics.optics)," Optical neural networks (ONNs), or optical neuromorphic hardware accelerators, have the potential to dramatically enhance the computing power and energy efficiency of mainstream electronic processors, due to their ultralarge bandwidths of up to 10s of terahertz together with their analog architecture that avoids the need for reading and writing data back and forth. Different multiplexing techniques have been employed to demonstrate ONNs, amongst which wavelength division multiplexing (WDM) techniques make sufficient use of the unique advantages of optics in terms of broad bandwidths. Here, we review recent advances in WDM based ONNs, focusing on methods that use integrated microcombs to implement ONNs. We present results for human image processing using an optical convolution accelerator operating at 11 Tera operations per second. The open challenges and limitations of ONNs that need to be addressed for future applications are also discussed.",https://arxiv.org/abs/2209.03265
$1D$ to $nD$: A Meta Algorithm for Multivariate Global Optimizationvia Univariate Optimizers,"KaanGokcesu, HakanGokcesu",06-sep-22,Optimization and Control (math.OC)," In this work, we propose a meta algorithm that can solve a multivariate global optimization problem using univariate global optimizers. Although the univariate global optimization does not receive much attention compared to the multivariate case, which is more emphasized in academia and industry; we show that it is still relevant and can be directly used to solve problems of multivariate optimization. We also provide the corresponding regret bounds in terms of the time horizon $T$ and the average regret of the univariate optimizer, when it is robust against nonnegative noises with robust regret guarantees.",https://arxiv.org/abs/2209.03252
Regret Analysis of Global Optimization in Univariate Functions withLipschitz Derivatives,"KaanGokcesu, HakanGokcesu",24 Aug 2021,Machine Learning (cs.LG)," In this work, we study the problem of global optimization in univariate loss functions, where we analyze the regret of the popular lower bounding algorithms (e.g., Piyavskii-Shubert algorithm). For any given time $T$, instead of the widely available simple regret (which is the difference of the losses between the best estimation up to $T$ and the global optimizer), we study the cumulative regret up to that time. With a suitable lower bounding algorithm, we show that it is possible to achieve satisfactory cumulative regret bounds for different classes of functions. For Lipschitz continuous functions with the parameter $L$, we show that the cumulative regret is $O(L\log T)$. For Lipschitz smooth functions with the parameter $H$, we show that the cumulative regret is $O(H)$. We also analytically extend our results for a broader class of functions that covers both the Lipschitz continuous and smooth functions individually.",https://arxiv.org/abs/2209.03246
Low Regret Binary Sampling Method for Efficient Global Optimization ofUnivariate Functions,"KaanGokcesu, HakanGokcesu",18 Jan 2022,Machine Learning (cs.LG)," In this work, we propose a computationally efficient algorithm for the problem of global optimization in univariate loss functions. For the performance evaluation, we study the cumulative regret of the algorithm instead of the simple regret between our best query and the optimal value of the objective function. Although our approach has similar regret results with the traditional lower-bounding algorithms such as the Piyavskii-Shubert method for the Lipschitz continuous or Lipschitz smooth functions, it has a major computational cost advantage. In Piyavskii-Shubert method, for certain types of functions, the query points may be hard to determine (as they are solutions to additional optimization problems). However, this issue is circumvented in our binary sampling approach, where the sampling set is predetermined irrespective of the function characteristics. For a search space of $[0,1]$, our approach has at most $L\log (3T)$ and $2.25H$ regret for $L$-Lipschitz continuous and $H$-Lipschitz smooth functions respectively. We also analytically extend our results for a broader class of functions that covers more complex regularity conditions.",https://arxiv.org/abs/2108.10859
Mixing time of random walk on dynamical random cluster,"AndreaLelli, AlexandreStauffer",07-sep-22,Probability (math.PR)," We study the mixing time of a random walker who moves inside a dynamical random cluster model on the d-dimensional torus of side-length n. In this model, edges switch at rate \mu between open and closed, following a Glauber dynamics for the random cluster model with parameters p,q. At the same time, the walker jumps at rate 1 as a simple random walk on the torus, but is only allowed to traverse open edges. We show that for small enough p the mixing time of the random walker is of order n^2/\mu. In our proof we construct of a non-Markovian coupling through a multi-scale analysis of the environment, which we believe could be more widely applicable.",https://arxiv.org/abs/2201.07164
Morphology-preserving Autoregressive 3D Generative Modelling of theBrain,"Petru-DanielTudosiu, Walter Hugo LopezPinaya, Mark S.Graham, PedroBorges, VirginiaFernandez, Dai Yang, JeremyAppleyard, GuidoNovati, DishaMehra, MikeVella, ParashkevNachev, SebastienOurselin, JorgeCardoso",07-sep-22,Image and Video Processing (eess.IV)," Human anatomy, morphology, and associated diseases can be studied using medical imaging data. However, access to medical imaging data is restricted by governance and privacy concerns, data ownership, and the cost of acquisition, thus limiting our ability to understand the human body. A possible solution to this issue is the creation of a model able to learn and then generate synthetic images of the human body conditioned on specific characteristics of relevance (e.g., age, sex, and disease status). Deep generative models, in the form of neural networks, have been recently used to create synthetic 2D images of natural scenes. Still, the ability to produce high-resolution 3D volumetric imaging data with correct anatomical morphology has been hampered by data scarcity and algorithmic and computational limitations. This work proposes a generative model that can be scaled to produce anatomically correct, high-resolution, and realistic images of the human brain, with the necessary quality to allow further downstream analyses. The ability to generate a potentially unlimited amount of data not only enables large-scale studies of human anatomy and pathology without jeopardizing patient privacy, but also significantly advances research in the field of anomaly detection, modality synthesis, learning under limited data, and fair and ethical AI. Code and trained models are available at: [this https URL](https://github.com/AmigoLab/SynthAnatomy).",https://arxiv.org/abs/2209.03227
Machine Learning Partners in Criminal Networks,"Diego D.Lopes, Bruno R. daCunha, Alvaro F.Martins, SebastianGoncalves, ErvinK.Lenzi, Quentin S.Hanley, MatjazPerc, Haroldo V.Ribeiro",07-sep-22,Physics and Society (physics.soc-ph)," Recent research has shown that criminal networks have complex organizational structures, but whether this can be used to predict static and dynamic properties of criminal networks remains little explored. Here, by combining graph representation learning and machine learning methods, we show that structural properties of political corruption, police intelligence, and money laundering networks can be used to recover missing criminal partnerships, distinguish among different types of criminal and legal associations, as well as predict the total amount of money exchanged among criminal agents, all with outstanding accuracy. We also show that our approach can anticipate future criminal associations during the dynamic growth of corruption networks with significant accuracy. Thus, similar to evidence found at crime scenes, we conclude that structural patterns of criminal networks carry crucial information about illegal activities, which allows machine learning methods to predict missing information and even anticipate future criminal behavior.",https://arxiv.org/abs/2209.03177
Non-Gaussian Process Regression,"YamanKÄ±ndap, SimonGodsill",07-sep-22,Machine Learning (stat.ML)," Standard GPs offer a flexible modelling tool for well-behaved processes. However, deviations from Gaussianity are expected to appear in real world datasets, with structural outliers and shocks routinely observed. In these cases GPs can fail to model uncertainty adequately and may over- smooth inferences. Here we extend the GP framework into a new class of time- changed GPs that allow for straightforward modelling of heavy-tailed non- Gaussian behaviours, while retaining a tractable conditional GP structure through an infinite mixture of non-homogeneous GPs representation. The conditional GP structure is obtained by conditioning the observations on a latent transformed input space and the random evolution of the latent transformation is modelled using a LÃ©vy process which allows Bayesian inference in both the posterior predictive density and the latent transformation function. We present Markov chain Monte Carlo inference procedures for this model and demonstrate the potential benefits compared to a standard GP.",https://arxiv.org/abs/2209.03171
On the Convergence of the ELBO to Entropy Sums,JÃ¶rgLÃ¼cke,07-sep-22,Machine Learning (stat.ML)," The variational lower bound (a.k.a. ELBO or free energy) is the central objective for many learning algorithms including algorithms for deep unsupervised learning. Learning algorithms change model parameters such that the variational lower bound increases, and until the parameters are close to a stationary point of the learning dynamics. In this purely theoretical contribution, we show that (for a very large class of generative models) the variational lower bound is at all stationary points of learning equal to a sum of entropies. For models with one set of latents and one set observed variables, the sum consists of three entropies: (A) the (average) entropy of the variational distributions, (B) the negative entropy of the model's prior distribution, and (C) the (expected) negative entropy of the observable distributions. The obtained result applies under realistic conditions including: finite numbers of data points, at any stationary points (including saddle points) and for any family of (well behaved) variational distributions. The class of generative models for which we show the equality to entropy sums contains many (and presumably most) standard generative models (including deep models). As concrete examples we discuss probabilistic PCA and Sigmoid Belief Networks. The prerequisites we use to show equality to entropy sums are relatively mild. Concretely, the distributions of a given generative model have to be of the exponential family (with constant base measure), and a model has to satisfy a parameterization criterion (which is usually fulfilled). Proving the equality of the ELBO to entropy sums at stationary points (under the stated conditions) is the main contribution of this work.",https://arxiv.org/abs/2209.03117
A learning theory for quantum photonic processors and beyond,Matteo Rosati,07-sep-22,Quantum Physics (quant-ph)," We consider the tasks of learning quantum states, measurements and channels generated by continuous-variable (CV) quantum circuits. This family of circuits is suited to describe optical quantum technologies and in particular it includes state-of-the-art photonic processors capable of showing quantum advantage. We define classes of functions that map classical variables, encoded into the CV circuit parameters, to outcome probabilities evaluated on those circuits. We then establish efficient learnability guarantees for such classes, by computing bounds on their pseudo-dimension or covering numbers, showing that CV quantum circuits can be learned with a sample complexity that scales polynomially with the circuit's size, i.e., the number of modes. Our results establish that CV circuits can be trained efficiently using a number of training samples that, unlike their finite- dimensional counterpart, does not scale with the circuit depth.",https://arxiv.org/abs/2209.03077
The art of algorithmic guessing in $\texttt{gfun}$,SergeyYurkevich,07-sep-22,Combinatorics (math.CO), The technique of guessing can be very fruitful when dealing with sequences which arise in practice. This holds true especially when guessing is performed algorithmically and efficiently. One highly useful tool for this purpose is the package named $\texttt{gfun}$ in the software Maple. In this text we explore and explain some of $\texttt{gfun}$'s possibilities and illustrate them on two examples from recent mathematical research by the author and his collaborators.,https://arxiv.org/abs/2209.03075
Graph Neural Networks for Low-Energy Event Classification &Reconstruction in IceCube,"R. Abbasi, M.Ackermann, J.Adams, N. Aggarwal, J. A.Aguilar, M.Ahlers, M. Ahrens, J.M.Alameddine, A. A. AlvesJr., N. M. Amin, K.Andeen, T. Anderson, G.Anton, C. ArgÃ¼elles, Y.Ashida, S. Athanasiadou, S.Axani, X. Bai, A. Balagopal V., M.Baricevic, S. W.Barwick, V.Basu, R.Bay, J. J.Beatty, K.-H.Becker, J. Becker Tjus, J.Beise, C. Bellenghi, S.Benda, S. BenZvi, D.Berley, E. Bernardini, D. Z.Besson, G.Binder, D. Bindig, E.Blaufuss, S.Blot, F.Bontempo, J. Y.Book, J. Borowka, C. BoscoloMeneguolo, S.BÃ¶ser, O.Botner, J. BÃ¶ttcher, E.Bourbeau, J.Braun, B. Brinson, J. Brostean-Kaiser, R. T. Burley, R. S.Busse, M. A. Campana, E. G. Carnie-Bronca, C. Chen, Z. Chen, D.Chirkin, K.Choi, B.A. Clark, L.Classen, A.Coleman, G. H.Collin, A.Connolly, J. M.Conrad, P.Coppin, P. Correa, S.Countryman, D. F.Cowen, R. Cross, C.Dappen, P. Dave, C. De Clercq, J. J.DeLaunay, D. DelgadoLÃ³pez, H.Dembinski, K.Deoskar, A.Desai, P. Desiati, K. D. deVries, G. deWasseige, T.DeYoung, A.Diaz, J.C. DÃ­az-VÃ©lez, M.Dittmer, H.Dujmovic, M. A.DuVernois, T.Ehrhardt, P.Eller, R. Engel, H.Erpenbeck, J.Evans, P. A. Evenson, K. L.Fan, A.R. Fazely, A.Fedynitch, N.Feigl, S. Fiedlschuster, A. T.Fienberg, C.Finley, L. Fischer, D.Fox, A.Franckowiak, E. Friedman, A.Fritz, P. FÃ¼rst, T. K.Gaisser, J.Gallagher, E.Ganster, A.Garcia, S. Garrappa, L.Gerhardt, A.Ghadimi, C.Glaser, T. Glauch, T.GlÃ¼senkamp, N.Goehlke, J. G.Gonzalez, S.Goswami, D.Grant, S. J. Gray, T.GrÃ©goire, S.Griswold, C.GÃ¼nther, P.Gutjahr, C.Haack, A. Hallgren, R.Halliday, L.Halve, F. Halzen, H.Hamdaoui, M. HaMinh, K. Hanson, J.Hardin, A. A. Harnisch, P.Hatch, A. Haungs, K.Helbing, J.Hellrung, F.Henningsen, L.Heuermann, S.Hickford, C.Hill, G.C. Hill, K. D. Hoffman, K.Hoshina, W.Hou, T.Huber, K. Hultqvist, M.HÃ¼nnefeld, R.Hussain, K.Hymon, S. In, N.Iovine, A. Ishihara, M.Jansson, G. S.Japaridze, M.Jeong, M. Jin, B. J. P. Jones, D.Kang, W.Kang, X.Kang, A.Kappes, D. Kappesser, L.Kardum, T. Karg, M. Karl, A. Karle, U.Katz, M.Kauer, J. L. Kelley, A.Kheirandish, K.Kin, J.Kiryluk, S. R.Klein, A. Kochocki, R.Koirala, H.Kolanoski, T.Kontrimas, L.KÃ¶pke, C.Kopper, D. J. Koskinen, P.Koundal, M.Kovacevich, M.Kowalski, T.Kozynets, E.Krupczak, E.Kun, N.Kurahashi, N.Lad, C.Lagunas Gualda, M. J.Larson, F.Lauber, J. P. Lazar, J. W.Lee, K.Leonard, A.LeszczyÅ„ska, M.Lincetto, Q. R.Liu, M.Liubarska, E.Lohfink, C.Love, C.J. Lozano Mariscal, L.Lu, F.Lucarelli, A.Ludwig, W. Luszczak, Y.Lyu, W. Y.Ma, J.Madsen, K. B. M. Mahn, Y.Makino, S. Mancina, W. MarieSainte, I. C.MariÅŸ, S.Marka, Z. Marka, M.Marsee, I. Martinez-Soler, R.Maruyama, T.McElroy, F.McNally, J. V.Mead, K. Meagher, S.Mechbal, A.Medina, M. Meier, S. Meighen-Berger, Y. Merckx, J.Micallef, D.Mockler, T.Montaruli, R. W.Moore, R. Morse, M.Moulai, T. Mukherjee, R.Naab, R.Nagai, U. Naumann, A.Nayerhoda, J.Necker, M. Neumann, H.Niederhausen, M. U.Nisa, S. C. Nowicki, A. ObertackePollmann, M.Oehler, B. Oeyen, A.Olivas, R. Orsoe, J.Osborn, E. O'Sullivan, H.Pandya, D. V. Pankova, N.Park, G.K. Parker, E. N.Paudel, L.Paul, C.PÃ©rez de los Heros, L.Peters, T. C. Petersen, J.Peterson, S.Philippen, S.Pieper, A. Pizzuto, M.Plum, Y.Popovych, A.Porcelli, M. PradoRodriguez, B.Pries, R. Procter-Murphy, G. T.Przybylski, C.Raab, J.Rack-Helleis, M. Rameez, K.Rawlins, Z.Rechav, A. Rehman, P.Reichherzer, G.Renzi, E. Resconi, S.Reusch, W. Rhode, M.Richman, B.Riedel, E. J. Roberts, S.Robertson, S.Rodan, G. Roellinghoff, M.Rongen, C. Rott, T. Ruhe, L. Ruohan, D.Ryckbosch, D. RysewykCantu, I. Safa, J. Saffer, D. Salazar-Gallegos, P. Sampathkumar, S. E. SanchezHerrera, A.Sandrock, M.Santander, S.Sarkar, S. Sarkar, M.Schaufel, H.Schieler, S.Schindler, B.Schlueter, T.Schmidt, J.Schneider, F. G.SchrÃ¶der, L.Schumacher, G.Schwefer, S.Sclafani, D.Seckel, S. Seunarine, A.Sharma, S. Shefali, N.Shimizu, M.Silva, B. Skrzypek, B.Smithers, R.Snihur, J. Soedingrekso, A.SÃ¸gaard, D.Soldin, C. Spannfellner, G. M.Spiczak, C.Spiering, M.Stamatikos, T.Stanev, R. Stein, T.Stezelberger, T.StÃ¼rwald, T.Stuttard, G. W.Sullivan, I.Taboada, S. Ter-Antonyan, W. G. Thompson, J.Thwaites, S.Tilav, K. Tollefson, C.TÃ¶nnis, S.Toscano, D.Tosi, A.Trettin, C. F.Tung, R. Turcotte, J. P.Twagirayezu, B.Ty, M. A.Unland Elorrieta, K.Upshaw, N. Valtonen-Mattila, J.Vandenbroucke, N. vanEijndhoven, D.Vannerom, J. vanSanten, J.Vara, J.Veitch-Michaelis, S.Verpoest, D.Veske, C. Walck, W.Wang, T.B. Watson, C.Weaver, P. Weigel, A.Weindl, J. Weldert, C.Wendt, J. Werthebach, M.Weyrauch, N.Whitehorn, C. H.Wiebusch, N.Willey, D. R. Williams, M.Wolf, G.Wrede, J. Wulff, X. W.Xu, J. P.Yanez, E. Yildizci, S.Yoshida, S.Yu, T.Yuan, Z.Zhang, P. Zhelnin, et al. (javascript:toggleAuthorList\('long-author-list''et al. \ You must enableJavaScript to view entire author list.",07-sep-22,High Energy Physics - Experiment (hep-ex)," IceCube, a cubic-kilometer array of optical sensors built to detect atmospheric and astrophysical neutrinos between 1 GeV and 1 PeV, is deployed 1.45 km to 2.45 km below the surface of the ice sheet at the South Pole. The classification and reconstruction of events from the in-ice detectors play a central role in the analysis of data from IceCube. Reconstructing and classifying events is a challenge due to the irregular detector geometry, inhomogeneous scattering and absorption of light in the ice and, below 100 GeV, the relatively low number of signal photons produced per event. To address this challenge, it is possible to represent IceCube events as point cloud graphs and use a Graph Neural Network (GNN) as the classification and reconstruction method. The GNN is capable of distinguishing neutrino events from cosmic-ray backgrounds, classifying different neutrino event types, and reconstructing the deposited energy, direction and interaction vertex. Based on simulation, we provide a comparison in the 1-100 GeV energy range to the current state-of-the-art maximum likelihood techniques used in current IceCube analyses, including the effects of known systematic uncertainties. For neutrino event classification, the GNN increases the signal efficiency by 18% at a fixed false positive rate (FPR), compared to current IceCube methods. Alternatively, the GNN offers a reduction of the FPR by over a factor 8 (to below half a percent) at a fixed signal efficiency. For the reconstruction of energy, direction, and interaction vertex, the resolution improves by an average of 13%-20% compared to current maximum likelihood techniques in the energy range of 1-30 GeV. The GNN, when run on a GPU, is capable of processing IceCube events at a rate nearly double of the median IceCube trigger rate of 2.7 kHz, which opens the possibility of using low energy neutrinos in online searches for transient events.",https://arxiv.org/abs/2209.03059
Bayesian learning of feature spaces for multitasks problems,"Carlos Sevilla-Salcedo, AscensiÃ³n Gallardo-AntolÃ­n, Vanessa GÃ³mez-Verdejo, Emilio Parrado-HernÃ¡ndez",07-sep-22,Machine Learning (stat.ML)," This paper presents a Bayesian framework to construct non-linear, parsimonious, shallow models for multitask regression. The proposed framework relies on the fact that Random Fourier Features (RFFs) enables the approximation of an RBF kernel by an extreme learning machine whose hidden layer is formed by RFFs. The main idea is to combine both dual views of a same model under a single Bayesian formulation that extends the Sparse Bayesian Extreme Learning Machines to multitask problems. From the kernel methods point of view, the proposed formulation facilitates the introduction of prior domain knowledge through the RBF kernel parameter. From the extreme learning machines perspective, the new formulation helps control overfitting and enables a parsimonious overall model (the models that serve each task share a same set of RFFs selected within the joint Bayesian optimisation). The experimental results show that combining advantages from kernel methods and extreme learning machines within the same framework can lead to significant improvements in the performance achieved by each of these two paradigms independently.",https://arxiv.org/abs/2209.03042
A Decomposition Approach to Multi-Agent Systems with Bernoulli PacketLoss,"ChristianHespe, HamidehSaadabadi, AdwaitDatar, HerbertWerner, Yang Tang",07-sep-22,Optimization and Control (math.OC)," In this paper, we extend the decomposable systems framework to multi-agent systems with Bernoulli distributed packet loss with uniform probability. The proposed sufficient analysis conditions for mean-square stability and $H_2$-performance - which are expressed in the form of linear matrix inequalities - scale linearly with increased network size and thus allow to analyse even very large-scale multi-agent systems. A numerical example demonstrates the potential of the approach by application to a first-order consensus problem.",https://arxiv.org/abs/2209.03028
Learning Distributions over Quantum Measurement Outcomes,"Weiyuan Gong, ScottAaronson",07-sep-22,Quantum Physics (quant-ph)," Shadow tomography for quantum states provides a sample efficient approach for predicting the properties of quantum systems when the properties are restricted to expectation values of $2$-outcome POVMs. However, these shadow tomography procedures yield poor bounds if there are more than 2 outcomes per measurement. In this paper, we consider a general problem of learning properties from unknown quantum states: given an unknown $d$-dimensional quantum state $\rho$ and $M$ unknown quantum measurements $\mathcal{M}_1,...,\mathcal{M}_M$ with $K\geq 2$ outcomes, estimating the probability distribution for applying $\mathcal{M}_i$ on $\rho$ to within total variation distance $\epsilon$. Compared to the special case when $K=2$, we need to learn unknown distributions instead of values. We develop an online shadow tomography procedure that solves this problem with high success probability requiring $\tilde{O}(K\log^2M\log d/\epsilon^4)$ copies of $\rho$. We further prove an information-theoretic lower bound that at least $\Omega(\min\\{d^2,K+\log M\\}/\epsilon^2)$ copies of $\rho$ are required to solve this problem with high success probability. Our shadow tomography procedure requires sample complexity with only logarithmic dependence on $M$ and $d$ and is sample-optimal for the dependence on $K$.",https://arxiv.org/abs/2209.03024
On the Sparse DAG Structure Learning Based on Adaptive Lasso,"DanruXu, ErdunGao, WeiHuang, MingmingGong",07-sep-22,Machine Learning (stat.ML)," Learning the underlying casual structure, represented by Directed Acyclic Graphs (DAGs), of concerned events from fully-observational data is a crucial part of causal reasoning, but it is challenging due to the combinatorial and large search space. A recent flurry of developments recast this combinatorial problem into a continuous optimization problem by leveraging an algebraic equality characterization of acyclicity. However, these methods suffer from the fixed-threshold step after optimization, which is not a flexible and systematic way to rule out the cycle-inducing edges or false discoveries edges with small values caused by numerical precision. In this paper, we develop a data-driven DAG structure learning method without the predefined threshold, called adaptive NOTEARS [30], achieved by applying adaptive penalty levels to each parameters in the regularization term. We show that adaptive NOTEARS enjoys the oracle properties under some specific conditions. Furthermore, simulation experimental results validate the effectiveness of our method, without setting any gap of edges weights around zero.",https://arxiv.org/abs/2209.03007
Skeleton structure inherent in discrete-time quantum walks,"Tomoki Yamagami, EtsuoSegawa, Ken'ichiro Tanaka, TakatomoMihana, AndrÃ© RÃ¶hm, RyoichiHorisaki, MakotoNaruse",07-sep-22,Mathematical Physics (math-ph)," In this paper, we demonstrate that a common underlying structure -- a skeleton structure -- is present behind discrete-time quantum walks on a one-dimensional lattice with a homogeneous coin matrix. More specifically, we examine the transition probabilities of random walks that replicate the probability distribution of quantum walks. We show that the transition probability contains a skeleton structure by considering the weak limit that excludes the oscillatory behavior. Remarkably, the skeleton structure does not depend on the coin matrix or the initial conditions of the quantum walk. Furthermore, we propose a random walk whose transition probabilities are defined by the skeleton structure, which we call a ""quantum-skeleton random walk."" We demonstrate that the resultant properties of the walkers are similar to the original quantum walk.",https://arxiv.org/abs/2209.02946
Boundary Guided Semantic Learning for Real-time COVID-19 LungInfection Segmentation System,"RunminCong, YumoZhang, Ning Yang, Haisheng Li, XueqiZhang, Ruochen Li, Zewen Chen, Yao Zhao, Sam Kwong",07-sep-22,Image and Video Processing (eess.IV)," The coronavirus disease 2019 (COVID-19) continues to have a negative impact on healthcare systems around the world, though the vaccines have been developed and national vaccination coverage rate is steadily increasing. At the current stage, automatically segmenting the lung infection area from CT images is essential for the diagnosis and treatment of COVID-19. Thanks to the development of deep learning technology, some deep learning solutions for lung infection segmentation have been proposed. However, due to the scattered distribution, complex background interference and blurred boundaries, the accuracy and completeness of the existing models are still unsatisfactory. To this end, we propose a boundary guided semantic learning network (BSNet) in this paper. On the one hand, the dual-branch semantic enhancement module that combines the top-level semantic preservation and progressive semantic integration is designed to model the complementary relationship between different high-level features, thereby promoting the generation of more complete segmentation results. On the other hand, the mirror-symmetric boundary guidance module is proposed to accurately detect the boundaries of the lesion regions in a mirror-symmetric way. Experiments on the publicly available dataset demonstrate that our BSNet outperforms the existing state-of-the-art competitors and achieves a real-time inference speed of 44 FPS.",https://arxiv.org/abs/2209.02943
Deep Learning for Medical Imaging From Diagnosis Prediction to itsCounterfactual Explanation,SumedhaSingla,07-sep-22,Image and Video Processing (eess.IV)," Deep neural networks (DNN) have achieved unprecedented performance in computer-vision tasks almost ubiquitously in business, technology, and science. While substantial efforts are made to engineer highly accurate architectures and provide usable model explanations, most state-of-the-art approaches are first designed for natural vision and then translated to the medical domain. This dissertation seeks to address this gap by proposing novel architectures that integrate the domain-specific constraints of medical imaging into the DNN model and explanation design.",https://arxiv.org/abs/2209.02934
Some explicit arithmetics on curves of genus three and theirapplications,"TomokiMoriya, MomonariKudo",07-sep-22,Algebraic Geometry (math.AG)," A Richelot isogeny between Jacobian varieties is an isogeny whose kernel is included in the $2$-torsion subgroup of the domain. In particular, a Richelot isogeny whose codomain is the product of two or more principally porlalized abelian varieties is called a decomposed Richelot isogeny. In this paper, we develop some explicit arithmetics on curves of genus $3$, including algorithms to compute the codomain of a decomposed Richelot isogeny. As solutions to compute the domain of a decomposed Richelot isogeny, explicit formulae of defining equations for Howe curves of genus $3$ are also given. Using the formulae, we shall construct an algorithm with complexity $\tilde{O}(p^3)$ (resp.\ $\tilde{O}(p^4)$) to enumerate all hyperelliptic (resp.\ non-hyperelliptic) superspecial Howe curves of genus $3$.",https://arxiv.org/abs/2209.02929
Magnitude-image based data-consistent deep learning method for MRIsuper resolution,"ZiyanLin, ZihaoChen",07-sep-22,Image and Video Processing (eess.IV)," Magnetic Resonance Imaging (MRI) is important in clinic to produce high resolution images for diagnosis, but its acquisition time is long for high resolution images. Deep learning based MRI super resolution methods can reduce scan time without complicated sequence programming, but may create additional artifacts due to the discrepancy between training data and testing data. Data consistency layer can improve the deep learning results but needs raw k-space data. In this work, we propose a magnitude-image based data consistency deep learning MRI super resolution method to improve super resolution images' quality without raw k-space data. Our experiments show that the proposed method can improve NRMSE and SSIM of super resolution images compared to the same Convolutional Neural Network (CNN) block without data consistency module.",https://arxiv.org/abs/2209.02926
Constructing Optimal Contraction Trees for Tensor Network QuantumCircuit Simulation,"Cameron Ibrahim, DanyloLykov, Zichang He, YuriAlexeev, IlyaSafro",07-sep-22,Quantum Physics (quant-ph)," One of the key problems in tensor network based quantum circuit simulation is the construction of a contraction tree which minimizes the cost of the simulation, where the cost can be expressed in the number of operations as a proxy for the simulation running time. This same problem arises in a variety of application areas, such as combinatorial scientific computing, marginalization in probabilistic graphical models, and solving constraint satisfaction problems. In this paper, we reduce the computationally hard portion of this problem to one of graph linear ordering, and demonstrate how existing approaches in this area can be utilized to achieve results up to several orders of magnitude better than existing state of the art methods for the same running time. To do so, we introduce a novel polynomial time algorithm for constructing an optimal contraction tree from a given order. Furthermore, we introduce a fast and high quality linear ordering solver, and demonstrate its applicability as a heuristic for providing orderings for contraction trees. Finally, we compare our solver with competing methods for constructing contraction trees in quantum circuit simulation on a collection of randomly generated Quantum Approximate Optimization Algorithm Max Cut circuits and show that our method achieves superior results on a majority of tested quantum circuits.   Reproducibility: Our source code and data are available at [this https URL](https://github.com/cameton/HPEC2022_ContractionTrees).",https://arxiv.org/abs/2209.02901
Improving Self-supervised Learning for Out-of-distribution Task viaAuxiliary Classifier,"HarshitaBoonlia, Tanmoy Dam, Md MeftahulFerdaus, Sreenatha G.Anavatti, AnkanMullick",07-sep-22,Image and Video Processing (eess.IV)," In real world scenarios, out-of-distribution (OOD) datasets may have a large distributional shift from training datasets. This phenomena generally occurs when a trained classifier is deployed on varying dynamic environments, which causes a significant drop in performance. To tackle this issue, we are proposing an end-to-end deep multi-task network in this work. Observing a strong relationship between rotation prediction (self- supervised) accuracy and semantic classification accuracy on OOD tasks, we introduce an additional auxiliary classification head in our multi-task network along with semantic classification and rotation prediction head. To observe the influence of this addition classifier in improving the rotation prediction head, our proposed learning method is framed into bi-level optimisation problem where the upper-level is trained to update the parameters for semantic classification and rotation prediction head. In the lower-level optimisation, only the auxiliary classification head is updated through semantic classification head by fixing the parameters of the semantic classification head. The proposed method has been validated through three unseen OOD datasets where it exhibits a clear improvement in semantic classification accuracy than other two baseline methods. Our code is available on GitHub \url{[this https URL](https://github.com/harshita-555/OSSL)}",https://arxiv.org/abs/2209.02895
Video Restoration with a Deep Plug-and-Play Prior,"AntoineMonod, JulieDelon, MatiasTassano, AndrÃ©sAlmansa",06-sep-22,Image and Video Processing (eess.IV)," This paper presents a novel method for restoring digital videos via a Deep Plug-and-Play (PnP) approach. Under a Bayesian formalism, the method consists in using a deep convolutional denoising network in place of the proximal operator of the prior in an alternating optimization scheme. We distinguish ourselves from prior PnP work by directly applying that method to restore a digital video from a degraded video observation. This way, a network trained once for denoising can be repurposed for other video restoration tasks. Our experiments in video deblurring, super-resolution, and interpolation of random missing pixels all show a clear benefit to using a network specifically designed for video denoising, as it yields better restoration performance and better temporal stability than a single image network with similar denoising performance using the same PnP formulation. Moreover, our method compares favorably to applying a different state-of- the-art PnP scheme separately on each frame of the sequence. This opens new perspectives in the field of video restoration.",https://arxiv.org/abs/2209.02881
Spherical Coordinates from Persistent Cohomology,"Nikolas C.Schonsheck, Stefan C.Schonsheck",06-sep-22,Algebraic Topology (math.AT)," We describe a method to obtain spherical parameterizations of arbitrary data through the use of persistent cohomology and variational optimization. We begin by computing the second-degree persistent cohomology of the filtered Vietoris-Rips (VR) complex of a data set $X$. We extract a cocycle $\alpha$ from any significant feature and define an associated map $\alpha: VR(X) \to S^2$. We use this map as an infeasible initialization for a variational optimization problem with a unique minimizer, up to rigid motion. We employ an alternating gradient descent/ MÃ¶bius transformation update method to solve the problem and generate a more suitable, i.e., smoother, representative of the homotopy class of $\alpha$. We show that this process preserves the relevant topological feature of the data and converges to a feasible optimum. Finally, we conduct numerical experiments on both synthetic and real-world data sets to show the efficacy of our proposed approach.",https://arxiv.org/abs/2209.02854
Semi-supervised Invertible DeepONets for Bayesian Inverse Problems,"SebastianKaltenbach, ParisPerdikaris, Phaedon-SteliosKoutsourelakis","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02772v1)), lastrevised 8 Sep 2022 (this version, v2)",Machine Learning (stat.ML)," Deep Operator Networks (DeepONets) offer a powerful, data-driven tool for solving parametric PDEs by learning operators, i.e. maps between infinite-dimensional function spaces. In this work, we employ physics- informed DeepONets in the context of high-dimensional, Bayesian inverse problems. Traditional solution strategies necessitate an enormous, and frequently infeasible, number of forward model solves, as well as the computation of parametric derivatives. In order to enable efficient solutions, we extend DeepONets by employing a realNVP architecture which yields an invertible and differentiable map between the parametric input and the branch net output. This allows us to construct accurate approximations of the full posterior which can be readily adapted irrespective of the number of observations and the magnitude of the observation noise. As a result, no additional forward solves are required, nor is there any need for costly sampling procedures. We demonstrate the efficacy and accuracy of the proposed methodology in the context of inverse problems based on a anti- derivative, a reaction-diffusion and a Darcy-flow equation.",https://arxiv.org/abs/2209.02791
Concentration bounds for quantum states and limitations on the QAOAfrom polynomial approximations,"Anurag Anshu, TonyMetger",06-sep-22,Quantum Physics (quant-ph)," We prove concentration bounds for the following classes of quantum states: (i) output states of shallow quantum circuits, answering an open question from [DPMRF22]; (ii) injective matrix product states; (iii) output states of dense Hamiltonian evolution, i.e. states of the form $e^{\iota H^{(p)}} \cdots e^{\iota H^{(1)}} |\psi_0\rangle$ for any $n$-qubit product state $|\psi_0\rangle$, where each $H^{(i)}$ can be any local commuting Hamiltonian satisfying a norm constraint, including dense Hamiltonians with interactions between any qubits. Our proofs use polynomial approximations to show that these states are close to local operators. This implies that the distribution of the Hamming weight of a computational basis measurement (and of other related observables) concentrates. An example of (iii) are the states produced by the quantum approximate optimisation algorithm (QAOA). Using our concentration results for these states, we show that for a random spin model, the QAOA can only succeed with negligible probability even at super-constant level $p = o(\log \log n)$. This gives the first limitations on the QAOA on dense instances at super-constant level, improving upon the recent result [BGMZ22].",https://arxiv.org/abs/2209.02772
Statistical Shape Modeling of Biventricular Anatomy with SharedBoundaries,"KrithikaIyer, AlanMorris, BrianZenger, KarthikKarnath, Benjamin AOrkild, OleksandreKorshak, ShireenElhabian",06-sep-22,Image and Video Processing (eess.IV)," Statistical shape modeling (SSM) is a valuable and powerful tool to generate a detailed representation of complex anatomy that enables quantitative analysis and the comparison of shapes and their variations. SSM applies mathematics, statistics, and computing to parse the shape into a quantitative representation (such as correspondence points or landmarks) that will help answer various questions about the anatomical variations across the population. Complex anatomical structures have many diverse parts with varying interactions or intricate architecture. For example, the heart is four-chambered anatomy with several shared boundaries between chambers. Coordinated and efficient contraction of the chambers of the heart is necessary to adequately perfuse end organs throughout the body. Subtle shape changes within these shared boundaries of the heart can indicate potential pathological changes that lead to uncoordinated contraction and poor end- organ perfusion. Early detection and robust quantification could provide insight into ideal treatment techniques and intervention timing. However, existing SSM approaches fall short of explicitly modeling the statistics of shared boundaries. This paper presents a general and flexible data-driven approach for building statistical shape models of multi-organ anatomies with shared boundaries that capture morphological and alignment changes of individual anatomies and their shared boundary surfaces throughout the population. We demonstrate the effectiveness of the proposed methods using a biventricular heart dataset by developing shape models that consistently parameterize the cardiac biventricular structure and the interventricular septum (shared boundary surface) across the population data.",https://arxiv.org/abs/2209.02715
OneEE: A One-Stage Framework for Fast Overlapping and Nested EventExtraction,"HuCao, JingyeLi, FangfangSu, FeiLi, HaoFei, Shengqiong Wu, Bobo Li, LiangZhao, Donghong Ji",06-sep-22,Computation and Language (cs.CL)," Event extraction (EE) is an essential task of information extraction, which aims to extract structured event information from unstructured text. Most prior work focuses on extracting flat events while neglecting overlapped or nested ones. A few models for overlapped and nested EE includes several successive stages to extract event triggers and arguments,which suffer from error propagation. Therefore, we design a simple yet effective tagging scheme and model to formulate EE as word-word relation recognition, called OneEE. The relations between trigger or argument words are simultaneously recognized in one stage with parallel grid tagging, thus yielding a very fast event extraction speed. The model is equipped with an adaptive event fusion module to generate event-aware representations and a distance-aware predictor to integrate relative distance information for word-word relation recognition, which are empirically demonstrated to be effective mechanisms. Experiments on 3 overlapped and nested EE benchmarks, namely FewFC, Genia11, and Genia13, show that OneEE achieves the state-of- the-art (SOTA) results. Moreover, the inference speed of OneEE is faster than those of baselines in the same condition, and can be further substantially improved since it supports parallel inference.",https://arxiv.org/abs/2209.02706
Deep Learning Assisted Optimization for 3D Reconstruction from Single2D Line Drawings,"JiaZheng, YifanZhu, KehanWang, QiangZou, ZihanZhou","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02692v1)), lastrevised 7 Sep 2022 (this version, v2)",Computer Vision and Pattern Recognition (cs.CV)," In this paper, we revisit the long-standing problem of automatic reconstruction of 3D objects from single line drawings. Previous optimization-based methods can generate compact and accurate 3D models, but their success rates depend heavily on the ability to (i) identifying a sufficient set of true geometric constraints, and (ii) choosing a good initial value for the numerical optimization. In view of these challenges, we propose to train deep neural networks to detect pairwise relationships among geometric entities (i.e., edges) in the 3D object, and to predict initial depth value of the vertices. Our experiments on a large dataset of CAD models show that, by leveraging deep learning in a geometric constraint solving pipeline, the success rate of optimization-based 3D reconstruction can be significantly improved.",https://arxiv.org/abs/2209.02693
Statistical Foundation Behind Machine Learning and Its Impact onComputer Vision,"LeiZhang, Heung-YeungShum",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," This paper revisits the principle of uniform convergence in statistical learning, discusses how it acts as the foundation behind machine learning, and attempts to gain a better understanding of the essential problem that current deep learning algorithms are solving. Using computer vision as an example domain in machine learning, the discussion shows that recent research trends in leveraging increasingly large-scale data to perform pre-training for representation learning are largely to reduce the discrepancy between a practically tractable empirical loss and its ultimately desired but intractable expected loss. Furthermore, this paper suggests a few future research directions, predicts the continued increase of data, and argues that more fundamental research is needed on robustness, interpretability, and reasoning capabilities of machine learning by incorporating structure and knowledge.",https://arxiv.org/abs/2209.02692
Classification Protocols with Minimal Disclosure,"JinshuoDong, JasonHartline, AravindanVijayaraghavan",06-sep-22,Cryptography and Security (cs.CR), We consider multi-party protocols for classification that are motivated by applications such as e-discovery in court proceedings. We identify a protocol that guarantees that the requesting party receives all responsive documents and the sending party discloses the minimal amount of non-responsive documents necessary to prove that all responsive documents have been received. This protocol can be embedded in a machine learning framework that enables automated labeling of points and the resulting multi- party protocol is equivalent to the standard one-party classification problem (if the one-party classification problem satisfies a natural independence-of-irrelevant-alternatives property). Our formal guarantees focus on the case where there is a linear classifier that correctly partitions the documents.,https://arxiv.org/abs/2209.02691
A fast-convolution based space-time Chebyshev spectral method forperidynamic models,"LucianoLopez, Sabrina FrancescaPellegrino",06-sep-22,Numerical Analysis (math.NA)," Peridynamics is a nonlocal generalization of continuum mechanics theory which adresses discontinuous problems without using partial derivatives and replacing its by an integral operator. As a consequence, it finds applications in the framework of the development and evolution of fractures and damages in elastic materials.   In this paper we consider a one-dimensional nonlinear model of peridynamics and propose a suitable two-dimensional fast-convolution spectral method based on Chebyshev polynomials to solve the model. This choice allows us to gain the same accuracy both in space and time. We show the convergence of the method and perform several simulations to study the performance of the spectral scheme.",https://arxiv.org/abs/2209.02690
Unpaired Image Translation via Vector Symbolic Architectures,"JustinTheiss, JayLeverett, Daeil Kim, AayushPrakash",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Image-to-image translation has played an important role in enabling synthetic data for computer vision. However, if the source and target domains have a large semantic mismatch, existing techniques often suffer from source content corruption aka semantic flipping. To address this problem, we propose a new paradigm for image-to-image translation using Vector Symbolic Architectures (VSA), a theoretical framework which defines algebraic operations in a high-dimensional vector (hypervector) space. We introduce VSA-based constraints on adversarial learning for source-to-target translations by learning a hypervector mapping that inverts the translation to ensure consistency with source content. We show both qualitatively and quantitatively that our method improves over other state-of-the-art techniques.",https://arxiv.org/abs/2209.02689
Bag of Tricks for FGSM Adversarial Training,"ZichaoLi, LiLiu, ZeyuWang, YuyinZhou, CihangXie",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Adversarial training (AT) with samples generated by Fast Gradient Sign Method (FGSM), also known as FGSM-AT, is a computationally simple method to train robust networks. However, during its training procedure, an unstable mode of ""catastrophic overfitting"" has been identified in [arXiv:2001.03994](https://arxiv.org/abs/2001.03994) [cs.LG], where the robust accuracy abruptly drops to zero within a single training step. Existing methods use gradient regularizers or random initialization tricks to attenuate this issue, whereas they either take high computational cost or lead to lower robust accuracy. In this work, we provide the first study, which thoroughly examines a collection of tricks from three perspectives: Data Initialization, Network Structure, and Optimization, to overcome the catastrophic overfitting in FGSM-AT.   Surprisingly, we find that simple tricks, i.e., a) masking partial pixels (even without randomness), b) setting a large convolution stride and smooth activation functions, or c) regularizing the weights of the first convolutional layer, can effectively tackle the overfitting issue. Extensive results on a range of network architectures validate the effectiveness of each proposed trick, and the combinations of tricks are also investigated. For example, trained with PreActResNet-18 on CIFAR-10, our method attains 49.8% accuracy against PGD-50 attacker and 46.4% accuracy against AutoAttack, demonstrating that pure FGSM-AT is capable of enabling robust learners. The code and models are publicly available at [this https URL](https://github.com/UCSC-VLAA/Bag-of-Tricks-for-FGSM-AT).",https://arxiv.org/abs/2209.02686
"How important are activation functions in regression andclassification? A survey, performance comparison, and future directions","Ameya D.Jagtap, George EmKarniadakis","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02681v1)), lastrevised 7 Sep 2022 (this version, v2)",Machine Learning (cs.LG)," Inspired by biological neurons, the activation functions play an essential part in the learning process of any artificial neural network commonly used in many real-world problems. Various activation functions have been proposed in the literature for classification as well as regression tasks. In this work, we survey the activation functions that have been employed in the past as well as the current state-of-the-art. In particular, we present various developments in activation functions over the years and the advantages as well as disadvantages or limitations of these activation functions. We also discuss classical (fixed) activation functions, including rectifier units, and adaptive activation functions. In addition to presenting the taxonomy of activation functions based on characterization, a taxonomy of activation functions based on applications is also presented. To this end, the systematic comparison of various fixed and adaptive activation functions is performed for classification data sets such as the MNIST, CIFAR-10, and CIFAR-100. In recent years, a physics-informed machine learning framework has emerged for solving problems related to scientific computations. To this purpose, we also discuss various requirements for activation functions that have been used in the physics-informed machine learning framework. Furthermore, various comparisons are made among different fixed and adaptive activation functions using various machine learning libraries such as TensorFlow, Pytorch, and JAX.",https://arxiv.org/abs/2209.02684
Risk Aware Belief-dependent Constrained POMDP Planning,"AndreyZhitnikov, VadimIndelman",06-sep-22,Artificial Intelligence (cs.AI)," Risk awareness is fundamental to an online operating agent. However, it received less attention in the challenging continuous domain under partial observability. Existing constrained POMDP algorithms are typically designed for discrete state and observation spaces. In addition, current solvers for constrained formulations do not support general belief- dependent constraints. Crucially, in the POMDP setting, risk awareness in the context of a constraint was addressed in a limited way. This paper presents a novel formulation for risk-averse belief-dependent constrained POMDP. Our probabilistic constraint is general and belief-dependent, as is the reward function. The proposed universal framework applies to a continuous domain with nonparametric beliefs represented by particles or parametric beliefs. We show that our formulation better accounts for the risk than previous approaches.",https://arxiv.org/abs/2209.02681
Orchestrating Collaborative Cybersecurity: A Secure Framework forDistributed Privacy-Preserving Threat Intelligence Sharing,"Juan R. Trocoso-Pastoriza, AlainMermoud, RomainBouyÃ©, FrancescoMarino, Jean-PhilippeBossuat, VincentLenders, Jean-PierreHubaux",06-sep-22,Cryptography and Security (cs.CR)," Cyber Threat Intelligence (CTI) sharing is an important activity to reduce information asymmetries between attackers and defenders. However, this activity presents challenges due to the tension between data sharing and confidentiality, that result in information retention often leading to a free-rider problem. Therefore, the information that is shared represents only the tip of the iceberg. Current literature assumes access to centralized databases containing all the information, but this is not always feasible, due to the aforementioned tension. This results in unbalanced or incomplete datasets, requiring the use of techniques to expand them; we show how these techniques lead to biased results and misleading performance expectations. We propose a novel framework for extracting CTI from distributed data on incidents, vulnerabilities and indicators of compromise, and demonstrate its use in several practical scenarios, in conjunction with the Malware Information Sharing Platforms (MISP). Policy implications for CTI sharing are presented and discussed. The proposed system relies on an efficient combination of privacy enhancing technologies and federated processing. This lets organizations stay in control of their CTI and minimize the risks of exposure or leakage, while enabling the benefits of sharing, more accurate and representative results, and more effective predictive and preventive defenses.",https://arxiv.org/abs/2209.02679
Bayesian Statistical Model Checking for Multi-agent Systems usingHyperPCTL*,"SpandanDas, PavithraPrabhakar",06-sep-22,Multiagent Systems (cs.MA)," In this paper, we present a Bayesian method for statistical model checking (SMC) of probabilistic hyperproperties specified in the logic HyperPCTL* on discrete-time Markov chains (DTMCs). While SMC of HyperPCTL* using sequential probability ratio test (SPRT) has been explored before, we develop an alternative SMC algorithm based on Bayesian hypothesis testing. In comparison to PCTL*, verifying HyperPCTL* formulae is complex owing to their simultaneous interpretation on multiple paths of the DTMC. In addition, extending the bottom-up model-checking algorithm of the non- probabilistic setting is not straight forward due to the fact that SMC does not return exact answers to the satisfiability problems of subformulae, instead, it only returns correct answers with high-confidence. We propose a recursive algorithm for SMC of HyperPCTL* based on a modified Bayes' test that factors in the uncertainty in the recursive satisfiability results. We have implemented our algorithm in a Python toolbox, HyProVer, and compared our approach with the SPRT based SMC. Our experimental evaluation demonstrates that our Bayesian SMC algorithm performs better both in terms of the verification time and the number of samples required to deduce satisfiability of a given HyperPCTL* formula.",https://arxiv.org/abs/2209.02676
TAPA: A Scalable Task-Parallel Dataflow Programming Framework forModern FPGAs with Co-Optimization of HLS and Physical Design,"LichengGuo, YuzeChi, JasonLau, LinghaoSong, XingyuTian, MoazinKhatti, Weikang Qiao, Jie Wang, EcenurUstun, Zhenman Fang, Zhiru Zhang, Jason Cong",06-sep-22,Hardware Architecture (cs.AR)," In this paper, we propose TAPA, an end-to-end framework that compiles a C++ task-parallel dataflow program into a high-frequency FPGA accelerator. Compared to existing solutions, TAPA has two major advantages. First, TAPA provides a set of convenient APIs that allow users to easily express flexible and complex inter-task communication structures. Second, TAPA adopts a coarse-grained floorplanning step during HLS compilation for accurate pipelining of potential critical paths. In addition, TAPA implements several optimization techniques specifically tailored for modern HBM-based FPGAs. In our experiments with a total of 43 designs, we improve the average frequency from 147 MHz to 297 MHz (a 102% improvement) with no loss of throughput and a negligible change in resource utilization. Notably, in 16 experiments we make the originally unroutable designs achieve 274 MHz on average. The framework is available at [this https URL](https://github.com/UCLA-VAST/tapa) and the core floorplan module is available at [this https URL](https://github.com/UCLA-VAST/AutoBridge).",https://arxiv.org/abs/2209.02672
Matching Consumer Fairness Objectives & Strategies for RecSys,"Michael D.Ekstrand, Maria SoledadPera","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02662v1)), lastrevised 7 Sep 2022 (this version, v2)",Information Retrieval (cs.IR)," The last several years have brought a growing body of work on ensuring that recommender systems are in some sense consumer-fair -- that is, they provide comparable quality of service, accuracy of representation, and other effects to their users. However, there are many different strategies to make systems more fair and a range of intervention points. In this position paper, we build on ongoing work to highlight the need for researchers and practitioners to attend to the details of their application, users, and the fairness objective they aim to achieve, and adopt interventions that are appropriate to the situation. We argue that consumer fairness should be a creative endeavor flowing from the particularities of the specific problem to be solved.",https://arxiv.org/abs/2209.02663
Group-$k$ Consistent Measurement Set Maximization for Robust OutlierDetection,"BrendonForsgren, RamVasudevan, MichaelKaess, Timothy W.McLain, Joshua G.Mangelson",06-sep-22,Robotics (cs.RO)," This paper presents a method for the robust selection of measurements in a simultaneous localization and mapping (SLAM) framework. Existing methods check consistency or compatibility on a pairwise basis, however many measurement types are not sufficiently constrained in a pairwise scenario to determine if either measurement is inconsistent with the other. This paper presents group-$k$ consistency maximization (G$k$CM) that estimates the largest set of measurements that is internally group-$k$ consistent. Solving for the largest set of group-$k$ consistent measurements can be formulated as an instance of the maximum clique problem on generalized graphs and can be solved by adapting current methods. This paper evaluates the performance of G$k$CM using simulated data and compares it to pairwise consistency maximization (PCM) presented in previous work.",https://arxiv.org/abs/2209.02662
Concentration of polynomial random matrices via Efron-Steininequalities,"GouthamRajendran, MadhurTulsiani",06-sep-22,Computational Complexity (cs.CC)," Analyzing concentration of large random matrices is a common task in a wide variety of fields. Given independent random variables, many tools are available to analyze random matrices whose entries are linear in the variables, e.g. the matrix-Bernstein inequality. However, in many applications, we need to analyze random matrices whose entries are polynomials in the variables. These arise naturally in the analysis of spectral algorithms, e.g., Hopkins et al. [STOC 2016], Moitra-Wein [STOC 2019]; and in lower bounds for semidefinite programs based on the Sum of Squares hierarchy, e.g. Barak et al. [FOCS 2016], Jones et al. [FOCS 2021]. In this work, we present a general framework to obtain such bounds, based on the matrix Efron-Stein inequalities developed by Paulin-Mackey-Tropp [Annals of Probability 2016]. The Efron-Stein inequality bounds the norm of a random matrix by the norm of another simpler (but still random) matrix, which we view as arising by ""differentiating"" the starting matrix. By recursively differentiating, our framework reduces the main task to analyzing far simpler matrices. For Rademacher variables, these simpler matrices are in fact deterministic and hence, analyzing them is far easier. For general non- Rademacher variables, the task reduces to scalar concentration, which is much easier. Moreover, in the setting of polynomial matrices, our results generalize the work of Paulin-Mackey-Tropp. Using our basic framework, we recover known bounds in the literature for simple ""tensor networks"" and ""dense graph matrices"". Using our general framework, we derive bounds for ""sparse graph matrices"", which were obtained only recently by Jones et al. [FOCS 2021] using a nontrivial application of the trace power method, and was a core component in their work. We expect our framework to be helpful for other applications involving concentration phenomena for nonlinear random matrices.",https://arxiv.org/abs/2209.02658
Learning Interpretable Temporal Properties from Positive Examples Only,"RajarshiRoy, Jean-RaphaÃ«lGaglione, NasimBaharisangari, DanielNeider, ZheXu, UfukTopcu",06-sep-22,Logic in Computer Science (cs.LO)," We consider the problem of explaining the temporal behavior of black-box systems using human-interpretable models. To this end, based on recent research trends, we rely on the fundamental yet interpretable models of deterministic finite automata (DFAs) and linear temporal logic (LTL) formulas. In contrast to most existing works for learning DFAs and LTL formulas, we rely on only positive examples. Our motivation is that negative examples are generally difficult to observe, in particular, from black-box systems. To learn meaningful models from positive examples only, we design algorithms that rely on conciseness and language minimality of models as regularizers. To this end, our algorithms adopt two approaches: a symbolic and a counterexample-guided one. While the symbolic approach exploits an efficient encoding of language minimality as a constraint satisfaction problem, the counterexample-guided one relies on generating suitable negative examples to prune the search. Both the approaches provide us with effective algorithms with theoretical guarantees on the learned models. To assess the effectiveness of our algorithms, we evaluate all of them on synthetic data.",https://arxiv.org/abs/2209.02655
Learn to Adapt to New Environment from Past Experience and Few Pilot,"OuyaWang, JiabaoGao, GeoffreyYe Li",02-sep-22,Information Theory (cs.IT)," In recent years, deep learning has been widely applied in communications and achieved remarkable performance improvement. Most of the existing works are based on data-driven deep learning, which requires a significant amount of training data for the communication model to adapt to new environments and results in huge computing resources for collecting data and retraining the model. In this paper, we will significantly reduce the required amount of training data for new environments by leveraging the learning experience from the known environments. Therefore, we introduce few-shot learning to enable the communication model to generalize to new environments, which is realized by an attention-based method. With the attention network embedded into the deep learning-based communication model, environments with different power delay profiles can be learnt together in the training process, which is called the learning experience. By exploiting the learning experience, the communication model only requires few pilot blocks to perform well in the new environment. Through an example of deep- learning-based channel estimation, we demonstrate that this novel design method achieves better performance than the existing data-driven approach designed for few-shot learning.",https://arxiv.org/abs/2209.02650
A Survey on Generative Diffusion Model,"HanqunCao, ChengTan, ZhangyangGao, GuangyongChen, Pheng-Ann Heng, Stan Z. Li",06-sep-22,Artificial Intelligence (cs.AI)," Deep learning shows great potential in generation tasks thanks to deep latent representation. Generative models are classes of models that can generate observations randomly with respect to certain implied parameters. Recently, the diffusion Model becomes a raising class of generative models by virtue of its power-generating ability. Nowadays, great achievements have been reached. More applications except for computer vision, speech generation, bioinformatics, and natural language processing are to be explored in this field. However, the diffusion model has its natural drawback of a slow generation process, leading to many enhanced works. This survey makes a summary of the field of the diffusion model. We firstly state the main problem with two landmark works - DDPM and DSM. Then, we present a diverse range of advanced techniques to speed up the diffusion models - training schedule, training-free sampling, mixed-modeling, and score & diffusion unification. Regarding existing models, we also provide a benchmark of FID score, IS, and NLL according to specific NFE. Moreover, applications with diffusion models are introduced including computer vision, sequence modeling, audio, and AI for science. Finally, there is a summarization of this field together with limitations & further directions.",https://arxiv.org/abs/2209.02649
DFI: An Interprocedural Value-Flow Analysis Framework that Scales toLarge Codebases,"Min-YihHsu, FelicitasHetzelt, MichaelFranz",06-sep-22,Programming Languages (cs.PL)," Context- and flow-sensitive value-flow information is an important building block for many static analysis tools. Unfortunately, current approaches to compute value-flows do not scale to large codebases, due to high memory and runtime requirements. This paper proposes a new scalable approach to compute value-flows via graph reachability. To this end, we develop a new graph structure as an extension of LLVM IR that contains two additional operations which significantly simplify the modeling of pointer aliasing. Further, by processing nodes in the opposite direction of SSA def- use chains, we are able to minimize the tree width of the resulting graph. This allows us to employ efficient tree traversal algorithms in order to resolve graph reachability.   We present a value-flow analysis framework,DFI, implementing our approach. We compare DFI against two state-of-the-art value-flow analysis frameworks, Phasar and SVF, to extract value-flows from 4 real-world software projects. Given 32GB of memory, Phasar and SVF are unable to complete analysis of larger projects such as OpenSSL or FFmpeg, while DFI is able to complete all evaluations. For the subset of benchmarks that Phasar and SVF do handle, DFI requires significantly less memory (1.5% of Phasar's, 6.4% of SVF's memory footprint on average) and runs significantly faster (23x speedup over Phasar, 57x compared to SVF). Our analysis shows that, in contrast to previous approaches, DFI's memory and runtime requirements scale almost linearly with the number of analyzed instructions.",https://arxiv.org/abs/2209.02646
Multi-agent Deep Reinforcement Learning for Charge-sustaining Controlof Multi-mode Hybrid Vehicles,"MinHua, QuanZhou, CetengfeiZhang, Hongming Xu, Wei Liu",06-sep-22,Machine Learning (cs.LG)," Transportation electrification requires an increasing number of electric components (e.g., electric motors and electric energy storage systems) on vehicles, and control of the electric powertrains usually involves multiple inputs and multiple outputs (MIMO). This paper focused on the online optimization of energy management strategy for a multi-mode hybrid electric vehicle based on multi-agent reinforcement learning (MARL) algorithms that aim to address MIMO control optimization while most existing methods only deal with single output control. A new collaborative cyber- physical learning with multi-agents is proposed based on the analysis of the evolution of energy efficiency of the multi-mode hybrid electric vehicle (HEV) optimized by a deep deterministic policy gradient (DDPG)-based MARL algorithm. Then a learning driving cycle is set by a novel random method to speed up the training process. Eventually, network design, learning rate, and policy noise are incorporated in the sensibility analysis and the DDPG- based algorithm parameters are determined, and the learning performance with the different relationships of multi-agents is studied and demonstrates that the not completely independent relationship with Ratio 0.2 is the best. The compassion study with the single-agent and multi-agent suggests that the multi-agent can achieve approximately 4% improvement of total energy over the single-agent scheme. Therefore, the multi-objective control by MARL can achieve good optimization effects and application efficiency.",https://arxiv.org/abs/2209.02638
Weak Collocation Regression method: fast reveal hidden stochasticdynamics from high-dimensional aggregate data,"LiweiLu, ZhijunZeng, YanJiang, YiZhu, PipiHu","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02628v1)), lastrevised 7 Sep 2022 (this version, v2)",Numerical Analysis (math.NA)," Revealing hidden dynamics from the stochastic data is a challenging problem as randomness takes part in the evolution of the data. The problem becomes exceedingly complex when the trajectories of the stochastic data are absent in many scenarios. Here we present an approach to effectively modeling the dynamics of the stochastic data without trajectories based on the weak form of the Fokker-Planck (FP) equation, which governs the evolution of the density function in the Brownian process. Taking the collocations of Gaussian functions as the test functions in the weak form of the FP equation, we transfer the derivatives to the Gaussian functions and thus approximate the weak form by the expectational sum of the data. With a dictionary representation of the unknown terms, a linear system is built and then solved by the regression, revealing the unknown dynamics of the data. Hence, we name the method with the Weak Collocation Regression (WCR) method for its three key components: weak form, collocation of Gaussian kernels, and regression. The numerical experiments show that our method is flexible and fast, which reveals the dynamics within seconds in multi-dimensional problems and can be easily extended to high-dimensional data such as 20 dimensions. WCR can also correctly identify the hidden dynamics of the complex tasks with variable-dependent diffusion and coupled drift, and the performance is robust, achieving high accuracy in the case with noise added.",https://arxiv.org/abs/2209.02633
Single-Stage Broad Multi-Instance Multi-Label Learning (BMIML) withDiverse Inter-Correlations and its application to medical image classification,"QiLai, JianhangZhou, YanfenGan, Chi-ManVong, DeshuangHuang",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In many real-world applications, one object (e.g., image) can be represented or described by multiple instances (e.g., image patches) and simultaneously associated with multiple labels. Such applications can be formulated as multi-instance multi-label learning (MIML) problems and have been extensively studied during the past few years. Existing MIML methods are useful in many applications but most of which suffer from relatively low accuracy and training efficiency due to several issues: i) the inter-label correlations (i.e., the probabilistic correlations between the multiple labels corresponding to an object) are neglected; ii) the inter-instance correlations cannot be learned directly (or jointly) with other types of correlations due to the missing instance labels; iii) diverse inter- correlations (e.g., inter-label correlations, inter-instance correlations) can only be learned in multiple stages. To resolve these issues, a new single-stage framework called broad multi-instance multi-label learning (BMIML) is proposed. In BMIML, there are three innovative modules: i) an auto-weighted label enhancement learning (AWLEL) based on broad learning system (BLS); ii) A specific MIML neural network called scalable multi- instance probabilistic regression (SMIPR); iii) Finally, an interactive decision optimization (IDO). As a result, BMIML can achieve simultaneous learning of diverse inter-correlations between whole images, instances, and labels in single stage for higher classification accuracy and much faster training time. Experiments show that BMIML is highly competitive to (or even better than) existing methods in accuracy and much faster than most MIML methods even for large medical image data sets ( 90K images).",https://arxiv.org/abs/2209.02628
Neural network approximation of coarse-scale surrogates in numericalhomogenization,"FabianKrÃ¶pfl, RolandMaier, DanielPeterseim",06-sep-22,Numerical Analysis (math.NA)," Coarse-scale surrogate models in the context of numerical homogenization of linear elliptic problems with arbitrary rough diffusion coefficients rely on the efficient solution of fine-scale sub-problems on local subdomains whose solutions are then employed to deduce appropriate coarse contributions to the surrogate model. However, in the absence of periodicity and scale separation, the reliability of such models requires the local subdomains to cover the whole domain which may result in high offline costs, in particular for parameter-dependent and stochastic problems. This paper justifies the use of neural networks for the approximation of coarse-scale surrogate models by analyzing their approximation properties. For a prototypical and representative numerical homogenization technique, the Localized Orthogonal Decomposition method, we show that one single neural network is sufficient to approximate the coarse contributions of all occurring coefficient-dependent local sub-problems for a non-trivial class of diffusion coefficients up to arbitrary accuracy. We present rigorous upper bounds on the depth and number of non-zero parameters for such a network to achieve a given accuracy. Further, we analyze the overall error of the resulting neural network enhanced numerical homogenization surrogate model.",https://arxiv.org/abs/2209.02625
Priority Based Synchronization for Faster Learning in Games,"AbbasaliKoochakzadeh, YasinYazÄ±cÄ±oÄŸlu",06-sep-22,Systems and Control (eess.SY)," Learning in games has been widely used to solve many cooperative multi-agent problems such as coverage control, consensus, self- reconfiguration or vehicle-target assignment. One standard approach in this domain is to formulate the problem as a potential game and to use an algorithm such as log-linear learning to achieve the stochastic stability of globally optimal configurations. Standard versions of such learning algorithms are asynchronous, i.e., only one agent updates its action at each round of the learning process. To enable faster learning, we propose a synchronization strategy based on decentralized random prioritization of agents, which allows multiple agents to change their actions simultaneously when they do not affect each other's utility or feasible actions. We show that the proposed approach can be integrated into any standard asynchronous learning algorithm to improve the convergence speed while maintaining the limiting behavior (e.g., stochastically stable configurations). We support our theoretical results with simulations in a coverage control scenario.",https://arxiv.org/abs/2209.02624
Variable binding and substitution for (nameless) dummies,"AndrÃ©, Hirschowitz, TomHirschowitz, AmbroiseLafont, MarcoMaggesi",06-sep-22,Logic in Computer Science (cs.LO)," By abstracting over well-known properties of De Bruijn's representation with nameless dummies, we design a new theory of syntax with variable binding and capture-avoiding substitution. We propose it as a simpler alternative to Fiore, Plotkin, and Turi's approach, with which we establish a strong formal link. We also show that our theory easily incorporates simple types and equations between terms.",https://arxiv.org/abs/2209.02617
A perspective to navigate the National Laboratory environment for RSEcareer growth,William FGodoy,06-sep-22,Software Engineering (cs.SE)," This paper shares a perspective for the research software engineering (RSE) community to navigate the National Laboratory landscape. The RSE role is a recent concept that led to organizational challenges to place and evaluate their impact, costs and benefits. The premise is that RSEs are a natural fit into the current landscape and can use traditional career growth strategies in science: publications, community engagements and proposals. Projects funding RSEs can benefit from this synergy and be inclusive on traditional activities. Still, a great deal of introspection is needed to close gaps between the rapidly evolving RSE landscape and the well-established communication patterns in science. This perspective is built upon interactions in industry, academia and government in high- performance computing (HPC) environments. The goal is to contribute to the conversation around RSE career growth and understand their return on investment for scientific projects and sponsors.",https://arxiv.org/abs/2209.02614
Merged-GHCIDR: Geometrical Approach to Reduce Image Data,"DevvratJoshi, JanviThakkar, SiddharthSoni, ShrilMody, RohanPatil, NipunBatra",06-sep-22,Machine Learning (cs.LG)," The computational resources required to train a model have been increasing since the inception of deep networks. Training neural networks on massive datasets have become a challenging and time-consuming task. So, there arises a need to reduce the dataset without compromising the accuracy. In this paper, we present novel variations of an earlier approach called reduction through homogeneous clustering for reducing dataset size. The proposed methods are based on the idea of partitioning the dataset into homogeneous clusters and selecting images that contribute significantly to the accuracy. We propose two variations: Geometrical Homogeneous Clustering for Image Data Reduction (GHCIDR) and Merged-GHCIDR upon the baseline algorithm - Reduction through Homogeneous Clustering (RHC) to achieve better accuracy and training time. The intuition behind GHCIDR involves selecting data points by cluster weights and geometrical distribution of the training set. Merged-GHCIDR involves merging clusters having the same labels using complete linkage clustering. We used three deep learning models- Fully Connected Networks (FCN), VGG1, and VGG16. We experimented with the two variants on four datasets- MNIST, CIFAR10, Fashion-MNIST, and Tiny-Imagenet. Merged-GHCIDR with the same percentage reduction as RHC showed an increase of 2.8%, 8.9%, 7.6% and 3.5% accuracy on MNIST, Fashion-MNIST, CIFAR10, and Tiny-Imagenet, respectively.",https://arxiv.org/abs/2209.02610
Automatic counting of mounds on UAV images: combining instancesegmentation and patch-level correction,"Majid NikougoftarNategh, AhmedZgaren, WassimBouachir, NizarBouguila",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Site preparation by mounding is a commonly used silvicultural treatment that improves tree growth conditions by mechanically creating planting microsites called mounds. Following site preparation, the next critical step is to count the number of mounds, which provides forest managers with a precise estimate of the number of seedlings required for a given plantation block. Counting the number of mounds is generally conducted through manual field surveys by forestry workers, which is costly and prone to errors, especially for large areas. To address this issue, we present a novel framework exploiting advances in Unmanned Aerial Vehicle (UAV) imaging and computer vision to accurately estimate the number of mounds on a planting block. The proposed framework comprises two main components. First, we exploit a visual recognition method based on a deep learning algorithm for multiple object detection by pixel-based segmentation. This enables a preliminary count of visible mounds, as well as other frequently seen objects (e.g. trees, debris, accumulation of water), to be used to characterize the planting block. Second, since visual recognition could limited by several perturbation factors (e.g. mound erosion, occlusion), we employ a machine learning estimation function that predicts the final number of mounds based on the local block properties extracted in the first stage. We evaluate the proposed framework on a new UAV dataset representing numerous planting blocks with varying features. The proposed method outperformed manual counting methods in terms of relative counting precision, indicating that it has the potential to be advantageous and efficient in difficult situations.",https://arxiv.org/abs/2209.02609
Unifying Generative Models with GFlowNets,"DinghuaiZhang, RickyT. Q.Chen, NikolayMalkin, YoshuaBengio",06-sep-22,Machine Learning (cs.LG)," There are many frameworks for deep generative modeling, each often presented with their own specific training algorithms and inference methods. We present a short note on the connections between existing deep generative models and the GFlowNet framework, shedding light on their overlapping traits and providing a unifying viewpoint through the lens of learning with Markovian trajectories. This provides a means for unifying training and inference algorithms, and provides a route to construct an agglomeration of generative models.",https://arxiv.org/abs/2209.02608
Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module,"YiheLiu, ZiqiYuan, Huisheng Mao, ZhiyunLiang, WanqiuyueYang, YuanzheQiu, TieCheng, Xiaoteng Li, Hua Xu, KaiGao",22 Aug 2022,Multimedia (cs.MM)," Multimodal sentiment analysis (MSA), which supposes to improve text-based sentiment analysis with associated acoustic and visual modalities, is an emerging research area due to its potential applications in Human-Computer Interaction (HCI). However, the existing researches observe that the acoustic and visual modalities contribute much less than the textual modality, termed as text-predominant. Under such circumstances, in this work, we emphasize making non-verbal cues matter for the MSA task. Firstly, from the resource perspective, we present the CH-SIMS v2.0 dataset, an extension and enhancement of the CH-SIMS. Compared with the original dataset, the CH-SIMS v2.0 doubles its size with another 2121 refined video segments with both unimodal and multimodal annotations and collects 10161 unlabelled raw video segments with rich acoustic and visual emotion-bearing context to highlight non-verbal cues for sentiment prediction. Secondly, from the model perspective, benefiting from the unimodal annotations and the unsupervised data in the CH-SIMS v2.0, the Acoustic Visual Mixup Consistent (AV-MC) framework is proposed. The designed modality mixup module can be regarded as an augmentation, which mixes the acoustic and visual modalities from different videos. Through drawing unobserved multimodal context along with the text, the model can learn to be aware of different non-verbal contexts for sentiment prediction. Our evaluations demonstrate that both CH- SIMS v2.0 and AV-MC framework enables further research for discovering emotion-bearing acoustic and visual cues and paves the path to interpretable end-to-end HCI applications for real-world scenarios.",https://arxiv.org/abs/2209.02606
Domain Engineering for Applied Monocular Reconstruction of ParametricFaces,"IgorBorovikov, KarineLevonyan, Jon Rein, PawelWrotek, NitishVictor",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Many modern online 3D applications and video games rely on parametric models of human faces for creating believable avatars. However, manually reproducing someone's facial likeness with a parametric model is difficult and time-consuming. Machine Learning solution for that task is highly desirable but is also challenging. The paper proposes a novel approach to the so-called Face-to-Parameters problem (F2P for short), aiming to reconstruct a parametric face from a single image. The proposed method utilizes synthetic data, domain decomposition, and domain adaptation to address multifaceted challenges in solving the F2P. The open-sourced codebase illustrates our key observations and provides means for quantitative evaluation. The presented approach proves practical in an industrial application; it improves accuracy and allows for more efficient models training. The techniques have the potential to extend to other types of parametric models.",https://arxiv.org/abs/2209.02604
Applied monocular reconstruction of parametric faces with domainengineering,"IgorBorovikov, KarineLevonyan, Jon Rein, PawelWrotek, NitishVictor",5 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," Many modern online 3D applications and videogames rely on parametric models of human faces for creating believable avatars. However, manual reproduction of someone's facial likeness with a parametric model is difficult and time-consuming. Machine Learning solution for that task is highly desirable but is also challenging. The paper proposes a novel approach to the so-called Face-to-Parameters problem (F2P for short), aiming to reconstruct a parametric face from a single image. The proposed method utilizes synthetic data, domain decomposition, and domain adaptation for addressing multifaceted challenges in solving the F2P. The open-sourced codebase illustrates our key observations and provides means for quantitative evaluation. The presented approach proves practical in an industrial application; it improves accuracy and allows for more efficient models training. The techniques have the potential to extend to other types of parametric models.",https://arxiv.org/abs/2209.02600
A neuromorphic approach to image processing and machine vision,ArvindSubramaniam,7 Aug 2022,Neural and Evolutionary Computing (cs.NE)," Neuromorphic engineering is essentially the development of artificial systems, such as electronic analog circuits that employ information representations found in biological nervous systems. Despite being faster and more accurate than the human brain, computers lag behind in recognition capability. However, it is envisioned that the advancement in neuromorphics, pertaining to the fields of computer vision and image processing will provide a considerable improvement in the way computers can interpret and analyze information. In this paper, we explore the implementation of visual tasks such as image segmentation, visual attention and object recognition. Moreover, the concept of anisotropic diffusion has been examined followed by a novel approach employing memristors to execute image segmentation. Additionally, we have discussed the role of neuromorphic vision sensors in artificial visual systems and the protocol involved in order to enable asynchronous transmission of signals. Moreover, two widely accepted algorithms that are used to emulate the process of object recognition and visual attention have also been discussed. Throughout the span of this paper, we have emphasized on the employment of non-volatile memory devices such as memristors to realize artificial visual systems. Finally, we discuss about hardware accelerators and wish to represent a case in point for arguing that progress in computer vision may benefit directly from progress in non-volatile memory technology.",https://arxiv.org/abs/2208.02935
Improving the Accuracy and Robustness of CNNs Using a Deep CCA NeuralData Regularizer,"CassidyPirlot, Richard C.Gerum, Cory Efird, JoelZylberberg, Alona Fyshe",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," As convolutional neural networks (CNNs) become more accurate at object recognition, their representations become more similar to the primate visual system. This finding has inspired us and other researchers to ask if the implication also runs the other way: If CNN representations become more brain-like, does the network become more accurate? Previous attempts to address this question showed very modest gains in accuracy, owing in part to limitations of the regularization method. To overcome these limitations, we developed a new neural data regularizer for CNNs that uses Deep Canonical Correlation Analysis (DCCA) to optimize the resemblance of the CNN's image representations to that of the monkey visual cortex. Using this new neural data regularizer, we see much larger performance gains in both classification accuracy and within-super-class accuracy, as compared to the previous state-of-the-art neural data regularizers. These networks are also more robust to adversarial attacks than their unregularized counterparts. Together, these results confirm that neural data regularization can push CNN performance higher, and introduces a new method that obtains a larger performance boost.",https://arxiv.org/abs/2209.02595
Contextualizing Large-Scale Domain Knowledge for Conceptual Modelingand Simulation,"SungeunAn, SpencerRugaber, JenniferHammock, Ashok K.Goel",06-sep-22,Human-Computer Interaction (cs.HC)," We present an interactive modeling tool, VERA, that scaffolds the acquisition of domain knowledge involved in conceptual modeling and agent- based simulations. We describe the knowledge engineering process of contextualizing large-scale domain knowledge. Specifically, we use the ontology of biotic interactions in Global Biotic Interactions, and the trait data of species in Encyclopedia of Life to facilitate the model construction. Learners can use VERA to construct qualitative conceptual models of ecological phenomena, run them as quantitative simulations, and review their predictions.",https://arxiv.org/abs/2209.02582
Avgust: Automating Usage-Based Test Generation from Videos of AppExecutions,"YixueZhao, SagharTalebipour, KesinaBaral, Hyojae Park, Leon Yee, Safwat AliKhan, YuriyBrun, NenadMedvidovic, Kevin Moran",06-sep-22,Software Engineering (cs.SE)," Writing and maintaining UI tests for mobile apps is a time- consuming and tedious task. While decades of research have produced automated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69% of the tests Avgust generates successfully execute the desired usage, and that Avgust's classifiers outperform the state of the art.",https://arxiv.org/abs/2209.02579
Cognitive Assistance for Inquiry-Based Modeling,"SungeunAn, RobertBates, SpencerRugaber, JenniferHammock, EmilyWeigel, Ashok K.Goel",06-sep-22,Human-Computer Interaction (cs.HC)," Inquiry-based modeling is essential to scientific practice. However, modeling is difficult for novice scientists in part due to limited domain-specific knowledge and quantitative skills. VERA is an interactive tool that helps users construct conceptual models of ecological phenomena, run them as simulations, and examine their predictions. VERA provides cognitive scaffolding for modeling by supplying access to large-scale domain knowledge. The VERA system was tested by college-level students in two different settings: a general ecology lecture course (N=91) at a large southeastern R1 university and a controlled experiment in a research laboratory (N=15). Both studies indicated that engaging students in ecological modeling through VERA helped them better understand basic biological concepts. The latter study additionally revealed that providing access to domain knowledge helped students build more complex models.",https://arxiv.org/abs/2209.02577
Towards Understanding Third-party Library Dependency in C/C++Ecosystem,"WeiTang, ZhengziXu, ChengweiLiu, JiahuiWu, ShouguoYang, YiLi, PingLuo, YangLiu",06-sep-22,Software Engineering (cs.SE)," Third-party libraries (TPLs) are frequently reused in software to reduce development cost and the time to market. However, external library dependencies may introduce vulnerabilities into host applications. The issue of library dependency has received considerable critical attention. Many package managers, such as Maven, Pip, and NPM, are proposed to manage TPLs. Moreover, a significant amount of effort has been put into studying dependencies in language ecosystems like Java, Python, and JavaScript except C/C++. Due to the lack of a unified package manager for C/C++, existing research has only few understanding of TPL dependencies in the C/C++ ecosystem, especially at large scale.   Towards understanding TPL dependencies in the C/C++ecosystem, we collect existing TPL databases, package management tools, and dependency detection tools, summarize the dependency patterns of C/C++ projects, and construct a comprehensive and precise C/C++ dependency detector. Using our detector, we extract dependencies from a large-scale database containing 24K C/C++ repositories from GitHub. Based on the extracted dependencies, we provide the results and findings of an empirical study, which aims at understanding the characteristics of the TPL dependencies. We further discuss the implications to manage dependency for C/C++ and the future research directions for software engineering researchers and developers in fields of library development, software composition analysis, and C/C++package manager.",https://arxiv.org/abs/2209.02576
When Privacy Meets Partial Information: A Refined Analysis ofDifferentially Private Bandits,"AchrafAzize, DebabrotaBasu",06-sep-22,Machine Learning (cs.LG)," We study the problem of multi-armed bandits with $\epsilon$-global Differential Privacy (DP). First, we prove the minimax and problem-dependent regret lower bounds for stochastic and linear bandits that quantify the hardness of bandits with $\epsilon$-global DP. These bounds suggest the existence of two hardness regimes depending on the privacy budget $\epsilon$. In the high-privacy regime (small $\epsilon$), the hardness depends on a coupled effect of privacy and partial information about the reward distributions. In the low-privacy regime (large $\epsilon$), bandits with $\epsilon$-global DP are not harder than the bandits without privacy. For stochastic bandits, we further propose a generic framework to design a near-optimal $\epsilon$ global DP extension of an index-based optimistic bandit algorithm. The framework consists of three ingredients: the Laplace mechanism, arm-dependent adaptive episodes, and usage of only the rewards collected in the last episode for computing private statistics. Specifically, we instantiate $\epsilon$-global DP extensions of UCB and KL- UCB algorithms, namely AdaP-UCB and AdaP-KLUCB. AdaP-KLUCB is the first algorithm that both satisfies $\epsilon$-global DP and yields a regret upper bound that matches the problem-dependent lower bound up to multiplicative constants.",https://arxiv.org/abs/2209.02575
Identification of Small Objects in Satellite Image Benchmarks,"DebojyotiBiswas, JelenaTeÅ¡iÄ‡",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Recent increases in aerial image access and volume, increases in computational power, and interest in applications have opened the door to scaling up object detection and domain adaptation research to production. Aerial data sets are very large in size, and each frame of the data set contains a huge number of dense and small objects. Deep learning applications for aerial imagery are behind due to a lack of training data, and researchers have recently turned to domain adaptation (DA) from a labeled data set to an unlabeled data set to alleviate the issue. These factors create two major challenges: the high variety between datasets (e.g. object sizes, class distributions, object feature uniformity, image acquisition, distance, weather conditions), and the size of objects in satellite imagery and subsequent failure of state-of-the-art to capture small objects, local features, and region proposals for densely overlapped objects in satellite image. In this paper, we propose two solutions to these problems: a domain discriminator to better align the local feature space between domains; and a novel pipeline that improves the back-end by spatial pyramid pooling, cross-stage partial network, region proposal network via heatmap-based region proposals, and object localization and identification through a novel image difficulty score that adapts the overall focal loss measure based on the image difficulty. Our proposed model outperformed the state-of-the-art method by 7.4%.",https://arxiv.org/abs/2209.02570
Project proposal: A modular reinforcement learning based automatedtheorem prover,BorisShminke,06-sep-22,Artificial Intelligence (cs.AI)," We propose to build a reinforcement learning prover of independent components: a deductive system (an environment), the proof state representation (how an agent sees the environment), and an agent training algorithm. To that purpose, we contribute an additional Vampire-based environment to $\texttt{gym-saturation}$ package of OpenAI Gym environments for saturation provers. We demonstrate a prototype of using $\texttt{gym- saturation}$ together with a popular reinforcement learning framework (Ray $\texttt{RLlib}$). Finally, we discuss our plans for completing this work in progress to a competitive automated theorem prover.",https://arxiv.org/abs/2209.02564
The Outcome of the 2022 Landslide4Sense Competition: AdvancedLandslide Detection from Multi-Source Satellite Imagery,"OmidGhorbanzadeh, Yonghao Xu, Hengwei Zhao, Junjue Wang, YanfeiZhong, DongZhao, QiZang, ShuangWang, FahongZhang, YileiShi, XiaoXiang Zhu, Lin Bai, Weile Li, Weihang Peng, PedramGhamisi",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The scientific outcomes of the 2022 Landslide4Sense (L4S) competition organized by the Institute of Advanced Research in Artificial Intelligence (IARAI) are presented here. The objective of the competition is to automatically detect landslides based on large-scale multiple sources of satellite imagery collected globally. The 2022 L4S aims to foster interdisciplinary research on recent developments in deep learning (DL) models for the semantic segmentation task using satellite imagery. In the past few years, DL-based models have achieved performance that meets expectations on image interpretation, due to the development of convolutional neural networks (CNNs). The main objective of this article is to present the details and the best-performing algorithms featured in this competition. The winning solutions are elaborated with state-of-the-art models like the Swin Transformer, SegFormer, and U-Net. Advanced machine learning techniques and strategies such as hard example mining, self- training, and mix-up data augmentation are also considered. Moreover, we describe the L4S benchmark data set in order to facilitate further comparisons, and report the results of the accuracy assessment online. The data is accessible on \textit{Future Development Leaderboard} for future evaluation at \url{[this https URL](https://www.iarai.ac.at/landslide4sense/challenge/)}, and researchers are invited to submit more prediction results, evaluate the accuracy of their methods, compare them with those of other users, and, ideally, improve the landslide detection results reported in this article.",https://arxiv.org/abs/2209.02562
Finite-Time Error Bounds for Greedy-GQ,"YueWang, YiZhou, Shaofeng Zou",06-sep-22,Machine Learning (cs.LG)," Greedy-GQ with linear function approximation, originally proposed in \cite{maei2010toward}, is a value-based off-policy algorithm for optimal control in reinforcement learning, and it has a non-linear two timescale structure with a non-convex objective function. This paper develops its finite-time error bounds. We show that the Greedy-GQ algorithm converges as fast as $\mathcal{O}({1}/{\sqrt{T}})$ under the i.i.d.\ setting and $\mathcal{O}({\log T}/{\sqrt{T}})$ under the Markovian setting. We further design a variant of the vanilla Greedy-GQ algorithm using the nested-loop approach, and show that its sample complexity is $\mathcal{O}({\log(1/\epsilon)\epsilon^{-2}})$, which matches with the one of the vanilla Greedy-GQ. Our finite-time error bounds match with one of the stochastic gradient descent algorithms for general smooth non-convex optimization problems. Our finite-sample analysis provides theoretical guidance on choosing step-sizes for faster convergence in practice and suggests the trade-off between the convergence rate and the quality of the obtained policy. Our techniques in this paper provide a general approach for finite-sample analysis of non-convex two timescale value-based reinforcement learning algorithms.",https://arxiv.org/abs/2209.02556
Explaining Machine Learning Models in Natural Conversations: Towards aConversational XAI Agent,"Van BachNguyen, JÃ¶rgSchlÃ¶tterer, ChristinSeifert",06-sep-22,Artificial Intelligence (cs.AI)," The goal of Explainable AI (XAI) is to design methods to provide insights into the reasoning process of black-box models, such as deep neural networks, in order to explain them to humans. Social science research states that such explanations should be conversational, similar to human-to-human explanations. In this work, we show how to incorporate XAI in a conversational agent, using a standard design for the agent comprising natural language understanding and generation components. We build upon an XAI question bank which we extend by quality-controlled paraphrases to understand the user's information needs. We further systematically survey the literature for suitable explanation methods that provide the information to answer those questions, and present a comprehensive list of suggestions. Our work is the first step towards truly natural conversations about machine learning models with an explanation agent. The comprehensive list of XAI questions and the corresponding explanation methods may support other researchers in providing the necessary information to address users' demands.",https://arxiv.org/abs/2209.02555
Graph-PHPA: Graph-based Proactive Horizontal Pod Autoscaling forMicroservices using LSTM-GNN,"Hoa X.Nguyen, Shaoshu Zhu, Mingming Liu",06-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," Microservice-based architecture has become prevalent for cloud- native applications. With an increasing number of applications being deployed on cloud platforms every day leveraging this architecture, more research efforts are required to understand how different strategies can be applied to effectively manage various cloud resources at scale. A large body of research has deployed automatic resource allocation algorithms using reactive and proactive autoscaling policies. However, there is still a gap in the efficiency of current algorithms in capturing the important features of microservices from their architecture and deployment environment, for example, lack of consideration of graphical dependency. To address this challenge, we propose Graph-PHPA, a graph-based proactive horizontal pod autoscaling strategy for allocating cloud resources to microservices leveraging long short-term memory (LSTM) and graph neural network (GNN) based prediction methods. We evaluate the performance of Graph-PHPA using the Bookinfo microservices deployed in a dedicated testing environment with real-time workloads generated based on realistic datasets. We demonstrate the efficacy of Graph-PHPA by comparing it with the rule-based resource allocation scheme in Kubernetes as our baseline. Extensive experiments have been implemented and our results illustrate the superiority of our proposed approach in resource savings over the reactive rule-based baseline algorithm in different testing scenarios.",https://arxiv.org/abs/2209.02552
Efficient search of active inference policy spaces using k-means,"Alex B.Kiefer, MahaultAlbarracin","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02550v1)), lastrevised 7 Sep 2022 (this version, v2)",Machine Learning (cs.LG)," We develop an approach to policy selection in active inference that allows us to efficiently search large policy spaces by mapping each policy to its embedding in a vector space. We sample the expected free energy of representative points in the space, then perform a more thorough policy search around the most promising point in this initial sample. We consider various approaches to creating the policy embedding space, and propose using k-means clustering to select representative points. We apply our technique to a goal-oriented graph-traversal problem, for which naive policy selection is intractable for even moderately large graphs.",https://arxiv.org/abs/2209.02551
XSimGCL: Towards Extremely Simple Graph Contrastive Learning forRecommendation,"JunliangYu, XinXia, TongChen, LizhenCui, NguyenQuoc VietHung, Hongzhi Yin",06-sep-22,Information Retrieval (cs.IR)," Contrastive learning (CL) has recently been demonstrated critical in improving recommendation performance. The fundamental idea of CL-based recommendation models is to maximize the consistency between representations learned from different graph augmentations of the user-item bipartite graph. In such a self-supervised manner, CL-based recommendation models are expected to extract general features from the raw data to tackle the data sparsity issue. Despite the effectiveness of this paradigm, we still have no clue what underlies the performance gains. In this paper, we first reveal that CL enhances recommendation through endowing the model with the ability to learn more evenly distributed user/item representations, which can implicitly alleviate the pervasive popularity bias and promote long-tail items. Meanwhile, we find that the graph augmentations, which were considered a necessity in prior studies, are relatively unreliable and less significant in CL-based recommendation. On top of these findings, we put forward an eXtremely Simple Graph Contrastive Learning method (XSimGCL) for recommendation, which discards the ineffective graph augmentations and instead employs a simple yet effective noise-based embedding augmentation to create views for CL. A comprehensive experimental study on three large and highly sparse benchmark datasets demonstrates that, though the proposed method is extremely simple, it can smoothly adjust the uniformity of learned representations and outperforms its graph augmentation-based counterparts by a large margin in both recommendation accuracy and training efficiency. The code is released at [this https URL](https://github.com/Coder-Yu/SELFRec).",https://arxiv.org/abs/2209.02550
CAMO-MOT: Combined Appearance-Motion Optimization for 3D Multi-ObjectTracking with Camera-LiDAR Fusion,"LiWang, XinyuZhang, Wenyuan Qin, Xiaoyu Li, LeiYang, ZhiweiLi, LeiZhu, HongWang, JunLi, HuapingLiu","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02540v1)), lastrevised 7 Sep 2022 (this version, v2)",Computer Vision and Pattern Recognition (cs.CV)," 3D Multi-object tracking (MOT) ensures consistency during continuous dynamic detection, conducive to subsequent motion planning and navigation tasks in autonomous driving. However, camera-based methods suffer in the case of occlusions and it can be challenging to accurately track the irregular motion of objects for LiDAR-based methods. Some fusion methods work well but do not consider the untrustworthy issue of appearance features under occlusion. At the same time, the false detection problem also significantly affects tracking. As such, we propose a novel camera-LiDAR fusion 3D MOT framework based on the Combined Appearance-Motion Optimization (CAMO-MOT), which uses both camera and LiDAR data and significantly reduces tracking failures caused by occlusion and false detection. For occlusion problems, we are the first to propose an occlusion head to select the best object appearance features multiple times effectively, reducing the influence of occlusions. To decrease the impact of false detection in tracking, we design a motion cost matrix based on confidence scores which improve the positioning and object prediction accuracy in 3D space. As existing multi-object tracking methods only consider a single category, we also propose to build a multi-category loss to implement multi-object tracking in multi-category scenes. A series of validation experiments are conducted on the KITTI and nuScenes tracking benchmarks. Our proposed method achieves state-of-the-art performance and the lowest identity switches (IDS) value (23 for Car and 137 for Pedestrian) among all multi-modal MOT methods on the KITTI test dataset. And our proposed method achieves state-of-the-art performance among all algorithms on the nuScenes test dataset with 75.3% AMOTA.",https://arxiv.org/abs/2209.02544
Semantic Image Synthesis with Semantically Coupled VQ-Model,"StephanAlaniz, ThomasHummel, Zeynep Akata",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Semantic image synthesis enables control over unconditional image generation by allowing guidance on what is being generated. We conditionally synthesize the latent space from a vector quantized model (VQ-model) pre- trained to autoencode images. Instead of training an autoregressive Transformer on separately learned conditioning latents and image latents, we find that jointly learning the conditioning and image latents significantly improves the modeling capabilities of the Transformer model. While our jointly trained VQ-model achieves a similar reconstruction performance to a vanilla VQ-model for both semantic and image latents, tying the two modalities at the autoencoding stage proves to be an important ingredient to improve autoregressive modeling performance. We show that our model improves semantic image synthesis using autoregressive models on popular semantic image datasets ADE20k, Cityscapes and COCO-Stuff.",https://arxiv.org/abs/2209.02540
Analyzing Transformers in Embedding Space,"GuyDar, MorGeva, AnkitGupta, JonathanBerant",06-sep-22,Computation and Language (cs.CL)," Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by ``translating'' the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only.",https://arxiv.org/abs/2209.02536
Understanding Longitudinal Behaviors of Toxic Accounts on Reddit,"DeepakKumar, JeffHancock, KurtThomas, ZakirDurumeric",06-sep-22,Social and Information Networks (cs.SI)," Toxic comments are the top form of hate and harassment experienced online. While many studies have investigated the types of toxic comments posted online, the effects that such content has on people, and the impact of potential defenses, no study has captured the long-term behaviors of the accounts that post toxic comments or how toxic comments are operationalized. In this paper, we present a longitudinal measurement study of 929K accounts that post toxic comments on Reddit over an 18~month period. Combined, these accounts posted over 14 million toxic comments that encompass insults, identity attacks, threats of violence, and sexual harassment. We explore the impact that these accounts have on Reddit, the targeting strategies that abusive accounts adopt, and the distinct patterns that distinguish classes of abusive accounts. Our analysis forms the foundation for new time-based and graph-based features that can improve automated detection of toxic behavior online and informs the nuanced interventions needed to address each class of abusive account.",https://arxiv.org/abs/2209.02535
A Combined Inverse Kinematics Algorithm Using FABRIK with Optimization,"ZichunXu, YuntaoLi, XiaohangYang, ZhiyuanZhao, JingdongZhao, HongLiu",06-sep-22,Robotics (cs.RO)," Forward and backward reaching inverse kinematics (FABRIK) is a heuristic inverse kinematics solver that is gradually applied to manipulators with the advantages of fast convergence and generating more realistic configurations. However, under the high error constraint, FABRIK exhibits unstable convergence behavior, which is unsatisfactory for the real-time motion planning of manipulators. In this paper, a novel inverse kinematics algorithm that combines FABRIK and the sequential quadratic programming (SQP) algorithm is presented, in which the joint angles deduced by FABRIK will be taken as the initial seed of the SQP algorithm to avoid getting stuck in local minima. The combined algorithm is evaluated with experiments, in which our algorithm can achieve higher success rates and faster solution times than FABRIK under the high error constraint. Furthermore, the combined algorithm can generate continuous trajectories for the UR5 and KUKA LBR IIWA 14 R820 manipulators in path tracking with no pose error and permitted position error of the end-effector.",https://arxiv.org/abs/2209.02533
Erato: Cooperative Data Story Editing via Fact Interpolation,"MengdiSun, LiganCai, WeiweiCui, YanqiuWu, YangShi, NanCao",06-sep-22,Human-Computer Interaction (cs.HC)," As an effective form of narrative visualization, visual data stories are widely used in data-driven storytelling to communicate complex insights and support data understanding. Although important, they are difficult to create, as a variety of interdisciplinary skills, such as data analysis and design, are required. In this work, we introduce Erato, a human-machine cooperative data story editing system, which allows users to generate insightful and fluent data stories together with the computer. Specifically, Erato only requires a number of keyframes provided by the user to briefly describe the topic and structure of a data story. Meanwhile, our system leverages a novel interpolation algorithm to help users insert intermediate frames between the keyframes to smooth the transition. We evaluated the effectiveness and usefulness of the Erato system via a series of evaluations including a Turing test, a controlled user study, a performance validation, and interviews with three expert users. The evaluation results showed that the proposed interpolation technique was able to generate coherent story content and help users create data stories more efficiently.",https://arxiv.org/abs/2209.02532
Rethinking Symmetric Matrix Factorization: A More General and BetterClustering Perspective,"MengyuanZhang, KaiLiu",06-sep-22,Machine Learning (cs.LG)," Nonnegative matrix factorization (NMF) is widely used for clustering with strong interpretability. Among general NMF problems, symmetric NMF is a special one which plays an important role for graph clustering where each element measures the similarity between data points. Most existing symmetric NMF algorithms require factor matrices to be nonnegative, and only focus on minimizing the gap between the original matrix and its approximation for clustering, without giving a consideration to other potential regularization terms which can yield better clustering. In this paper, we explore to factorize a symmetric matrix that does not have to be nonnegative, presenting an efficient factorization algorithm with a regularization term to boost the clustering performance. Moreover, a more generalized framework is proposed to solve symmetric matrix factorization problems with different constraints on the factor matrices.",https://arxiv.org/abs/2209.02529
UPAR: Unified Pedestrian Attribute Recognition and Person Retrieval,"AndreasSpecker, MickaelCormier, JÃ¼rgenBeyerer",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Recognizing soft-biometric pedestrian attributes is essential in video surveillance and fashion retrieval. Recent works show promising results on single datasets. Nevertheless, the generalization ability of these methods under different attribute distributions, viewpoints, varying illumination, and low resolutions remains rarely understood due to strong biases and varying attributes in current datasets. To close this gap and support a systematic investigation, we present UPAR, the Unified Person Attribute Recognition Dataset. It is based on four well-known person attribute recognition datasets: PA100K, PETA, RAPv2, and Market1501. We unify those datasets by providing 3,3M additional annotations to harmonize 40 important binary attributes over 12 attribute categories across the datasets. We thus enable research on generalizable pedestrian attribute recognition as well as attribute-based person retrieval for the first time. Due to the vast variance of the image distribution, pedestrian pose, scale, and occlusion, existing approaches are greatly challenged both in terms of accuracy and efficiency. Furthermore, we develop a strong baseline for PAR and attribute-based person retrieval based on a thorough analysis of regularization methods. Our models achieve state-of-the-art performance in cross-domain and specialization settings on PA100k, PETA, RAPv2, Market1501-Attributes, and UPAR. We believe UPAR and our strong baseline will contribute to the artificial intelligence community and promote research on large-scale, generalizable attribute recognition systems.",https://arxiv.org/abs/2209.02528
Sequential Cross Attention Based Multi-task Learning,"SunkyungKim, HyesongChoi, DongboMin",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In multi-task learning (MTL) for visual scene understanding, it is crucial to transfer useful information between multiple tasks with minimal interferences. In this paper, we propose a novel architecture that effectively transfers informative features by applying the attention mechanism to the multi-scale features of the tasks. Since applying the attention module directly to all possible features in terms of scale and task requires a high complexity, we propose to apply the attention module sequentially for the task and scale. The cross-task attention module (CTAM) is first applied to facilitate the exchange of relevant information between the multiple task features of the same scale. The cross-scale attention module (CSAM) then aggregates useful information from feature maps at different resolutions in the same task. Also, we attempt to capture long range dependencies through the self-attention module in the feature extraction network. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the NYUD-v2 and PASCAL-Context dataset.",https://arxiv.org/abs/2209.02522
Learned Distributed Image Compression with Multi-Scale Patch Matchingin Feature Domai,"YujunHuang, BinChen, ShiyuQin, JiaweiLi, YaoweiWang, TaoDai, Shu-TaoXia",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Beyond achieving higher compression efficiency over classical image compression codecs, deep image compression is expected to be improved with additional side information, e.g., another image from a different perspective of the same scene. To better utilize the side information under the distributed compression scenario, the existing method (Ayzik and Avidan 2020) only implements patch matching at the image domain to solve the parallax problem caused by the difference in viewing points. However, the patch matching at the image domain is not robust to the variance of scale, shape, and illumination caused by the different viewing angles, and can not make full use of the rich texture information of the side information image. To resolve this issue, we propose Multi-Scale Feature Domain Patch Matching (MSFDPM) to fully utilizes side information at the decoder of the distributed image compression model. Specifically, MSFDPM consists of a side information feature extractor, a multi-scale feature domain patch matching module, and a multi-scale feature fusion network. Furthermore, we reuse inter-patch correlation from the shallow layer to accelerate the patch matching of the deep layer. Finally, we nd that our patch matching in a multi-scale feature domain further improves compression rate by about 20% compared with the patch matching method at image domain (Ayzik and Avidan 2020).",https://arxiv.org/abs/2209.02518
Semi-Supervised Clustering via Dynamic Graph Structure Learning,"HuamingLing, Chenglong Bao, Xin Liang, Zuoqiang Shi",06-sep-22,Machine Learning (cs.LG)," Most existing semi-supervised graph-based clustering methods exploit the supervisory information by either refining the affinity matrix or directly constraining the low-dimensional representations of data points. The affinity matrix represents the graph structure and is vital to the performance of semi-supervised graph-based clustering. However, existing methods adopt a static affinity matrix to learn the low-dimensional representations of data points and do not optimize the affinity matrix during the learning process. In this paper, we propose a novel dynamic graph structure learning method for semi-supervised clustering. In this method, we simultaneously optimize the affinity matrix and the low-dimensional representations of data points by leveraging the given pairwise constraints. Moreover, we propose an alternating minimization approach with proven convergence to solve the proposed nonconvex model. During the iteration process, our method cyclically updates the low-dimensional representations of data points and refines the affinity matrix, leading to a dynamic affinity matrix (graph structure). Specifically, for the update of the affinity matrix, we enforce the data points with remarkably different low- dimensional representations to have an affinity value of 0. Furthermore, we construct the initial affinity matrix by integrating the local distance and global self-representation among data points. Experimental results on eight benchmark datasets under different settings show the advantages of the proposed approach.",https://arxiv.org/abs/2209.02514
"""Mama Always Had a Way of Explaining Things So I Could Understand'': ADialogue Corpus for Learning to Construct Explanations","HenningWachsmuth, MiladAlshomary",06-sep-22,Computation and Language (cs.CL)," As AI is more and more pervasive in everyday life, humans have an increasing demand to understand its behavior and decisions. Most research on explainable AI builds on the premise that there is one ideal explanation to be found. In fact, however, everyday explanations are co-constructed in a dialogue between the person explaining (the explainer) and the specific person being explained to (the explainee). In this paper, we introduce a first corpus of dialogical explanations to enable NLP research on how humans explain as well as on how AI can learn to imitate this process. The corpus consists of 65 transcribed English dialogues from the Wired video series \emph{5 Levels}, explaining 13 topics to five explainees of different proficiency. All 1550 dialogue turns have been manually labeled by five independent professionals for the topic discussed as well as for the dialogue act and the explanation move performed. We analyze linguistic patterns of explainers and explainees, and we explore differences across proficiency levels. BERT-based baseline results indicate that sequence information helps predicting topics, acts, and moves effectively",https://arxiv.org/abs/2209.02513
Passive Realizations of Series Elastic Actuation: Passivity andEffects of Plant and Controller Dynamics on Haptic Rendering Performance,"Celal UmutKenanoglu, VolkanPatoglu",06-sep-22,Robotics (cs.RO)," We establish the necessary and sufficient conditions for the frequency domain passivity of series (damped) elastic actuation (S(D)EA) while rendering null impedance and ideal springs under velocity sourced impedance control (VSIC). We introduce passive physical equivalents for S(D)EA under closed-loop control to help establish an intuitive understanding of the passivity bounds and to highlight the effect of different plant parameters and controller gains on the closed-loop performance of the system. Through the passive physical equivalents, we rigorously compare the effect of different plants dynamics (e.g., SEA and SDEA) and different cascaded controller architectures (e.g., P-P and P-PI) on the system performance. We show that passive physical equivalents establish a natural means for effective impedance analysis. We provide experimental validations of our theoretical results and evaluate the haptic rendering performance of S(D)EA under VSIC.",https://arxiv.org/abs/2209.02508
CausalRCA: Causal Inference based Precise Fine-grained Root CauseLocalization for Microservice Applications,"RuyueXin, PengChen, ZhimingZhao",06-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," For microservice applications with detected performance anomalies, localizing root causes based on monitoring data is important to enabling rapid recovery and loss mitigation. Existing research mainly focuses on coarse-grained faulty service localization. However, the fine-grained root cause localization to identify not only faulty service but also the root cause metric in the service is more helpful for operators to fix application anomalies, which is also more challenging. Recently, causal inference (CI) based methods is becoming popular but currently used CI methods have limitations, such as linear causal relations assumption. Therefore, this paper provides a framework named CausalRCA to implement fine-grained, automated, and real-time root cause localization. The CausalRCA works with a gradient-based causal structure learning method to generate weighted causal graphs and a root cause inference method to localize root cause metrics. We conduct coarse-grained and fine-grained root cause localization to validate the localization performance of CausalRCA. Experimental results show that CausalRCA performs best localization accuracy compared with baseline methods, e.g., the average $AC@3$ of the fine-grained root cause metric localization in the faulty service is 0.719, and the average improvement is 17\% compared with baseline methods.",https://arxiv.org/abs/2209.02505
Transfer Learning of Lexical Semantic Families for ArgumentativeDiscourse Units Identification,"JoÃ£oRodrigues, RubenBranco, AntÃ³nioBranco",06-sep-22,Computation and Language (cs.CL), Argument mining tasks require an informed range of low to high complexity linguistic phenomena and commonsense knowledge. Previous work has shown that pre-trained language models are highly effective at encoding syntactic and semantic linguistic phenomena when applied with transfer learning techniques and built on different pre-training objectives. It remains an issue of how much the existing pre-trained language models encompass the complexity of argument mining tasks. We rely on experimentation to shed light on how language models obtained from different lexical semantic families leverage the performance of the identification of argumentative discourse units task. Experimental results show that transfer learning techniques are beneficial to the task and that current methods may be insufficient to leverage commonsense knowledge from different lexical semantic families.,https://arxiv.org/abs/2209.02500
Surya Namaskar: real-time advanced yoga pose recognition andcorrection for smart healthcare,"AbhishekSharma, PranjalSharma, DarshanPincha, Prateek Jain",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Nowadays, yoga has gained worldwide attention because of increasing levels of stress in the modern way of life, and there are many ways or resources to learn yoga. The word yoga means a deep connection between the mind and body. Today there is substantial Medical and scientific evidence to show that the very fundamentals of the activity of our brain, our chemistry even our genetic content can be changed by practicing different systems of yoga. Suryanamaskar, also known as salute to the sun, is a yoga practice that combines eight different forms and 12 asanas(4 asana get repeated) devoted to the Hindu Sun God, Surya. Suryanamaskar offers a number of health benefits such as strengthening muscles and helping to control blood sugar levels. Here the Mediapipe Library is used to analyze Surya namaskar situations. Standing is detected in real time with advanced software, as one performs Surya namaskar in front of the camera. The class divider identifies the form as one of the following: Pranamasana, Hasta Padasana, Hasta Uttanasana, Ashwa - Sanchalan asana, Ashtanga Namaskar, Dandasana, or Bhujangasana and Svanasana. Deep learning-based techniques(CNN) are used to develop this model with model accuracy of 98.68 percent and an accuracy score of 0.75 to detect correct yoga (Surya Namaskar ) posture. With this method, the users can practice the desired pose and can check if the pose that the person is doing is correct or not. It will help in doing all the different poses of surya namaskar correctly and increase the efficiency of the yoga practitioner. This paper describes the whole framework which is to be implemented in the model.",https://arxiv.org/abs/2209.02495
Reconstructing Action-Conditioned Human-Object Interactions UsingCommonsense Knowledge Priors,"XiWang, GenLi, Yen-LingKuo, MuhammedKocabas, Emre Aksan, OtmarHilliges",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We present a method for inferring diverse 3D models of human- object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.",https://arxiv.org/abs/2209.02492
Segment Augmentation and Differentiable Ranking for Logo Retrieval,"FeyzaYavuz, SinanKalkan",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Logo retrieval is a challenging problem since the definition of similarity is more subjective compared to image retrieval tasks and the set of known similarities is very scarce. To tackle this challenge, in this paper, we propose a simple but effective segment-based augmentation strategy to introduce artificially similar logos for training deep networks for logo retrieval. In this novel augmentation strategy, we first find segments in a logo and apply transformations such as rotation, scaling, and color change, on the segments, unlike the conventional image-level augmentation strategies. Moreover, we evaluate whether the recently introduced ranking- based loss function, Smooth-AP, is a better approach for learning similarity for logo retrieval. On the large scale METU Trademark Dataset, we show that (i) our segment-based augmentation strategy improves retrieval performance compared to the baseline model or image-level augmentation strategies, and (ii) Smooth-AP indeed performs better than conventional losses for logo retrieval.",https://arxiv.org/abs/2209.02485
Mimose: An Input-Aware Checkpointing Planner for Efficient Training onGPU,"JianjinLiao, Mingzhen Li, Qingxiao Sun, Jiwei Hao, Fengwei Yu, ShengdongChen, YeTao, ZichengZhang, Hailong Yang, ZhongzhiLuan, DepeiQian",06-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," Larger deep learning models usually lead to higher model quality with an ever-increasing GPU memory footprint. Although tensor checkpointing techniques have been proposed to enable training under a restricted GPU memory budget, the input tensor dynamics have been unexploited for optimizing performance while reducing GPU memory footprint. Specifically, due to the diverse datasets and subsequent data argumentation, the input tensor size per mini-batch is dynamic during the training process, leading to a changing GPU memory footprint. However, to leverage such input tensor dynamics in checkpointing, there are two challenges to be solved. First, the checkpointing plan needs to be determined during runtime due to the dynamics of input tensors. Second, the checkpointing plan needs to be applied on the fly without significantly deteriorating the performance.   In this paper, we propose Mimose, an input-aware tensor checkpointing planner respecting the memory budget while enabling efficient model training on GPU. Mimose builds a lightweight but accurate prediction model of GPU memory usage online, without pre-analyzing the model. It generates a tensor checkpointing plan based on per-layer memory prediction and applies it to training progress on the fly. It also adopts a caching strategy to avoid having to regenerate the plan for repeated input size. Our experiments show that Mimose achieves superior training throughput compared to state-of-the- art memory planners under the same GPU memory budgets.",https://arxiv.org/abs/2209.02482
Comparing Methods for Extractive Summarization of Call Centre Dialogue,"Alexandra N.Uma, DmitrySityaev",06-sep-22,Computation and Language (cs.CL)," This paper provides results of evaluating some text summarisation techniques for the purpose of producing call summaries for contact centre solutions. We specifically focus on extractive summarisation methods, as they do not require any labelled data and are fairly quick and easy to implement for production use. We experimentally compare several such methods by using them to produce summaries of calls, and evaluating these summaries objectively (using ROUGE-L) and subjectively (by aggregating the judgements of several annotators). We found that TopicSum and Lead-N outperform the other summarisation methods, whilst BERTSum received comparatively lower scores in both subjective and objective evaluations. The results demonstrate that even such simple heuristics-based methods like Lead-N ca n produce meaningful and useful summaries of call centre dialogues.",https://arxiv.org/abs/2209.02478
"Circumventing volumetric locking in explicit material point methods: Asimple, efficient, and general approach","YidongZhao, ChenfanfuJiang, JinhyunChoo",06-sep-22,Numerical Analysis (math.NA)," The material point method (MPM) is frequently used to simulate large deformations of nearly incompressible materials such as water, rubber, and undrained porous media. However, MPM solutions to nearly incompressible materials are susceptible to volumetric locking, that is, overly stiff behavior with erroneous strain and stress fields. While several approaches have been devised to mitigate volumetric locking in MPM, none of them lends itself to a straightforward application to standard explicit MPM formulations. In this work, we propose a locking-mitigation approach that features an unprecedented combination of simplicity, efficacy, and generality for a family of explicit MPM formulations. The approach combines the assumed deformation gradient ($\bar{\boldsymbol{F}}$) method with a volume-averaging operation built on standard particle-grid transfer schemes in MPM. Upon explicit time integration, this combination yields a new and simple algorithm for updating the deformation gradient, preserving all other MPM procedures. The proposed approach is thus easy to implement, low-cost, and compatible with the existing machinery in MPM. Through various types of nearly incompressible problems in solid and fluid mechanics, we verify that the proposed approach efficiently circumvents volumetric locking in explicit MPM, regardless of the basis functions and material types.",https://arxiv.org/abs/2209.02472
Monolingual alignment of word senses and definitions inlexicographical resources,SinaAhmadi,06-sep-22,Computation and Language (cs.CL)," The focus of this thesis is broadly on the alignment of lexicographical data, particularly dictionaries. In order to tackle some of the challenges in this field, two main tasks of word sense alignment and translation inference are addressed. The first task aims to find an optimal alignment given the sense definitions of a headword in two different monolingual dictionaries. This is a challenging task, especially due to differences in sense granularity, coverage and description in two resources. After describing the characteristics of various lexical semantic resources, we introduce a benchmark containing 17 datasets of 15 languages where monolingual word senses and definitions are manually annotated across different resources by experts. In the creation of the benchmark, lexicographers' knowledge is incorporated through the annotations where a semantic relation, namely exact, narrower, broader, related or none, is selected for each sense pair. This benchmark can be used for evaluation purposes of word-sense alignment systems. The performance of a few alignment techniques based on textual and non-textual semantic similarity detection and semantic relation induction is evaluated using the benchmark. Finally, we extend this work to translation inference where translation pairs are induced to generate bilingual lexicons in an unsupervised way using various approaches based on graph analysis. This task is of particular interest for the creation of lexicographical resources for less-resourced and under- represented languages and also, assists in increasing coverage of the existing resources. From a practical point of view, the techniques and methods that are developed in this thesis are implemented within a tool that can facilitate the alignment task.",https://arxiv.org/abs/2209.02466
Finite-Cliquewidth Sets of Existential Rules: Toward a GeneralCriterion for Decidable yet Highly Expressive Querying,"ThomasFeller, TimS. Lyon, Piotr Ostropolski-Nalewaja, SebastianRudolph",06-sep-22,Logic in Computer Science (cs.LO)," In our pursuit of generic criteria for decidable ontology-based querying, we introduce 'finite-cliquewidth sets' (FCS) of existential rules, a model-theoretically defined class of rule sets, inspired by the cliquewidth measure from graph theory. By a generic argument, we show that FCS ensures decidability of entailment for a sizable class of queries (dubbed 'DaMSOQs') subsuming conjunctive queries (CQs). The FCS class properly generalizes the class of finite-expansion sets (FES), and for signatures of arity at most 2, the class of bounded-treewidth sets (BTS). For higher arities, BTS is only indirectly subsumed by FCS by means of reification. Despite the generality of FCS, we provide a rule set with decidable CQ entailment (by virtue of first-order-rewritability) that falls outside FCS, thus demonstrating the incomparability of FCS and the class of finite-unification sets (FUS). In spite of this, we show that if we restrict ourselves to single-headed rule sets over signatures of arity at most 2, then FCS subsumes FUS.",https://arxiv.org/abs/2209.02465
Rethinking The Memory Staleness Problem In Dynamics GNN,"MorVentura, Hadas BenAtya, DekelBrav",06-sep-22,Social and Information Networks (cs.SI)," The staleness problem is a well-known problem when working with dynamic data, due to the absence of events for a long time. Since the memory of the node is updated only when the node is involved in an event, its memory becomes stale. Usually, it refers to a lack of events such as a temporal deactivation of a social account. To overcome the memory staleness problem aggregate information from the nodes neighbors memory in addition to the nodes memory. Inspired by that, we design an updated embedding module that inserts the most similar node in addition to the nodes neighbors. Our method achieved similar results to the TGN, with a slight improvement. This could indicate a potential improvement after fine-tuning our hyper- parameters, especially the time threshold, and using a learnable similarity metric.",https://arxiv.org/abs/2209.02464
ABS+ Polar Codes: Exploiting More Linear Transforms on Adjacent Bits,"GuodongLi, MinYe, SihuangHu",06-sep-22,Information Theory (cs.IT)," ABS polar codes were recently proposed to speed up polarization by swapping certain pairs of adjacent bits after each layer of polar transform. In this paper, we observe that applying the Arikan transform $(U_i, U_{i+1}) \mapsto (U_{i}+U_{i+1}, U_{i+1})$ on certain pairs of adjacent bits after each polar transform layer leads to even faster polarization.   In light of this, we propose ABS+ polar codes which incorporate the Arikan transform in addition to the swapping transform in ABS polar codes. In order to efficiently construct and decode ABS+ polar codes, we derive a new recursive relation between the joint distributions of adjacent bits through different layers of polar transforms. Simulation results over a wide range of parameters show that the CRC-aided SCL decoder of ABS+ polar codes improves upon that of ABS polar codes by 0.1dB--0.25dB while maintaining the same decoding time. Moreover, ABS+ polar codes improve upon standard polar codes by 0.2dB--0.45dB when they both use the CRC-aided SCL decoder with list size $32$. The implementations of all the algorithms in this paper are available at [this https URL](https://github.com/PlumJelly/ABS-Polar)",https://arxiv.org/abs/2209.02462
Adjacent-Bits-Swapped Polar codes: A new code construction to speed uppolarization,"GuodongLi, MinYe, SihuangHu","9 Feb 2022 (v1(https://arxiv.org/abs/2202.04454v1)), lastrevised 29 Aug 2022 (this version, v2)",Information Theory (cs.IT)," The construction of polar codes with code length $n=2^m$ involves $m$ layers of polar transforms. In this paper, we observe that after each layer of polar transforms, one can swap certain pairs of adjacent bits to accelerate the polarization process. More precisely, if the previous bit is more reliable than its next bit under the successive decoder, then switching the decoding order of these two adjacent bits will make the reliable bit even more reliable and the noisy bit even noisier.   Based on this observation, we propose a new family of codes called the Adjacent-Bits-Swapped (ABS) polar codes. We add a permutation layer after each polar transform layer in the construction of the ABS polar codes. In order to choose which pairs of adjacent bits to swap in the permutation layers, we rely on a new polar transform that combines two independent channels with $4$-ary inputs. This new polar transform allows us to track the evolution of every pair of adjacent bits through different layers of polar transforms, and it also plays an essential role in the Successive Cancellation List (SCL) decoder for the ABS polar codes. Extensive simulation results show that ABS polar codes consistently outperform standard polar codes by 0.15dB--0.3dB when we use CRC-aided SCL decoder with list size $32$ for both codes. The implementations of all the algorithms in this paper are available at [this https URL](https://github.com/PlumJelly/ABS-Polar)",https://arxiv.org/abs/2209.02461
Robust and Efficient Imbalanced Positive-Unlabeled Learning with Self-supervision,"EmilioDorigatti, JonasSchweisthal, BerndBischl, Mina Rezaei",06-sep-22,Machine Learning (cs.LG)," Learning from positive and unlabeled (PU) data is a setting where the learner only has access to positive and unlabeled samples while having no information on negative examples. Such PU setting is of great importance in various tasks such as medical diagnosis, social network analysis, financial markets analysis, and knowledge base completion, which also tend to be intrinsically imbalanced, i.e., where most examples are actually negatives. Most existing approaches for PU learning, however, only consider artificially balanced datasets and it is unclear how well they perform in the realistic scenario of imbalanced and long-tail data distribution. This paper proposes to tackle this challenge via robust and efficient self- supervised pretraining. However, training conventional self-supervised learning methods when applied with highly imbalanced PU distribution needs better reformulation. In this paper, we present \textit{ImPULSeS}, a unified representation learning framework for \underline{Im}balanced \underline{P}ositive \underline{U}nlabeled \underline{L}earning leveraging \underline{Se}lf-\underline{S}upervised debiase pre-training. ImPULSeS uses a generic combination of large-scale unsupervised learning with debiased contrastive loss and additional reweighted PU loss. We performed different experiments across multiple datasets to show that ImPULSeS is able to halve the error rate of the previous state-of-the-art, even compared with previous methods that are given the true prior. Moreover, our method showed increased robustness to prior misspecification and superior performance even when pretraining was performed on an unrelated dataset. We anticipate such robustness and efficiency will make it much easier for practitioners to obtain excellent results on other PU datasets of interest. The source code is available at \url{[this https URL](https://github.com/JSchweisthal/ImPULSeS)}",https://arxiv.org/abs/2202.04454
Extending the Universal Approximation Theorem for a Broad Class ofHypercomplex-Valued Neural Networks,"Wington L.Vital, GuilhermeVieira, Marcos EduardoValle",06-sep-22,Machine Learning (cs.LG)," The universal approximation theorem asserts that a single hidden layer neural network approximates continuous functions with any desired precision on compact sets. As an existential result, the universal approximation theorem supports the use of neural networks for various applications, including regression and classification tasks. The universal approximation theorem is not limited to real-valued neural networks but also holds for complex, quaternion, tessarines, and Clifford-valued neural networks. This paper extends the universal approximation theorem for a broad class of hypercomplex-valued neural networks. Precisely, we first introduce the concept of non-degenerate hypercomplex algebra. Complex numbers, quaternions, and tessarines are examples of non-degenerate hypercomplex algebras. Then, we state the universal approximation theorem for hypercomplex-valued neural networks defined on a non-degenerate algebra.",https://arxiv.org/abs/2209.02459
Optimal design of photonic nanojets under uncertainty,"Amal Mohammed AAlghamdi, Peng Chen, MirzaKaramehmedoviÄ‡",06-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," Photonic nanojets (PNJs) have promising applications as optical probes in super-resolution optical microscopy, Raman microscopy, as well as fluorescence microscopy. In this work, we consider optimal design of PNJs using a heterogeneous lens refractive index with a fixed lens geometry and uniform plane wave illumination. In particular, we consider the presence of manufacturing error of heterogeneous lens, and propose a computational framework of Optimization Under Uncertainty (OUU) for robust optimal design of PNJ. We formulate a risk-averse stochastic optimization problem with the objective to minimize both the mean and the variance of a target function, which is constrained by the Helmholtz equation that governs the 2D transverse electric (2D TE) electromagnetic field in a neighborhood of the lens. The design variable is taken as a spatially-varying field variable, where we use a finite element method for its discretization, impose a total variation penalty to promote its sparsity, and employ an adjoint-based BFGS method to solve the resulting high-dimensional optimization problem. We demonstrate that our proposed OUU computational framework can achieve more robust optimal design than a deterministic optimization scheme to significantly mitigate the impact of manufacturing uncertainty.",https://arxiv.org/abs/2209.02456
Instance Attack:An Explanation-based Vulnerability Analysis FrameworkAgainst DNNs for Malware Detection,"SunRuiJin, GuoShiZe, GuoJinHong, XingChangYou, YangLuMing, GuoXi, PanZhiSong",06-sep-22,Cryptography and Security (cs.CR)," Deep neural networks (DNNs) are increasingly being applied in malware detection and their robustness has been widely debated. Traditionally an adversarial example generation scheme relies on either detailed model information (gradient-based methods) or lots of samples to train a surrogate model, neither of which are available in most scenarios.   We propose the notion of the instance-based attack. Our scheme is interpretable and can work in a black-box environment. Given a specific binary example and a malware classifier, we use the data augmentation strategies to produce enough data from which we can train a simple interpretable model. We explain the detection model by displaying the weight of different parts of the specific binary. By analyzing the explanations, we found that the data subsections play an important role in Windows PE malware detection. We proposed a new function preserving transformation algorithm that can be applied to data subsections. By employing the binary- diversification techniques that we proposed, we eliminated the influence of the most weighted part to generate adversarial examples. Our algorithm can fool the DNNs in certain cases with a success rate of nearly 100\%. Our method outperforms the state-of-the-art method . The most important aspect is that our method operates in black-box settings and the results can be validated with domain knowledge. Our analysis model can assist people in improving the robustness of malware detectors.",https://arxiv.org/abs/2209.02454
Efficient quantum non-fungible tokens for blockchain,"Subhash ShankarPandey, Tadasha Dash, Prasanta K.Panigrahi, AhmedFarouk",02-sep-22,Cryptography and Security (cs.CR)," Blockchain is a decentralized system that allows transaction transmission and storage according to the roles of the Consensus algorithm and Smart contracts. Non-fungible tokens (NFTs) consolidate the best characteristics of blockchain technology to deliver unique and bona fide tokens, each with distinctive attributes with non-fungible resources. Unfortunately, current classical NFTs are suffering from high costs regarding the consumed power of mining and lack of security. Therefore, this paper presents a new protocol for preparing quantum non-fungible tokens where a quantum state representing NFT is mounted on a blockchain instead of physically giving it to the owner. The proposed scheme is simulated and analyzed against various attacks and proves its ability to secure against them. Furthermore, the presented protocol provides reliable and cheaper NFTs than the classical one.",https://arxiv.org/abs/2209.02453
Fast Adaptive Regression-based Model Predictive Control,"EslamMostafa, AhmedElliethy, Hussein A.Aly",06-sep-22,Systems and Control (eess.SY)," Model predictive control (MPC) is an optimal control method that predicts the future states of the system being controlled and estimates the optimal control inputs that drive the predicted states to the required reference. The computations of the MPC are performed at pre-determined sample instances over a finite time horizon. The number of sample instances and the horizon length determine the performance of the MPC and its computational cost. A long horizon with a large sample count allows the MPC to better estimate the inputs when the states have rapid changes over time, which results in better performance but at the expense of high computational cost. However, this long horizon is not always necessary, especially for slowly-varying states. In this case, a short horizon with less sample count is preferable as the same MPC performance can be obtained but at a fraction of the computational cost. In this paper, we propose an adaptive regression- based MPC that predicts the best minimum horizon length and the sample count from several features extracted from the time-varying changes of the states. The proposed technique builds a synthetic dataset using the system model and utilizes the dataset to train a support vector regressor that performs the prediction. The proposed technique is experimentally compared with several state-of-the-art techniques on both linear and non-linear models. The proposed technique shows a superior reduction in computational time with a reduction of about 35-65\% compared with the other techniques without introducing a noticeable loss in performance.",https://arxiv.org/abs/2209.02449
Web3 Challenges and Opportunities for the Market,"DanSheridan, JamesHarris, FrankWear, Jerry CowellJr, Easton Wong, AbbasYazdinejad",06-sep-22,Computers and Society (cs.CY)," The inability of a computer to think has been a limiter in its usefulness and a point of reassurance for humanity since the first computers were created. The semantic web is the first step toward removing that barrier, enabling computers to operate based on conceptual understanding, and AI and ML are the second. Both semantic knowledge and the ability to learn are fundamental to web3, as are blockchain, decentralization, transactional transparency, and ownership. Web3 is the next generational step in the information age, where the web evolves into a more digestible medium for users and machines to browse knowledge. The slow introduction of Web3 across the global software ecosystem will impact the people who enable the current iteration. This evolution of the internet space will expand the way knowledge is shared, consumed, and owned, which will lessen the requirement for a global standard and allow data to interact efficiently, no matter the construction of the knowledge. The heart of this paper understands the: 1) Enablement of Web3 across the digital ecosystem. 2) What a Web3 developer will look like. 3) How this alteration will evolve the market around software and knowledge in general.",https://arxiv.org/abs/2209.02448
Fun2Vec:a Contrastive Learning Framework of Function-levelRepresentation for Binary,"SunRuiJin, GuoShiZe, GuoJinHong, Sun Meng, PanZhiSong",06-sep-22,Cryptography and Security (cs.CR)," Function-level binary code similarity detection is essential in the field of cyberspace security. It helps us find bugs and detect patent infringements in released software and plays a key role in the prevention of supply chain attacks. A practical embedding learning framework relies on the robustness of vector representation system of assembly code and the accuracy of the annotation of function pairs. Supervised learning based methods are traditionally emploied. But annotating different function pairs with accurate labels is very difficult. These supervised learning methods are easily overtrained and suffer from vector robustness issues. To mitigate these problems, we propose Fun2Vec: a contrastive learning framework of function-level representation for binary. We take an unsupervised learning approach and formulate the binary code similarity detection as instance discrimination. Fun2Vec works directly on disassembled binary functions, and could be implemented with any encoder. It does not require manual labeled similar or dissimilar information. We use the compiler optimization options and code obfuscation techniques to generate augmented data. Our experimental results demonstrate that our method surpasses the state-of-the-art in accuracy and have great advantage in few-shot settings.",https://arxiv.org/abs/2209.02446
Threat Detection In Self-Driving Vehicles Using Computer Vision,"UmangGoenka, AaryanJagetia, Param Patil, AkshaySingh, TareshSharma, Poonam Saini",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," On-road obstacle detection is an important field of research that falls in the scope of intelligent transportation infrastructure systems. The use of vision-based approaches results in an accurate and cost-effective solution to such systems. In this research paper, we propose a threat detection mechanism for autonomous self-driving cars using dashcam videos to ensure the presence of any unwanted obstacle on the road that falls within its visual range. This information can assist the vehicle's program to en route safely. There are four major components, namely, YOLO to identify the objects, advanced lane detection algorithm, multi regression model to measure the distance of the object from the camera, the two-second rule for measuring the safety, and limiting speed. In addition, we have used the Car Crash Dataset(CCD) for calculating the accuracy of the model. The YOLO algorithm gives an accuracy of around 93%. The final accuracy of our proposed Threat Detection Model (TDM) is 82.65%.",https://arxiv.org/abs/2209.02442
ViTKD: Practical Guidelines for ViT feature knowledge distillation,"ZhendongYang, ZheLi, AilingZeng, ZexianLi, ChunYuan, YuLi",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Knowledge Distillation (KD) for Convolutional Neural Network (CNN) is extensively studied as a way to boost the performance of a small model. Recently, Vision Transformer (ViT) has achieved great success on many computer vision tasks and KD for ViT is also desired. However, besides the output logit-based KD, other feature-based KD methods for CNNs cannot be directly applied to ViT due to the huge structure gap. In this paper, we explore the way of feature-based distillation for ViT. Based on the nature of feature maps in ViT, we design a series of controlled experiments and derive three practical guidelines for ViT's feature distillation. Some of our findings are even opposite to the practices in the CNN era. Based on the three guidelines, we propose our feature-based method ViTKD which brings consistent and considerable improvement to the student. On ImageNet-1k, we boost DeiT-Tiny from 74.42% to 76.06%, DeiT-Small from 80.55% to 81.95%, and DeiT-Base from 81.76% to 83.46%. Moreover, ViTKD and the logit-based KD method are complementary and can be applied together directly. This combination can further improve the performance of the student. Specifically, the student DeiT-Tiny, Small, and Base achieve 77.78%, 83.59%, and 85.41%, respectively. The code is available at [this https URL](https://github.com/yzd-v/cls_KD).",https://arxiv.org/abs/2209.02438
DPIT: Dual-Pipeline Integrated Transformer for Human Pose Estimation,"ShuaitaoZhao, KunLiu, YuhangHuang, QianBao, DanZeng, WuLiu",02-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Human pose estimation aims to figure out the keypoints of all people in different scenes. Current approaches still face some challenges despite promising results. Existing top-down methods deal with a single person individually, without the interaction between different people and the scene they are situated in. Consequently, the performance of human detection degrades when serious occlusion happens. On the other hand, existing bottom-up methods consider all people at the same time and capture the global knowledge of the entire image. However, they are less accurate than the top-down methods due to the scale variation. To address these problems, we propose a novel Dual-Pipeline Integrated Transformer (DPIT) by integrating top-down and bottom-up pipelines to explore the visual clues of different receptive fields and achieve their complementarity. Specifically, DPIT consists of two branches, the bottom-up branch deals with the whole image to capture the global visual information, while the top-down branch extracts the feature representation of local vision from the single-human bounding box. Then, the extracted feature representations from bottom-up and top-down branches are fed into the transformer encoder to fuse the global and local knowledge interactively. Moreover, we define the keypoint queries to explore both full-scene and single-human posture visual clues to realize the mutual complementarity of the two pipelines. To the best of our knowledge, this is one of the first works to integrate the bottom-up and top-down pipelines with transformers for human pose estimation. Extensive experiments on COCO and MPII datasets demonstrate that our DPIT achieves comparable performance to the state-of-the-art methods.",https://arxiv.org/abs/2209.02432
Adversarial Color Film: Effective Physical-World Attack to DNNs,"ChengyinHu, WeiwenShi",02-sep-22,Computer Vision and Pattern Recognition (cs.CV)," It is well known that the performance of deep neural networks (DNNs) is susceptible to subtle interference. So far, camera-based physical adversarial attacks haven't gotten much attention, but it is the vacancy of physical attack. In this paper, we propose a simple and efficient camera- based physical attack called Adversarial Color Film (AdvCF), which manipulates the physical parameters of color film to perform attacks. Carefully designed experiments show the effectiveness of the proposed method in both digital and physical environments. In addition, experimental results show that the adversarial samples generated by AdvCF have excellent performance in attack transferability, which enables AdvCF effective black- box attacks. At the same time, we give the guidance of defense against AdvCF by means of adversarial training. Finally, we look into AdvCF's threat to future vision-based systems and propose some promising mentality for camera- based physical attacks.",https://arxiv.org/abs/2209.02431
Which country is this picture from? New data and methods for DNN-basedcountry recognition,"OmranAlamayreh, Giovanna MariaDimitri, Jun Wang, BenedettaTondi, MauroBarni",02-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Predicting the country where a picture has been taken from has many potential applications, like detection of false claims, impostors identification, prevention of disinformation campaigns, identification of fake news and so on. Previous works have focused mostly on the estimation of the geo-coordinates where a picture has been taken. Yet, recognizing the country where an image has been taken could potentially be more important, from a semantic and forensic point of view, than identifying its spatial coordinates. So far only a few works have addressed this task, mostly by relying on images containing characteristic landmarks, like iconic monuments. In the above framework, this paper provides two main contributions. First, we introduce a new dataset, the VIPPGeo dataset, containing almost 4 million images, that can be used to train DL models for country classification. The dataset contains only urban images given the relevance of this kind of image for country recognition, and it has been built by paying attention to removing non-significant images, like images portraying faces or specific, non-relevant objects, like airplanes or ships. Secondly, we used the dataset to train a deep learning architecture casting the country recognition problem as a classification problem. The experiments, we performed, show that our network provides significantly better results than current state of the art. In particular, we found that asking the network to directly identify the country provides better results than estimating the geo-coordinates first and then using them to trace back to the country where the picture was taken.",https://arxiv.org/abs/2209.02430
Predictive GAN-powered Multi-Objective Optimization for HybridFederated Split Learning,"BenshunYin, ZhiyongChen, MeixiaTao",02-sep-22,Machine Learning (cs.LG)," As an edge intelligence algorithm for multi-device collaborative training, federated learning (FL) can reduce the communication burden but increase the computing load of wireless devices. In contrast, split learning (SL) can reduce the computing load of devices by using model splitting and assignment, but increase the communication burden to transmit intermediate results. In this paper, to exploit the advantages of FL and SL, we propose a hybrid federated split learning (HFSL) framework in wireless networks, which combines the multi-worker parallel update of FL and flexible splitting of SL. To reduce the computational idleness in model splitting, we design a parallel computing scheme for model splitting without label sharing, and theoretically analyze the influence of the delayed gradient caused by the scheme on the convergence speed. Aiming to obtain the trade-off between the training time and energy consumption, we optimize the splitting decision, the bandwidth and computing resource allocation. The optimization problem is multi-objective, and we thus propose a predictive generative adversarial network (GAN)-powered multi-objective optimization algorithm to obtain the Pareto front of the problem. Experimental results show that the proposed algorithm outperforms others in finding Pareto optimal solutions, and the solutions of the proposed HFSL dominate the solution of FL.",https://arxiv.org/abs/2209.02429
Multi-Modal Experience Inspired AI Creation,"QianCao, XuChen, RuihuaSong, HaoJiang, GuangYang, ZhaoCao",02-sep-22,Artificial Intelligence (cs.AI)," AI creation, such as poem or lyrics generation, has attracted increasing attention from both industry and academic communities, with many promising models proposed in the past few years. Existing methods usually estimate the outputs based on single and independent visual or textual information. However, in reality, humans usually make creations according to their experiences, which may involve different modalities and be sequentially correlated. To model such human capabilities, in this paper, we define and solve a novel AI creation problem based on human experiences. More specifically, we study how to generate texts based on sequential multi- modal information. Compared with the previous works, this task is much more difficult because the designed model has to well understand and adapt the semantics among different modalities and effectively convert them into the output in a sequential manner. To alleviate these difficulties, we firstly design a multi-channel sequence-to-sequence architecture equipped with a multi-modal attention network. For more effective optimization, we then propose a curriculum negative sampling strategy tailored for the sequential inputs. To benchmark this problem and demonstrate the effectiveness of our model, we manually labeled a new multi-modal experience dataset. With this dataset, we conduct extensive experiments by comparing our model with a series of representative baselines, where we can demonstrate significant improvements in our model based on both automatic and human-centered metrics. The code and data are available at: \url{[this https URL](https://github.com/Aman-4-Real/MMTG)}.",https://arxiv.org/abs/2209.02428
Learning an Ensemble of Deep Fingerprint Representations,"AkashGodbole, KarthikNandakumar, Anil K.Jain",02-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Deep neural networks (DNNs) have shown incredible promise in learning fixed-length representations from fingerprints. Since the representation learning is often focused on capturing specific prior knowledge (e.g., minutiae), there is no universal representation that comprehensively encapsulates all the discriminatory information available in a fingerprint. While learning an ensemble of representations can mitigate this problem, two critical challenges need to be addressed: (i) How to extract multiple diverse representations from the same fingerprint image? and (ii) How to optimally exploit these representations during the matching process? In this work, we train multiple instances of DeepPrint (a state-of- the-art DNN-based fingerprint encoder) on different transformations of the input image to generate an ensemble of fingerprint embeddings. We also propose a feature fusion technique that distills these multiple representations into a single embedding, which faithfully captures the diversity present in the ensemble without increasing the computational complexity. The proposed approach has been comprehensively evaluated on five databases containing rolled, plain, and latent fingerprints (NIST SD4, NIST SD14, NIST SD27, NIST SD302, and FVC2004 DB2A) and statistically significant improvements in accuracy have been consistently demonstrated across a range of verification as well as closed- and open-set identification settings. The proposed approach serves as a wrapper capable of improving the accuracy of any DNN-based recognition system.",https://arxiv.org/abs/2209.02427
Jeopardy: An Invertible Functional Programming Language,"Joachim TilstedKristensen, RobinKaarsgaard, Michael KirkedalThomsen",06-sep-22,Programming Languages (cs.PL)," An algorithm describes a sequence of steps that transform a problem into its solution. Furthermore, when the inverted sequence is well- defined, we say that the algorithm is invertible.   While invertible algorithms can be described in general-purpose languages, no guarantees are generally made by such languages as regards invertibility, so ensuring invertibility requires additional (and often non-trivial) proof. On the other hand, while reversible programming languages guarantee that their programs are invertible by restricting the permissible operations to those which are locally invertible, writing programs in the reversible style can be cumbersome, and may differ significantly from conventional implementations even when the implemented algorithm is, in fact, invertible.   In this paper we introduce Jeopardy, a functional programming language that guarantees program invertibility without imposing local reversibility. In particular, Jeopardy allows the limited use of uninvertible -- and even nondeterministic! -- operations, provided that they are used in a way that can be statically determined to be invertible. However, guaranteeing invertibility is not obvious. Thus, we furthermore outline three approaches that can give a partial static guarantee.",https://arxiv.org/abs/2209.02425
Getting Users Smart Quick about Security: Results from 90 Minutes ofUsing a Persuasive Toolkit for Facilitating Information Security ProblemSolving by Non-Professionals,"MartinRuskov, PaulEkblom, M.AngelaSasse",06-sep-22,Computers and Society (cs.CY)," There is a conflict between the need for security compliance by users and the fact that commonly they cannot afford to dedicate much of their time and energy to that security. A balanced level of user engagement in security is difficult to achieve due to difference of priorities between the business perspective and the security perspective. We sought to find a way to engage users minimally, yet efficiently, so that they would both improve their security awareness and provide necessary feedback for improvement purposes to security designers. We have developed a persuasive software toolkit to engage users in structured discussions about security vulnerabilities in their company and potential interventions addressing these. In the toolkit we have adapted and integrated an established framework from conventional crime prevention. In the research reported here we examine how non-professionals perceived security problems through a short-term use of the toolkit. We present perceptions from a pilot lab study in which randomly recruited participants had to analyze a crafted insider threat problem using the toolkit. Results demonstrate that study participants were able to successfully identify causes, propose interventions and engage in providing feedback on proposed interventions. Subsequent interviews show that participants have developed greater awareness of information security issues and the framework to address these, which in a real setting would lead ultimately to significant benefits for the organization. These results indicate that when well-structured such short-term engagement is sufficient for users to meaningfully take part in complex security discussions and develop in-depth understanding of theoretical principles of security.",https://arxiv.org/abs/2209.02422
Mortaring for linear elasticity using low order finite elements,"TomGustafsson, PeterRÃ¥back, JuhaVideman",06-sep-22,Numerical Analysis (math.NA)," We introduce a stabilized mortar method for linear elasticity and compare it to the standard mixed mortar method without stabilization. We present the stability criteria of the lowest order mixed approximation and investigate its use for tie contact problems. Our numerical results demonstrate the stability and the convergence of the methods. Moreover, the results show that the low order mixed method can be successfully extended to three dimensions.",https://arxiv.org/abs/2209.02420
Volume Rendering Digest (for NeRF),"AndreaTagliasacchi, BenMildenhall",29 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," Neural Radiance Fields employ simple volume rendering as a way to overcome the challenges of differentiating through ray-triangle intersections by leveraging a probabilistic notion of visibility. This is achieved by assuming the scene is composed by a cloud of light-emitting particles whose density changes in space. This technical report summarizes the derivations for differentiable volume rendering. It is a condensed version of previous reports, but rewritten in the context of NeRF, and adopting its commonly used notation.",https://arxiv.org/abs/2209.02418
Automatic Infectious Disease Classification Analysis with ConceptDiscovery,"ElenaSizikova, JoshuaVendrow, Xu Cao, RachelGrotheer, JamieHaddock, LaraKassab, AlonaKryshchenko, ThomasMerkh, R. W.M. A.Madushani, Kenny Moise, AnnieUlichney, Huy V. Vo, ChuntianWang, MeganCoffee, KathrynLeonard, DeannaNeedell",28 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," Automatic infectious disease classification from images can facilitate needed medical diagnoses. Such an approach can identify diseases, like tuberculosis, which remain under-diagnosed due to resource constraints and also novel and emerging diseases, like monkeypox, which clinicians have little experience or acumen in diagnosing. Avoiding missed or delayed diagnoses would prevent further transmission and improve clinical outcomes. In order to understand and trust neural network predictions, analysis of learned representations is necessary. In this work, we argue that automatic discovery of concepts, i.e., human interpretable attributes, allows for a deep understanding of learned information in medical image analysis tasks, generalizing beyond the training labels or protocols. We provide an overview of existing concept discovery approaches in medical image and computer vision communities, and evaluate representative methods on tuberculosis (TB) prediction and monkeypox prediction tasks. Finally, we propose NMFx, a general NMF formulation of interpretability by concept discovery that works in a unified way in unsupervised, weakly supervised, and supervised scenarios.",https://arxiv.org/abs/2209.02417
From Smart Sensing to Consciousness: An info-structural model ofcomputational consciousness for non-interacting agents,"GerardoIovane, Riccardo EmanueleLandi",29 Aug 2022,Artificial Intelligence (cs.AI)," This study proposes a model of computational consciousness for non-interacting agents. The phenomenon of interest was assumed as sequentially dependent on the cognitive tasks of sensation, perception, emotion, affection, attention, awareness, and consciousness. Starting from the Smart Sensing prodromal study, the cognitive levels associated with the processes of attention, awareness, and consciousness were formally defined and tested together with the other processes concerning sensation, perception, emotion, and affection. The output of the model consists of an index that synthesizes the energetic and entropic contributions of consciousness from a computationally moral perspective. Attention was modeled through a bottom-up approach, while awareness and consciousness by distinguishing environment from subjective cognitive processes. By testing the solution on visual stimuli eliciting the emotions of happiness, anger, fear, surprise, contempt, sadness, disgust, and the neutral state, it was found that the proposed model is concordant with the scientific evidence concerning covert attention. Comparable results were also obtained regarding studies investigating awareness as a consequence of visual stimuli repetition, as well as those investigating moral judgments to visual stimuli eliciting disgust and sadness. The solution represents a novel approach for defining computational consciousness through artificial emotional activity and morality.",https://arxiv.org/abs/2209.02415
SIAN: Style-Guided Instance-Adaptive Normalization for Multi-OrganHistopathology Image Synthesis,"HaotianWang, MinXian, AleksandarVakanski, BryarShareef",02-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Existing deep networks for histopathology image synthesis cannot generate accurate boundaries for clustered nuclei and cannot output image styles that align with different organs. To address these issues, we propose a style-guided instance-adaptive normalization (SIAN) to synthesize realistic color distributions and textures for different organs. SIAN contains four phases, semantization, stylization, instantiation, and modulation. The four phases work together and are integrated into a generative network to embed image semantics, style, and instance-level boundaries. Experimental results demonstrate the effectiveness of all components in SIAN, and show that the proposed method outperforms the state- of-the-art conditional GANs for histopathology image synthesis using the Frechet Inception Distance (FID), structural similarity Index (SSIM), detection quality(DQ), segmentation quality(SQ), and panoptic quality(PQ). Furthermore, the performance of a segmentation network could be significantly improved by incorporating synthetic images generated using SIAN.",https://arxiv.org/abs/2209.02414
Robustness and invariance properties of image classifiers,ApostolosModas,30 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," Deep neural networks have achieved impressive results in many image classification tasks. However, since their performance is usually measured in controlled settings, it is important to ensure that their decisions remain correct when deployed in noisy environments. In fact, deep networks are not robust to a large variety of semantic-preserving image modifications, even to imperceptible image changes known as adversarial perturbations. The poor robustness of image classifiers to small data distribution shifts raises serious concerns regarding their trustworthiness. To build reliable machine learning models, we must design principled methods to analyze and understand the mechanisms that shape robustness and invariance. This is exactly the focus of this thesis.   First, we study the problem of computing sparse adversarial perturbations. We exploit the geometry of the decision boundaries of image classifiers for computing sparse perturbations very fast, and reveal a qualitative connection between adversarial examples and the data features that image classifiers learn. Then, to better understand this connection, we propose a geometric framework that connects the distance of data samples to the decision boundary, with the features existing in the data. We show that deep classifiers have a strong inductive bias towards invariance to non- discriminative features, and that adversarial training exploits this property to confer robustness. Finally, we focus on the challenging problem of generalization to unforeseen corruptions of the data, and we propose a novel data augmentation scheme for achieving state-of-the-art robustness to common corruptions of the images.   Overall, our results contribute to the understanding of the fundamental mechanisms of deep image classifiers, and pave the way for building more reliable machine learning systems that can be deployed in real-world environments.",https://arxiv.org/abs/2209.02412
Unrestricted Adversarial Samples Based on Non-semantic FeatureClusters Substitution,"MingWeiZhou, Xiaobing Pei",31 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," Most current methods generate adversarial examples with the $L_p$ norm specification. As a result, many defense methods utilize this property to eliminate the impact of such attacking algorithms. In this paper,we instead introduce ""unrestricted"" perturbations that create adversarial samples by using spurious relations which were learned by model training. Specifically, we find feature clusters in non-semantic features that are strongly correlated with model judgment results, and treat them as spurious relations learned by the model. Then we create adversarial samples by using them to replace the corresponding feature clusters in the target image. Experimental evaluations show that in both black-box and white-box situations. Our adversarial examples do not change the semantics of images, while still being effective at fooling an adversarially trained DNN image classifier.",https://arxiv.org/abs/2209.02408
Guidelines to Develop Trustworthy Conversational Agents for Children,"Marina Escobar-Planas, EmiliaGÃ³mez, Carlos-D MartÃ­nez-Hinarejos",01-sep-22,Human-Computer Interaction (cs.HC)," Conversational agents (CAs) embodied in speakers or chatbots are becoming very popular in some countries, and despite their adult-centred design, they have become part of children's lives, generating a need for children-centric trustworthy systems. This paper presents a literature review to identify the main opportunities, challenges and risks brought by CAs when used by children. We then consider relevant ethical guidelines for AI and adapt them to this particular system and population, using a Delphi methodology with a set of experts from different disciplines. From this analysis, we propose specific guidelines to help CAs developers improve their design towards trustworthiness and children.",https://arxiv.org/abs/2209.02406
Topic Detection in Continuous Sign Language Videos,"AlvaroBudria, LaiaTarres, Gerard I.Gallego, Francesc Moreno-Noguer, JordiTorres, Xavier Giro-i-Nieto",01-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Significant progress has been made recently on challenging tasks in automatic sign language understanding, such as sign language recognition, translation and production. However, these works have focused on datasets with relatively few samples, short recordings and limited vocabulary and signing space. In this work, we introduce the novel task of sign language topic detection. We base our experiments on How2Sign, a large-scale video dataset spanning multiple semantic domains. We provide strong baselines for the task of topic detection and present a comparison between different visual features commonly used in the domain of sign language.",https://arxiv.org/abs/2209.02403
SkeletonMAE: Spatial-Temporal Masked Autoencoders for Self-supervisedSkeleton Action Recognition,"WenhanWu, YileiHua, Cezheng, Shiqian Wu, Chen Chen, Aidong Lu",01-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Fully supervised skeleton-based action recognition has achieved great progress with the blooming of deep learning techniques. However, these methods require sufficient labeled data which is not easy to obtain. In contrast, self-supervised skeleton-based action recognition has attracted more attention. With utilizing the unlabeled data, more generalizable features can be learned to alleviate the overfitting problem and reduce the demand of massive labeled training data. Inspired by the MAE, we propose a spatial-temporal masked autoencoder framework for self-supervised 3D skeleton-based action recognition (SkeletonMAE). Following MAE's masking and reconstruction pipeline, we utilize a skeleton based encoder-decoder transformer architecture to reconstruct the masked skeleton sequences. A novel masking strategy, named Spatial-Temporal Masking, is introduced in terms of both joint-level and frame-level for the skeleton sequence. This pre-training strategy makes the encoder output generalizable skeleton features with spatial and temporal dependencies. Given the unmasked skeleton sequence, the encoder is fine-tuned for the action recognition task. Extensive experiments show that our SkeletonMAE achieves remarkable performance and outperforms the state-of-the-art methods on both NTU RGB+D and NTU RGB+D 120 datasets.",https://arxiv.org/abs/2209.02402
A Scene-Text Synthesis Engine Achieved Through Learning fromDecomposed Real-World Data,"ZhengmiTang, TomoMiyazaki, ShinichiroOmachi",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Scene-text image synthesis techniques aimed at naturally composing text instances on background scene images are very appealing for training deep neural networks because they can provide accurate and comprehensive annotation information. Prior studies have explored generating synthetic text images on two-dimensional and three-dimensional surfaces based on rules derived from real-world observations. Some of these studies have proposed generating scene-text images from learning; however, owing to the absence of a suitable training dataset, unsupervised frameworks have been explored to learn from existing real-world data, which may not result in a robust performance. To ease this dilemma and facilitate research on learning-based scene text synthesis, we propose DecompST, a real-world dataset prepared using public benchmarks, with three types of annotations: quadrilateral- level BBoxes, stroke-level text masks, and text-erased images. Using the DecompST dataset, we propose an image synthesis engine that includes a text location proposal network (TLPNet) and a text appearance adaptation network (TAANet). TLPNet first predicts the suitable regions for text embedding. TAANet then adaptively changes the geometry and color of the text instance according to the context of the background. Our comprehensive experiments verified the effectiveness of the proposed method for generating pretraining data for scene text detectors.",https://arxiv.org/abs/2209.02399
On Effectively Predicting Autism Spectrum Disorder Using an Ensembleof Classifiers,"BhekisiphoTwala, EamonMolloy",02-sep-22,Machine Learning (cs.LG)," An ensemble of classifiers combines several single classifiers to deliver a final prediction or classification decision. An increasingly provoking question is whether such systems can outperform the single best classifier. If so, what form of an ensemble of classifiers (also known as multiple classifier learning systems or multiple classifiers) yields the most significant benefits in the size or diversity of the ensemble itself? Given that the tests used to detect autism traits are time-consuming and costly, developing a system that will provide the best outcome and measurement of autism spectrum disorder (ASD) has never been critical. In this paper, several single and later multiple classifiers learning systems are evaluated in terms of their ability to predict and identify factors that influence or contribute to ASD for early screening purposes. A dataset of behavioural data and robot-enhanced therapy of 3,000 sessions and 300 hours, recorded from 61 children are utilised for this task. Simulation results show the superior predictive performance of multiple classifier learning systems (especially those with three classifiers per ensemble) compared to individual classifiers, with bagging and boosting achieving excellent results. It also appears that social communication gestures remain the critical contributing factor to the ASD problem among children.",https://arxiv.org/abs/2209.02397
Butterflies: A new source of inspiration for futuristic aerialrobotics,"ChakravarthiJada, LokeshCh.R.S, AshokUrlana, Shridi SwamyYerubandi, Kantha RaoBora, GouseBashaShaik, PavanBaswani, BalarajuKarri",24 Aug 2022,Robotics (cs.RO)," Nature is an inhabitant for enormous number of species. All the species do perform complex activities with simple and elegant rules for their survival. The property of emergence of collective behavior is remarkably supporting their activities. One form of the collective behaviour is the swarm intelligence -- all agents poses same rules and capabilities. This equality along with local cooperation in the agents tremendously leads to achieving global results. Some of the swarm behaviours in the nature includes birds formations , fish school maneuverings, ants movement. Recently, one school of research has studied these behaviours and proposed artificial paradigms such as Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), Glowworm Swarm Optimization (GSO) etc. Another school of research used these models and designed robotic platforms to detect (locate) multiple signal sources such as light, fire, plume, odour etc. Kinbots platform is one such recent experiment. In the same line of thought, this extended abstract presents the recently proposed butterfly inspired metaphor and corresponding simulations, ongoing experiments with outcomes.",https://arxiv.org/abs/2209.02395
ProjB: An Improved Bilinear Biased ProjE model for Knowledge GraphCompletion,"MojtabaMoattari, SaharVahdati, FarhanaZulkernine",15 Aug 2022,Artificial Intelligence (cs.AI)," Knowledge Graph Embedding (KGE) methods have gained enormous attention from a wide range of AI communities including Natural Language Processing (NLP) for text generation, classification and context induction. Embedding a huge number of inter-relationships in terms of a small number of dimensions, require proper modeling in both cognitive and computational aspects. Recently, numerous objective functions regarding cognitive and computational aspects of natural languages are developed. Among which are the state-of-the-art methods of linearity, bilinearity, manifold-preserving kernels, projection-subspace, and analogical inference. However, the major challenge of such models lies in their loss functions that associate the dimension of relation embeddings to corresponding entity dimension. This leads to inaccurate prediction of corresponding relations among entities when counterparts are estimated wrongly. ProjE KGE, published by Bordes et al., due to low computational complexity and high potential for model improvement, is improved in this work regarding all translative and bilinear interactions while capturing entity nonlinearity. Experimental results on benchmark Knowledge Graphs (KGs) such as FB15K and WN18 show that the proposed approach outperforms the state-of-the-art models in entity prediction task using linear and bilinear methods and other recent powerful ones. In addition, a parallel processing structure is proposed for the model in order to improve the scalability on large KGs. The effects of different adaptive clustering and newly proposed sampling approaches are also explained which prove to be effective in improving the accuracy of knowledge graph completion.",https://arxiv.org/abs/2209.02391
Autonomous Passage Planning for a Polar Vessel,"Jonathan D.Smith, Samuel Hall, GeorgeCoombs, James Byrne, Michael A.S.Thorne, J. AlexanderBrearley, Derek Long, MichaelMeredith, Maria Fox",17 Aug 2022,Robotics (cs.RO)," We introduce a method for long-distance maritime route planning in polar regions, taking into account complex changing environmental conditions. The method allows the construction of optimised routes, describing the three main stages of the process: discrete modelling of the environmental conditions using a non-uniform mesh, the construction of mesh- optimal paths, and path smoothing. In order to account for different vehicle properties we construct a series of data driven functions that can be applied to the environmental mesh to determine the speed limitations and fuel requirements for a given vessel and mesh cell, representing these quantities graphically and geospatially. In describing our results, we demonstrate an example use case for route planning for the polar research ship the RRS Sir David Attenborough (SDA), accounting for ice-performance characteristics and validating the spatial-temporal route construction in the region of the Weddell Sea, Antarctica. We demonstrate the versatility of this route construction method by demonstrating that routes change depending on the seasonal sea ice variability, differences in the route-planning objective functions used, and the presence of other environmental conditions such as currents. To demonstrate the generality of our approach, we present examples in the Arctic Ocean and the Baltic Sea. The techniques outlined in this manuscript are generic and can therefore be applied to vessels with different characteristics. Our approach can have considerable utility beyond just a single vessel planning procedure, and we outline how this workflow is applicable to a wider community, e.g. commercial and passenger shipping.",https://arxiv.org/abs/2209.02390
Pathway to Future Symbiotic Creativity,"YikeGuo, QifengLiu, JieChen, WeiXue, HenrikJensen, FernandoRosas, Jeffrey Shaw, Xing Wu, JijiZhang, Jianliang Xu",18 Aug 2022,Artificial Intelligence (cs.AI)," This report presents a comprehensive view of our vision on the development path of the human-machine symbiotic art creation. We propose a classification of the creative system with a hierarchy of 5 classes, showing the pathway of creativity evolving from a mimic-human artist (Turing Artists) to a Machine artist in its own right. We begin with an overview of the limitations of the Turing Artists then focus on the top two-level systems, Machine Artists, emphasizing machine-human communication in art creation. In art creation, it is necessary for machines to understand humans' mental states, including desires, appreciation, and emotions, humans also need to understand machines' creative capabilities and limitations. The rapid development of immersive environment and further evolution into the new concept of metaverse enable symbiotic art creation through unprecedented flexibility of bi-directional communication between artists and art manifestation environments. By examining the latest sensor and XR technologies, we illustrate the novel way for art data collection to constitute the base of a new form of human-machine bidirectional communication and understanding in art creation. Based on such communication and understanding mechanisms, we propose a novel framework for building future Machine artists, which comes with the philosophy that a human- compatible AI system should be based on the ""human-in-the-loop"" principle rather than the traditional ""end-to-end"" dogma. By proposing a new form of inverse reinforcement learning model, we outline the platform design of machine artists, demonstrate its functions and showcase some examples of technologies we have developed. We also provide a systematic exposition of the ecosystem for AI-based symbiotic art form and community with an economic model built on NFT technology. Ethical issues for the development of machine artists are also discussed.",https://arxiv.org/abs/2209.02389
"MARTI-4: new model of human brain, considering neocortex and basalganglia -- learns to play Atari game by reinforcement learning on a single CPU","IgorPivovarov, SergeyShumsky",18 Aug 2022,Artificial Intelligence (cs.AI)," We present Deep Control - new ML architecture of cortico-striatal brain circuits, which use whole cortical column as a structural element, instead of a singe neuron. Based on this architecture, we present MARTI - new model of human brain, considering neocortex and basal ganglia. This model is de-signed to implement expedient behavior and is capable to learn and achieve goals in unknown environments. We introduce a novel surprise feeling mechanism, that significantly improves reinforcement learning process through inner rewards. We use OpenAI Gym environment to demonstrate MARTI learning on a single CPU just in several hours.",https://arxiv.org/abs/2209.02388
Inferring Implicit 3D Representations from Human Figures on PictorialMaps,"RaimundSchnÃ¼rer, A. CengizÃ–ztireli, MagnusHeitzler, RenÃ©Sieber, Lorenz Hurni",30 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," In this work, we present an automated workflow to bring human figures, one of the most frequently appearing entities on pictorial maps, to the third dimension. Our workflow is based on training data (i.e., SMPL-X, AGORA) and neural networks for single-view 3D reconstruction of real humans from photos. We first let a network consisting of fully connected layers estimate the depth coordinate of 2D pose points. The gained 3D pose points are inputted together with 2D masks of body parts into a deep implicit surface network to infer 3D signed distance fields (SDFs). By assembling all body parts, we derive 2D depth images and body part masks of the whole figure for different views, which are fed into a fully convolutional network to predict UV images. These UV images and the texture for the given perspective are inserted into a generative network to inpaint the textures for the other views. Finally, the generated textures are assigned to the inferred body parts in a ray marcher. We test our workflow with 12 pictorial human figures after having validated several network configurations. The created 3D models look generally promising, especially when considering the challenges of silhouette-based 3D recovery and real-time rendering of the implicit SDFs. Further improvement is needed to reduce gaps between the body parts and to add pictorial details to the textures. Overall, the constructed figures may be used for animation and storytelling in digital 3D maps.",https://arxiv.org/abs/2209.02387
YouTube and Science: Models for Research Impact,"Abdul RahmanShaikh, HamedAlhoori, Maoyuan Sun",01-sep-22,Digital Libraries (cs.DL)," Video communication has been rapidly increasing over the past decade, with YouTube providing a medium where users can post, discover, share, and react to videos. There has also been an increase in the number of videos citing research articles, especially since it has become relatively commonplace for academic conferences to require video submissions. However, the relationship between research articles and YouTube videos is not clear, and the purpose of the present paper is to address this issue. We created new datasets using YouTube videos and mentions of research articles on various online platforms. We found that most of the articles cited in the videos are related to medicine and biochemistry. We analyzed these datasets through statistical techniques and visualization, and built machine learning models to predict (1) whether a research article is cited in videos, (2) whether a research article cited in a video achieves a level of popularity, and (3) whether a video citing a research article becomes popular. The best models achieved F1 scores between 80% and 94%. According to our results, research articles mentioned in more tweets and news coverage have a higher chance of receiving video citations. We also found that video views are important for predicting citations and increasing research articles' popularity and public engagement with science.",https://arxiv.org/abs/2209.02385
Self-Calibrating Anomaly and Change Detection for AutonomousInspection Robots,"SaharSalimpour, Jorge PeÃ±aQueralta, TomiWesterlund",26 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," Automatic detection of visual anomalies and changes in the environment has been a topic of recurrent attention in the fields of machine learning and computer vision over the past decades. A visual anomaly or change detection algorithm identifies regions of an image that differ from a reference image or dataset. The majority of existing approaches focus on anomaly or fault detection in a specific class of images or environments, while general purpose visual anomaly detection algorithms are more scarce in the literature. In this paper, we propose a comprehensive deep learning framework for detecting anomalies and changes in a priori unknown environments after a reference dataset is gathered, and without need for retraining the model. We use the SuperPoint and SuperGlue feature extraction and matching methods to detect anomalies based on reference images taken from a similar location and with partial overlapping of the field of view. We also introduce a self-calibrating method for the proposed model in order to address the problem of sensitivity to feature matching thresholds and environmental conditions. To evaluate the proposed framework, we have used a ground robot system for the purpose of reference and query data collection. We show that high accuracy can be obtained using the proposed method. We also show that the calibration process enhances changes and foreign object detection performance",https://arxiv.org/abs/2209.02380
MangoLeafBD: A Comprehensive Image Dataset to Classify Diseased andHealthy Mango Leaves,"Sarder IftekharAhmed, MuhammadIbrahim, Md. Nadim, Md. MizanurRahman, Maria MehjabinShejunti, TaskeedJabid, Md.Sawkat Ali",27 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," Agriculture is of one of the few remaining sectors that is yet to receive proper attention from the machine learning community. The importance of datasets in the machine learning discipline cannot be overemphasized. The lack of standard and publicly available datasets related to agriculture impedes practitioners of this discipline to harness the full benefit of these powerful computational predictive tools and techniques. To improve this scenario, we develop, to the best of our knowledge, the first-ever standard, ready-to-use, and publicly available dataset of mango leaves. The images are collected from four mango orchards of Bangladesh, one of the top mango-growing countries of the world. The dataset contains 4000 images of about 1800 distinct leaves covering seven diseases. Although the dataset is developed using mango leaves of Bangladesh only, since we deal with diseases that are common across many countries, this dataset is likely to be applicable to identify mango diseases in other countries as well, thereby boosting mango yield. This dataset is expected to draw wide attention from machine learning researchers and practitioners in the field of automated agriculture.",https://arxiv.org/abs/2209.02379
Understanding and Reducing Crater Counting Errors in Citizen ScienceData and the Need for Standardisation,"P.D.Tar, N.A.Thacker",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Citizen science has become a popular tool for preliminary data processing tasks, such as identifying and counting Lunar impact craters in modern high-resolution imagery. However, use of such data requires that citizen science products are understandable and reliable. Contamination and missing data can reduce the usefulness of datasets so it is important that such effects are quantified. This paper presents a method, based upon a newly developed quantitative pattern recognition system (Linear Poisson Models) for estimating levels of contamination within MoonZoo citizen science crater data. Evidence will show that it is possible to remove the effects of contamination, with reference to some agreed upon ground truth, resulting in estimated crater counts which are highly repeatable. However, it will also be shown that correcting for missing data is currently more difficult to achieve. The techniques are tested on MoonZoo citizen science crater annotations from the Apollo 17 site and also undergraduate and expert results from the same region.",https://arxiv.org/abs/2209.02377
Continual Learning: Fast and Slow,"QuangPham, Chenghao Liu, Steven C. H.Hoi",06-sep-22,Artificial Intelligence (cs.AI)," According to the Complementary Learning Systems (CLS) theory~\cite{mcclelland1995there} in neuroscience, humans do effective \emph{continual learning} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics, individual experiences; and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a general continual learning framework comprising a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for representation learning of task-agnostic general representation via Self-Supervised Learning (SSL). DualNets can seamlessly incorporate both representation types into a holistic framework to facilitate better continual learning in deep neural networks. Via extensive experiments, we demonstrate the promising results of DualNets on a wide range of continual learning protocols, ranging from the standard offline, task-aware setting to the challenging online, task-free scenario. Notably, on the CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly different visual images, DualNets can achieve competitive performance with existing state-of-the-art dynamic architecture strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive ablation studies to validate DualNets efficacy, robustness, and scalability. Code is publicly available at \url{[this https URL](https://github.com/phquang/DualNet)}.",https://arxiv.org/abs/2209.02375
"DualNet: Continual Learning, Fast and Slow","QuangPham, Chenghao Liu, Steven Hoi",01-oct-21,Machine Learning (cs.LG)," According to Complementary Learning Systems (CLS) theory~\citep{mcclelland1995there} in neuroscience, humans do effective \emph{continual learning} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics and individual experiences, and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose a novel continual learning framework named ""DualNet"", which comprises a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for unsupervised representation learning of task-agnostic general representation via a Self-Supervised Learning (SSL) technique. The two fast and slow learning systems are complementary and work seamlessly in a holistic continual learning framework. Our extensive experiments on two challenging continual learning benchmarks of CORE50 and miniImageNet show that DualNet outperforms state-of-the-art continual learning methods by a large margin. We further conduct ablation studies of different SSL objectives to validate DualNet's efficacy, robustness, and scalability. Code will be made available upon acceptance.",https://arxiv.org/abs/2209.02370
Improving Robustness to Out-of-Distribution Data by Frequency-basedAugmentation,"KokiMukai, SoichiroKumano, ToshihikoYamasaki",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Although Convolutional Neural Networks (CNNs) have high accuracy in image recognition, they are vulnerable to adversarial examples and out- of-distribution data, and the difference from human recognition has been pointed out. In order to improve the robustness against out-of-distribution data, we present a frequency-based data augmentation technique that replaces the frequency components with other images of the same class. When the training data are CIFAR10 and the out-of-distribution data are SVHN, the Area Under Receiver Operating Characteristic (AUROC) curve of the model trained with the proposed method increases from 89.22\% to 98.15\%, and further increased to 98.59\% when combined with another data augmentation method. Furthermore, we experimentally demonstrate that the robust model for out-of-distribution data uses a lot of high-frequency components of the image.",https://arxiv.org/abs/2110.00175
Finger Multimodal Feature Fusion and Recognition Based on ChannelSpatial Attention,"JianGuo, JiaxiangTu, HengyiRen, ChongHan, LijuanSun",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Due to the instability and limitations of unimodal biometric systems, multimodal systems have attracted more and more attention from researchers. However, how to exploit the independent and complementary information between different modalities remains a key and challenging problem. In this paper, we propose a multimodal biometric fusion recognition algorithm based on fingerprints and finger veins (Fingerprint Finger Veins- Channel Spatial Attention Fusion Module, FPV-CSAFM). Specifically, for each pair of fingerprint and finger vein images, we first propose a simple and effective Convolutional Neural Network (CNN) to extract features. Then, we build a multimodal feature fusion module (Channel Spatial Attention Fusion Module, CSAFM) to fully fuse the complementary information between fingerprints and finger veins. Different from existing fusion strategies, our fusion method can dynamically adjust the fusion weights according to the importance of different modalities in channel and spatial dimensions, so as to better combine the information between different modalities and improve the overall recognition performance. To evaluate the performance of our method, we conduct a series of experiments on multiple public datasets. Experimental results show that the proposed FPV-CSAFM achieves excellent recognition performance on three multimodal datasets based on fingerprints and finger veins.",https://arxiv.org/abs/2209.02369
Inverse methods: How feasible are spatially low-resolved capacityexpansion modeling results when dis-aggregated at high resolution?,"Martha MariaFrysztacki, VeitHagenmeyer, Tom Brown",06-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," Spatially highly-resolved capacity expansion models are computationally intensive. As a result, models are often simplified to a lower spatial resolution by clustering multiple regions to a smaller number of representatives. However, when capacity expansion is modeled for electricity systems at a coarse resolution, the aggregation mixes sites with different renewable features while removing transmission lines that can cause congestion. As a consequence, the modeling results may represent an infeasible electricity system when the capacities are fed back into higher spatial detail. Thus far there has been no detailed investigation of how best to dis-aggregate the capacity expansion results into its original resolution and whether the spatially highly-resolved dis-aggregated model is technically feasible. This is a challenge since there is no unique or obvious way to invert the clustering. In this paper we proceed in two stages. First, we propose three methods to dis-aggregate spatially low- resolved model results into higher resolution: (a) uniformly distribute the regionalised results across their original set of regions, (b) re-optimising each clustered region separately at high resolution (c) a novel approach that minimises the custom ""excess electricity"" function. Second, we investigate the resulting highly-resolved models' feasibility by running an operational dispatch. While re-optimising yields the lowest amounts of load- shedding and curtailment, our novel inverse-method provides comparable results for considerably less computational effort. Feasibility-wise, our results strengthen previously published research that modeling every country by a single region is insufficient. Beyond that, we find that results obtained from state-of-the-art reduced models with 100-200 regions for Europe still yield 3-7% of load-shedding, depending on the model resolution and inverse method.",https://arxiv.org/abs/2209.02368
Token Multiplicity in Reversing Petri Nets Under the Individual TokenInterpretation,"AnnaPhilippou, KyriakiPsara",06-sep-22,Logic in Computer Science (cs.LO)," Reversing Petri nets (RPNs) have recently been proposed as a net- basedapproach to model causal and out-of-causal order reversibility. They are based on the notion of individual tokens that can be connected together via bonds. In this paper we extend RPNs by allowing multiple tokens of the same type to exist within a net based on the individual token interpretation of Petri nets. According to this interpretation, tokens of the same type are distinguished via their causal path. We develop a causal semantics of the model and we prove that the expressive power of RPNs with multiple tokens is equivalent to that of RPNs with single tokens by establishing an isomporphism between the Labelled Transition Systems (LTSs) capturing the reachable parts of the respective RPN models.",https://arxiv.org/abs/2209.02364
Proceedings Combined 29th International Workshop on Expressiveness inConcurrency and 19th Workshop on Structural Operational Semantics,"ValentinaCastiglioni, Claudio A.Mezzina",31 Aug 2022,Logic in Computer Science (cs.LO)," This volume contains the proceedings of EXPRESS/SOS 2022: the Combined 29th International Workshop on Expressiveness in Concurrency and the 19th Workshop on Structural Operational Semantics, which was held in Warsaw, Poland, as an affiliated workshop of CONCUR 2022, the 33rd International Conference on Concurrency Theory. The EXPRESS/SOS workshop series aims at bringing together researchers interested in the formal semantics of systems and programming concepts, and in the expressiveness of computational models.",https://arxiv.org/abs/2209.02359
Encodability and Separation for a Reflective Higher-Order Calculus,StianLybech,06-sep-22,Logic in Computer Science (cs.LO)," The $\rho$-calculus (Reflective Higher-Order Calculus) of Meredith and Radestock is a $\pi$-calculus-like language with some unusual features, notably, structured names, runtime generation of free names, and the lack of an operator for scoping visibility of names. These features pose some interesting difficulties for proofs of encodability and separation results. We describe two errors in a previously published attempt to encode the $\pi$-calculus in the $\rho$-calculus by Meredith and Radestock. Then we give a new encoding and prove its correctness, using a set of encodability criteria close to those proposed by Gorla, and discuss the adaptations necessary to work with a calculus with runtime generation of structured names. Lastly we prove a separation result, showing that the $\rho$-calculus cannot be encoded in the $\pi$-calculus.",https://arxiv.org/abs/2208.14777
Proceedings Combined 29th International Workshop on Expressiveness inConcurrency and 19th Workshop on Structural Operational Semantics,"ValentinaCastiglioni, Claudio A.Mezzina",31 Aug 2022,Logic in Computer Science (cs.LO)," This volume contains the proceedings of EXPRESS/SOS 2022: the Combined 29th International Workshop on Expressiveness in Concurrency and the 19th Workshop on Structural Operational Semantics, which was held in Warsaw, Poland, as an affiliated workshop of CONCUR 2022, the 33rd International Conference on Concurrency Theory. The EXPRESS/SOS workshop series aims at bringing together researchers interested in the formal semantics of systems and programming concepts, and in the expressiveness of computational models.",https://arxiv.org/abs/2209.02356
A Generic Type System for Higher-Order $Î¨$-calculi,"Alex RÃ¸nningBendixen, Bjarke BredowBojesen, HansHÃ¼ttel, StianLybech",06-sep-22,Logic in Computer Science (cs.LO)," The Higher-Order $\Psi$-calculus framework (HO$\Psi$) is a generalisation of many first- and higher-order extensions of the $\pi$-calculus. It was proposed by Parrow et al. who showed that higher- order calculi such as HO$\pi$ and CHOCS can be expressed as HO$\Psi$-calculi. In this paper we present a generic type system for HO$\Psi$-calculi which extends previous work by HÃ¼ttel on a generic type system for first-order $\Psi$-calculi. Our generic type system satisfies the usual property of subject reduction and can be instantiated to yield type systems for variants of HO{\pi}, including the type system for termination due to Demangeon et al. Moreover, we derive a type system for the $\rho$-calculus, a reflective higher-order calculus proposed by Meredith and Radestock. This establishes that our generic type system is richer than its predecessor, as the $\rho$-calculus cannot be encoded in the $\pi$-calculus in a way that satisfies standard criteria of encodability.",https://arxiv.org/abs/2208.14777
Proceedings Combined 29th International Workshop on Expressiveness inConcurrency and 19th Workshop on Structural Operational Semantics,"ValentinaCastiglioni, Claudio A.Mezzina",31 Aug 2022,Logic in Computer Science (cs.LO)," This volume contains the proceedings of EXPRESS/SOS 2022: the Combined 29th International Workshop on Expressiveness in Concurrency and the 19th Workshop on Structural Operational Semantics, which was held in Warsaw, Poland, as an affiliated workshop of CONCUR 2022, the 33rd International Conference on Concurrency Theory. The EXPRESS/SOS workshop series aims at bringing together researchers interested in the formal semantics of systems and programming concepts, and in the expressiveness of computational models.",https://arxiv.org/abs/2209.02354
From Legal Contracts to Legal Calculi: the code-driven normativity,SilviaCrafa,06-sep-22,Programming Languages (cs.PL)," Using dedicated software to represent or enact legislation or regulation has the advantage of solving the inherent ambiguity of legal texts and enabling the automation of compliance with legal norms. On the other hand, the so-called code-driven normativity is less flexible than the legal provisions it claims to implement, and transforms the nature of legal protection, potentially reducing the capability of individual human beings to invoke legal remedies.   In this article we focus on software-based legal contracts; we illustrate the design of a legal calculus whose primitives allow a direct formalisation of contracts' normative elements (i.e., permissions, prohibitions, obligations, asset transfer, judicial enforcement and openness to the external context). We show that interpreting legal contracts as interaction protocols between (untrusted) parties enables the generalisation of formal methods and tools for concurrent systems to the legal setting",https://arxiv.org/abs/2208.14777
Proceedings Combined 29th International Workshop on Expressiveness inConcurrency and 19th Workshop on Structural Operational Semantics,"ValentinaCastiglioni, Claudio A.Mezzina",31 Aug 2022,Logic in Computer Science (cs.LO)," This volume contains the proceedings of EXPRESS/SOS 2022: the Combined 29th International Workshop on Expressiveness in Concurrency and the 19th Workshop on Structural Operational Semantics, which was held in Warsaw, Poland, as an affiliated workshop of CONCUR 2022, the 33rd International Conference on Concurrency Theory. The EXPRESS/SOS workshop series aims at bringing together researchers interested in the formal semantics of systems and programming concepts, and in the expressiveness of computational models.",https://arxiv.org/abs/2209.02353
Pacta sunt servanda: legal contracts in Stipula,"SilviaCrafa, CosimoLaneve, GiovanniSartor",21-oct-21,Programming Languages (cs.PL)," There is a growing interest in running legal contracts on digital systems, at the same time, it is important to understand to what extent software contracts may capture legal content. We then undertake a foundational study of legal contracts and we distill four main features: agreement, permissions, violations and obligations. We therefore design Stipula, a domain specific language that assists lawyers in programming legal contracts through specific patterns. The language is based on a small set of abstractions that correspond to common patterns in legal contracts, and that are amenable to be executed either on centralized or on distributed systems. Stipula comes with a formal semantics and an observational equivalence, that provide for a clear account of the contracts' behaviour. The expressive power of the language is illustrated by a set of examples that correspond to template contracts that are often used in practice.",https://arxiv.org/abs/2208.14777
Measure Construction by Extension in Dependent Type Theory withApplication to Integration,"ReynaldAffeldt, Cyril Cohen",06-sep-22,Logic in Computer Science (cs.LO)," We report on an original formalization of measure and integration theory in the Coq proof assistant. We build the Lebesgue measure following a standard construction that has not yet been formalized in type theory-based proof assistants: by extension of a measure over a semiring of sets. We achieve this formalization by leveraging on existing techniques from the Mathematics Components project. We explain how we extend Mathematical Components' iterated operators and mathematical structures for analysis to provide support for infinite sums and extended real numbers. We introduce new mathematical structures for measure theory and incidentally provide an illustrative, concrete application of Hierarchy-Builder, a generic tool for the formalization of hierarchies of mathematical structures. This formalization of measure theory provides the basis for a new formalization of the Lebesgue integration compatible with the Mathematical Components project.",https://arxiv.org/abs/2110.11069
Convergence and error estimates of a penalization finite volume methodfor the compressible Navier-Stokes system,"MÃ¡ria LukÃ¡ÄovÃ¡-Medvid'ovÃ¡, Bangwei She, Yuhuan Yuan",06-sep-22,Numerical Analysis (math.NA)," In numerical simulations a smooth domain occupied by a fluid has to be approximated by a computational domain that typically does not coincide with a physical domain. Consequently, in order to study convergence and error estimates of a numerical method domain-related discretization errors, the so-called variational crimes, need to be taken into account. In this paper we present an elegant alternative to a direct, but rather technical, analysis of variational crimes by means of the penalty approach. We embed the physical domain into a large enough cubed domain and study the convergence of a finite volume method for the corresponding domain-penalized problem. We show that numerical solutions of the penalized problem converge to a generalized, the so-called dissipative weak, solution of the original problem. If a strong solution exists, the dissipative weak solution emanating from the same initial data coincides with the strong solution. In this case, we apply a novel tool of the relative energy and derive the error estimates between the numerical solution and the strong solution. Extensive numerical experiments that confirm theoretical results are presented.",https://arxiv.org/abs/2209.02345
EnergonAI: An Inference System for 10-100 Billion ParameterTransformer Models,"JiangsuDu, ZimingLiu, JiaruiFang, Shenggui Li, Yongbin Li, Yutong Lu, Yang You",06-sep-22,Machine Learning (cs.LG)," Large transformer models display promising performance on a wide range of natural language processing (NLP) tasks. Although the AI community has expanded the model scale to the trillion parameter level, the practical deployment of 10-100 billion parameter models is still uncertain due to the latency, throughput, and memory constraints.   In this paper, we proposed EnergonAI to solve the challenges of the efficient deployment of 10-100 billion parameter transformer models on single- or multi-GPU systems. EnergonAI adopts a hierarchy-controller system architecture to coordinate multiple devices and efficiently support different parallel patterns. It delegates the execution of sub-models to multiple workers in the single-controller style and applies tensor parallelism and pipeline parallelism among the workers in a multi-controller style. Upon the novel architecture, we propose three techniques, i.e. non- blocking pipeline parallelism, distributed redundant computation elimination, and peer memory pooling. EnergonAI enables the users to program complex parallel code the same as a serial one. Compared with the FasterTransformer, we have proven that EnergonAI has superior performance on latency and throughput. In our experiments, EnergonAI can achieve 37% latency reduction in tensor parallelism, 10% scalability improvement in pipeline parallelism, and it improves the model scale inferred on a single GPU by using a larger heterogeneous memory space at cost of limited performance reduction.",https://arxiv.org/abs/2209.02344
MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detectionwith Natural Trigger in Real-World,"Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, AlsharifAbuadbba, Anmin Fu, SaidF. Al-Sarawi, NepalSurya, DerekAbbott",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Object detection is the foundation of various critical computer- vision tasks such as segmentation, object tracking, and event detection. To train an object detector with satisfactory accuracy, a large amount of data is required. However, due to the intensive workforce involved with annotating large datasets, such a data curation task is often outsourced to a third party or relied on volunteers. This work reveals severe vulnerabilities of such data curation pipeline. We propose MACAB that crafts clean-annotated images to stealthily implant the backdoor into the object detectors trained on them even when the data curator can manually audit the images. We observe that the backdoor effect of both misclassification and the cloaking are robustly achieved in the wild when the backdoor is activated with inconspicuously natural physical triggers. Backdooring non- classification object detection with clean-annotation is challenging compared to backdooring existing image classification tasks with clean- label, owing to the complexity of having multiple objects within each frame, including victim and non-victim objects. The efficacy of the MACAB is ensured by constructively i abusing the image-scaling function used by the deep learning framework, ii incorporating the proposed adversarial clean image replica technique, and iii combining poison data selection criteria given constrained attacking budget. Extensive experiments demonstrate that MACAB exhibits more than 90% attack success rate under various real-world scenes. This includes both cloaking and misclassification backdoor effect even restricted with a small attack budget. The poisoned samples cannot be effectively identified by state-of-the-art detection techniques.The comprehensive video demo is at [this https URL](https://youtu.be/MA7L_LpXkp4), which is based on a poison rate of 0.14% for YOLOv4 cloaking backdoor and Faster R-CNN misclassification backdoor.",https://arxiv.org/abs/2209.02341
Let's Learn from Children: Scaffolding to Enable Search as Learning inthe Educational Environment,"MonicaLandoni, Maria SoledadPera, EmilianaMurgia, TheoHuibers",06-sep-22,Information Retrieval (cs.IR)," In this manuscript, we argue for the need to further look at search as learning (SAL) with children as the primary stakeholders. Inspired by how children learn and considering the classroom (regardless of the teaching modality) as a natural educational ecosystem, we posit that scaffolding is the tie that can simultaneously allow for learning to search while searching for learning. The main contribution of this work is a list of open challenges focused on the primary school classroom for the IR community to consider when setting up to explore and make progress on SAL research with and for children and beyond.",https://arxiv.org/abs/2209.02339
An Adaptive Column Compression Family for Self-Driving Databases,"MarcellFehÃ©r, Daniel E.Lucani, IoannisChatzigeorgiou",06-sep-22,Databases (cs.DB)," Modern in-memory databases are typically used for high-performance workloads, therefore they have to be optimized for small memory footprint and high query speed at the same time. Data compression has the potential to reduce memory requirements but often reduces query speed too. In this paper we propose a novel, adaptive compressor that offers a new trade-off point of these dimensions, achieving better compression than LZ4 while reaching query speeds close to the fastest existing segment encoders. We evaluate our compressor both with synthetic data in isolation and on the TPC-H and Join Order Benchmarks, integrated into a modern relational column store, Hyrise.",https://arxiv.org/abs/2209.02338
Market Model 3.0: A New Ecosystem for Demand-side FlexibilityAggregators in Denmark,"Peter Alexander VistarGade, TrygveSkjÃ¸tskift, Henrik W.Bindner, JalalKazempour",06-sep-22,Systems and Control (eess.SY)," Denmark has recently set a new legislation called Market Model 3.0 that explicitly re-defines the position of flexibility aggregators, aiming to further promote demand-side flexibility. The main change is to relax the previous mandate that required each aggregator to be associated with a retailer and a balance responsible party. We explain the rationale behind such a change and its implications, particularly on the pre-qualification of demand portfolios providing ancillary services. We also discuss other undergoing conceptual and technical changes.",https://arxiv.org/abs/2209.02334
Multimodal contrastive learning for remote sensing tasks,"UmangiJain, AlexWilson, VarunGulshan",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Self-supervised methods have shown tremendous success in the field of computer vision, including applications in remote sensing and medical imaging. Most popular contrastive-loss based methods like SimCLR, MoCo, MoCo-v2 use multiple views of the same image by applying contrived augmentations on the image to create positive pairs and contrast them with negative examples. Although these techniques work well, most of these techniques have been tuned on ImageNet (and similar computer vision datasets). While there have been some attempts to capture a richer set of deformations in the positive samples, in this work, we explore a promising alternative to generating positive examples for remote sensing data within the contrastive learning framework. Images captured from different sensors at the same location and nearby timestamps can be thought of as strongly augmented instances of the same scene, thus removing the need to explore and tune a set of hand crafted strong augmentations. In this paper, we propose a simple dual-encoder framework, which is pre-trained on a large unlabeled dataset (~1M) of Sentinel-1 and Sentinel-2 image pairs. We test the embeddings on two remote sensing downstream tasks: flood segmentation and land cover mapping, and empirically show that embeddings learnt from this technique outperform the conventional technique of collecting positive examples via aggressive data augmentations.",https://arxiv.org/abs/2209.02332
Well-Separation and Hyperplane Transversals in High Dimensions,"HelenaBergold, DanielBertschinger, NicolasGrelier, WolfgangMulzer, PatrickSchnider",06-sep-22,Computational Geometry (cs.CG)," A family of $k$ point sets in $d$ dimensions is well-separated if the convex hulls of any two disjoint subfamilies can be separated by a hyperplane. Well-separation is a strong assumption that allows us to conclude that certain kinds of generalized ham-sandwich cuts for the point sets exist. But how hard is it to check if a given family of high- dimensional point sets has this property? Starting from this question, we study several algorithmic aspects of the existence of transversals and separations in high-dimensions.   First, we give an explicit proof that $k$ point sets are well-separated if and only if their convex hulls admit no $(k - 2)$-transversal, i.e., if there exists no $(k - 2)$-dimensional flat that intersects the convex hulls of all $k$ sets. It follows that the task of checking well-separation lies in the complexity class coNP. Next, we show that it is NP-hard to decide whether there is a hyperplane-transversal (that is, a $(d - 1)$-transversal) of a family of $d + 1$ line segments in $\mathbb{R}^d$, where $d$ is part of the input. As a consequence, it follows that the general problem of testing well-separation is coNP-complete. Furthermore, we show that finding a hyperplane that maximizes the number of intersected sets is NP-hard, but allows for an $\Omega\left(\frac{\log k}{k \log \log k}\right)$-approximation algorithm that is polynomial in $d$ and $k$, when each set consists of a single point. When all point sets are finite, we show that checking whether there exists a $(k - 2)$-transversal is in fact strongly NP-complete.",https://arxiv.org/abs/2209.02329
Layer or Representation Space: What makes BERT-based EvaluationMetrics Robust?,"Doan Nam LongVu, NafiseSadatMoosavi, Steffen Eger","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02317v1)), lastrevised 7 Sep 2022 (this version, v2)",Computation and Language (cs.CL)," The evaluation of recent embedding-based evaluation metrics for text generation is primarily based on measuring their correlation with human evaluations on standard benchmarks. However, these benchmarks are mostly from similar domains to those used for pretraining word embeddings. This raises concerns about the (lack of) generalization of embedding-based metrics to new and noisy domains that contain a different vocabulary than the pretraining data. In this paper, we examine the robustness of BERTScore, one of the most popular embedding-based metrics for text generation. We show that (a) an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, (b) taking embeddings from the first layer of pretrained models improves the robustness of all metrics, and (c) the highest robustness is achieved when using character- level embeddings, instead of token-based embeddings, from the first layer of the pretrained model.",https://arxiv.org/abs/2209.02319
Wavelet-based Loss for High-frequency Interface Dynamics,"LukasPrantl, JanBender, TassiloKugelstadt, NilsThuerey",06-sep-22,Machine Learning (cs.LG)," Generating highly detailed, complex data is a long-standing and frequently considered problem in the machine learning field. However, developing detail-aware generators remains an challenging and open problem. Generative adversarial networks are the basis of many state-of-the-art methods. However, they introduce a second network to be trained as a loss function, making the interpretation of the learned functions much more difficult. As an alternative, we present a new method based on a wavelet loss formulation, which remains transparent in terms of what is optimized. The wavelet-based loss function is used to overcome the limitations of conventional distance metrics, such as L1 or L2 distances, when it comes to generate data with high-frequency details. We show that our method can successfully reconstruct high-frequency details in an illustrative synthetic test case. Additionally, we evaluate the performance when applied to more complex surfaces based on physical simulations. Taking a roughly approximated simulation as input, our method infers corresponding spatial details while taking into account how they evolve. We consider this problem in terms of spatial and temporal frequencies, and leverage generative networks trained with our wavelet loss to learn the desired spatio-temporal signal for the surface dynamics. We test the capabilities of our method with a set of synthetic wave function tests and complex 2D and 3D dynamics of elasto-plastic materials.",https://arxiv.org/abs/2209.02317
A Multi-FPGA High Performance Computing System for 3D FFT-basedNumerical Simulations,RobertoAmmendola,06-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," In the field of High Performance Computing, communications among processes represent a typical bottleneck for massively parallel scientific applications. Object of this research is the development of a network interface card with specific offloading capabilities that could help large scale simulations in terms of communication latency and scalability with the number of computing elements. In particular this work deals with the development of a double precision floating point complex arithmetic unit with a parallel-pipelined architecture, in order to implement a massively parallel computing system tailored for three dimensional Fast Fourier Transform.",https://arxiv.org/abs/2209.02316
Is it Fun?: Understanding Enjoyment in Non-Game HCI Research,"MichinariKono, KoichiAraake",06-sep-22,Human-Computer Interaction (cs.HC)," An experience of fun can be an important factor for validating the value of games. Research on non-game HCI has been attempted to measure the enjoyment of work. However, a majority of the studies do not discuss the importance and value of the result. It is not clear as to how the term fun is understood in a non-game context. To analyze this shortcoming, we reviewed extant studies, and explored as to how researchers determine if the value of an activity is fun. Consequently, we discussed and categorized the usage of the terms and analyzed the methodologies that are used in extant studies that evaluate the effects of fun and related terms. To gain a better understanding of fun in HCI, we provided several directions that can be discussed for strengthening enjoyable HCI research beyond applications involving games.",https://arxiv.org/abs/2209.02314
A first-order logic characterization of safety and co-safety languages,"AlessandroCimatti, LucaGeatti, NicolaGigante, AngeloMontanari, StefanoTonetta",06-sep-22,Artificial Intelligence (cs.AI)," Linear Temporal Logic (LTL) is one of the most popular temporal logics, that comes into play in a variety of branches of computer science. Among the various reasons of its widespread use there are its strong foundational properties: LTL is equivalent to counter-free omega-automata, to star-free omega-regular expressions, and (by Kamp's theorem) to the first-order theory of one successor (S1S[FO]). Safety and co-safety languages, where a finite prefix suffices to establish whether a word does not belong or belongs to the language, respectively, play a crucial role in lowering the complexity of problems like model checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL) is a fragment of LTL where only universal (resp., existential) temporal modalities are allowed, that recognises safety (resp., co-safety) languages only.   The main contribution of this paper is the introduction of a fragment of S1S[FO], called SafetyFO, and of its dual coSafetyFO, which are expressively complete with respect to the LTL-definable safety and co-safety languages. We prove that they exactly characterize SafetyLTL and coSafetyLTL, respectively, a result that joins Kamp's theorem, and provides a clearer view of the characterization of (fragments of) LTL in terms of first-order languages. In addition, it gives a direct, compact, and self-contained proof that any safety language definable in LTL is definable in SafetyLTL as well. As a by-product, we obtain some interesting results on the expressive power of the weak tomorrow operator of SafetyLTL, interpreted over finite and infinite words. Moreover, we prove that, when interpreted over finite words, SafetyLTL (resp. coSafetyLTL) devoid of the tomorrow (resp., weak tomorrow) operator captures the safety (resp., co-safety) fragment of LTL over finite words.",https://arxiv.org/abs/2209.02308
Coexistence of Pulsed Radar and Communications: InterferenceSuppression and Multi-path Combining,"HaoyuZhang, LiChen, YunfeiChen, HuaruiYin, GuoWei",06-sep-22,Information Theory (cs.IT)," The focus of this study is on the spectrum sharing between multiple-input multiple-output (MIMO) communications and co-located pulsed MIMO radar systems in multi-path environments. The major challenge is to suppress the mutual interference between the two systems while combining the useful multi-path components received at each system. We tackle this challenge by jointly designing the communication precoder, radar transmit waveform and receive filter. Specifically, the signal-to-interference-plus- noise ratio (SINR) at the radar receiver is maximized subject to constraints on the radar waveform, communication rate and transmit power. The multi-path propagation complicates the expressions of the radar SINR and communication rate, leading to a non-convex problem. To solve it, a sub-optimal algorithm based on the alternating maximization is used to optimize the precoder, radar transmit waveform and receive filter iteratively. The radar receive filter can be updated by a closed-form solution. The communication precoder and radar transmit waveform can be obtained by the successive convex approximation and alternating direction method of multipliers. Simulation results are provided to demonstrate the effectiveness of the proposed design.",https://arxiv.org/abs/2209.02307
Towards non-linear quadrature formulae,Georg M. vonHippel,06-sep-22,Numerical Analysis (math.NA)," Prompted by an observation about the integral of exponential functions of the form $f(x)=\lambda e^{\alpha x}$, we investigate the possibility to exactly integrate families of functions generated from a given function by scaling or by affine transformations of the argument using nonlinear generalizations of quadrature formulae. The main result of this paper is that such formulae can be explicitly constructed for a wide class of functions, and have the same accuracy as Newton-Cotes formulae based on the same nodes.",https://arxiv.org/abs/2209.02304
A Survey of Machine Unlearning,"Thanh TamNguyen, Thanh TrungHuynh, PhiLe Nguyen, Alan Wee-ChungLiew, Hongzhi Yin, Quoc Viet HungNguyen","6 Sep 2022 (v1(https://arxiv.org/abs/2209.02299v1)), lastrevised 8 Sep 2022 (this version, v3)",Machine Learning (cs.LG)," Computer systems hold a large amount of personal data over decades. On the one hand, such data abundance allows breakthroughs in artificial intelligence (AI), especially machine learning (ML) models. On the other hand, it can threaten the privacy of users and weaken the trust between humans and AI. Recent regulations require that private information about a user can be removed from computer systems in general and from ML models in particular upon request (e.g. the ""right to be forgotten""). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often ""remember"" the old data. Existing adversarial attacks proved that we can learn private membership or attributes of the training data from the trained models. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to solve the problem completely due to the lack of common frameworks and resources. In this survey paper, we seek to provide a thorough investigation of machine unlearning in its definitions, scenarios, mechanisms, and applications. Specifically, as a categorical collection of state-of-the-art research, we hope to provide a broad reference for those seeking a primer on machine unlearning and its various formulations, design requirements, removal requests, algorithms, and uses in a variety of ML applications. Furthermore, we hope to outline key findings and trends in the paradigm as well as highlight new areas of research that have yet to see the application of machine unlearning, but could nonetheless benefit immensely. We hope this survey provides a valuable reference for ML researchers as well as those seeking to innovate privacy technologies. Our resources are at [this https URL](https://github.com/tamlhp/awesome-machine- unlearning).",https://arxiv.org/abs/2209.02302
Unrolling SGD: Understanding Factors Influencing Machine Unlearning,"AnvithThudi, Gabriel Deza, VarunChandrasekaran, NicolasPapernot","27 Sep 2021 (v1(https://arxiv.org/abs/2109.13398v1)), lastrevised 2 Mar 2022 (this version, v2)",Machine Learning (cs.LG)," Machine unlearning is the process through which a deployed machine learning model is made to forget about some of its training data points. While naively retraining the model from scratch is an option, it is almost always associated with large computational overheads for deep learning models. Thus, several approaches to approximately unlearn have been proposed along with corresponding metrics that formalize what it means for a model to forget about a data point. In this work, we first taxonomize approaches and metrics of approximate unlearning. As a result, we identify verification error, i.e., the L2 difference between the weights of an approximately unlearned and a naively retrained model, as an approximate unlearning metric that should be optimized for as it subsumes a large class of other metrics. We theoretically analyze the canonical training algorithm, stochastic gradient descent (SGD), to surface the variables which are relevant to reducing the verification error of approximate unlearning for SGD. From this analysis, we first derive an easy-to-compute proxy for verification error (termed unlearning error). The analysis also informs the design of a new training objective penalty that limits the overall change in weights during SGD and as a result facilitates approximate unlearning with lower verification error. We validate our theoretical work through an empirical evaluation on learning with CIFAR-10, CIFAR-100, and IMDB sentiment analysis.",https://arxiv.org/abs/2209.02299
Hard to Forget: Poisoning Attacks on Certified Machine Unlearning,"Neil G.Marchant, Benjamin I. P.Rubinstein, ScottAlfeld","17 Sep 2021 (v1(https://arxiv.org/abs/2109.08266v1)), lastrevised 10 Feb 2022 (this version, v2)",Machine Learning (cs.LG)," The right to erasure requires removal of a user's information from data held by organizations, with rigorous interpretations extending to downstream products such as learned models. Retraining from scratch with the particular user's data omitted fully removes its influence on the resulting model, but comes with a high computational cost. Machine ""unlearning"" mitigates the cost incurred by full retraining: instead, models are updated incrementally, possibly only requiring retraining when approximation errors accumulate. Rapid progress has been made towards privacy guarantees on the indistinguishability of unlearned and retrained models, but current formalisms do not place practical bounds on computation. In this paper we demonstrate how an attacker can exploit this oversight, highlighting a novel attack surface introduced by machine unlearning. We consider an attacker aiming to increase the computational cost of data removal. We derive and empirically investigate a poisoning attack on certified machine unlearning where strategically designed training data triggers complete retraining when removed.",https://arxiv.org/abs/2109.13398
You Are What You Use: Usage-based Profiling in IoT Environments,"MananChoksi, DipankarChaki, AbdallahLakhdari, AthmanBouguettaya",06-sep-22,Human-Computer Interaction (cs.HC)," Habit extraction is essential to automate services and provide appliance usage insights in the smart home environment. However, habit extraction comes with plenty of challenges in viewing typical start and end times for particular activities. This paper introduces a novel way of identifying habits using an ensemble of unsupervised clustering techniques. We use different clustering algorithms to extract habits based on how static or dynamic they are. Silhouette coefficients and a novel noise metric are utilized to extract habits appropriately. Furthermore, we associate the extracted habits with time intervals and a confidence score to denote how confident we are that a habit is likely to occur at that time.",https://arxiv.org/abs/2109.08266
SIND: A Drone Dataset at Signalized Intersection in China,"YanchaoXu, WenboShao, JunLi, KaiYang, WeidaWang, HuaHuang, ChenLv, HongWang",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Intersection is one of the most challenging scenarios for autonomous driving tasks. Due to the complexity and stochasticity, essential applications (e.g., behavior modeling, motion prediction, safety validation, etc.) at intersections rely heavily on data-driven techniques. Thus, there is an intense demand for trajectory datasets of traffic participants (TPs) in intersections. Currently, most intersections in urban areas are equipped with traffic lights. However, there is not yet a large-scale, high-quality, publicly available trajectory dataset for signalized intersections. Therefore, in this paper, a typical two-phase signalized intersection is selected in Tianjin, China. Besides, a pipeline is designed to construct a Signalized INtersection Dataset (SIND), which contains 7 hours of recording including over 13,000 TPs with 7 types. Then, the behaviors of traffic light violations in SIND are recorded. Furthermore, the SIND is also compared with other similar works. The features of the SIND can be summarized as follows: 1) SIND provides more comprehensive information, including traffic light states, motion parameters, High Definition (HD) map, etc. 2) The category of TPs is diverse and characteristic, where the proportion of vulnerable road users (VRUs) is up to 62.6% 3) Multiple traffic light violations of non- motor vehicles are shown. We believe that SIND would be an effective supplement to existing datasets and can promote related research on autonomous driving.The dataset is available online via: [this https URL](https://github.com/SOTIF-AVLab/SinD)",https://arxiv.org/abs/2209.02298
High Dynamic Range Image Quality Assessment Based on FrequencyDisparity,"YueLiu, ZhangkaiNi, ShiqiWang, HanliWang, SamKwong",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In this paper, a novel and effective image quality assessment (IQA) algorithm based on frequency disparity for high dynamic range (HDR) images is proposed, termed as local-global frequency feature-based model (LGFM). Motivated by the assumption that the human visual system is highly adapted for extracting structural information and partial frequencies when perceiving the visual scene, the Gabor and the Butterworth filters are applied to the luminance of the HDR image to extract local and global frequency features, respectively. The similarity measurement and feature pooling are sequentially performed on the frequency features to obtain the predicted quality score. The experiments evaluated on four widely used benchmarks demonstrate that the proposed LGFM can provide a higher consistency with the subjective perception compared with the state-of-the- art HDR IQA methods. Our code is available at: \url{[this https URL](https://github.com/eezkni/LGFM)}.",https://arxiv.org/abs/2209.02297
Compatibility checking of multiple control barrier functions for inputconstrained systems,"XiaoTan, DimosV.Dimarogonas",06-sep-22,Systems and Control (eess.SY)," State and input constraints are ubiquitous in control system design. One recently developed tool to deal with these constraints is control barrier functions (CBF) which transform state constraints into conditions in the input space. CBF-based controller design thus incorporates both the CBF conditions and input constraints in a quadratic program. However, the CBF-based controller is well-defined only if the CBF conditions are compatible. In the case of perturbed systems, robust compatibility is of relevance. In this work, we propose an algorithmic solution to verify or falsify the (robust) compatibility of given CBFs a priori. Leveraging the Lipschitz properties of the CBF conditions, a grid sampling and refinement method with theoretical analysis and guarantees is proposed.",https://arxiv.org/abs/2209.02285
Progressive Glass Segmentation,"LetianYu, HaiyangMei, WenDong, ZiqiWei, LiZhu, YuxinWang, XinYang",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Glass is very common in the real world. Influenced by the uncertainty about the glass region and the varying complex scenes behind the glass, the existence of glass poses severe challenges to many computer vision tasks, making glass segmentation as an important computer vision task. Glass does not have its own visual appearances but only transmit/reflect the appearances of its surroundings, making it fundamentally different from other common objects. To address such a challenging task, existing methods typically explore and combine useful cues from different levels of features in the deep network. As there exists a characteristic gap between level-different features, i.e., deep layer features embed more high-level semantics and are better at locating the target objects while shallow layer features have larger spatial sizes and keep richer and more detailed low-level information, fusing these features naively thus would lead to a sub-optimal solution. In this paper, we approach the effective features fusion towards accurate glass segmentation in two steps. First, we attempt to bridge the characteristic gap between different levels of features by developing a Discriminability Enhancement (DE) module which enables level-specific features to be a more discriminative representation, alleviating the features incompatibility for fusion. Second, we design a Focus-and-Exploration Based Fusion (FEBF) module to richly excavate useful information in the fusion process by highlighting the common and exploring the difference between level-different features.",https://arxiv.org/abs/2209.02284
Automated Defect Recognition of Castings defects using Neural Networks,"Alberto GarcÃ­a-PÃ©rez, MarÃ­a JosÃ© GÃ³mez-Silva, Arturo de laEscalera",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Industrial X-ray analysis is common in aerospace, automotive or nuclear industries where structural integrity of some parts needs to be guaranteed. However, the interpretation of radiographic images is sometimes difficult and may lead to two experts disagree on defect classification. The Automated Defect Recognition (ADR) system presented herein will reduce the analysis time and will also help reducing the subjective interpretation of the defects while increasing the reliability of the human inspector. Our Convolutional Neural Network (CNN) model achieves 94.2\% accuracy (mAP@IoU=50\%), which is considered as similar to expected human performance, when applied to an automotive aluminium castings dataset (GDXray), exceeding current state of the art for this dataset. On an industrial environment, its inference time is less than 400 ms per DICOM image, so it can be installed on production facilities with no impact on delivery time. In addition, an ablation study of the main hyper-parameters to optimise model accuracy from the initial baseline result of 75\% mAP up to 94.2\% mAP, was also conducted.",https://arxiv.org/abs/2209.02280
Zero-shot Aspect-level Sentiment Classification via ExplicitUtilization of Aspect-to-Document Sentiment Composition,"PengfeiDeng, JianhuaYuan, YanyanZhao, BingQin",06-sep-22,Computation and Language (cs.CL)," As aspect-level sentiment labels are expensive and labor-intensive to acquire, zero-shot aspect-level sentiment classification is proposed to learn classifiers applicable to new domains without using any annotated aspect-level data. In contrast, document-level sentiment data with ratings are more easily accessible. In this work, we achieve zero-shot aspect-level sentiment classification by only using document-level reviews. Our key intuition is that the sentiment representation of a document is composed of the sentiment representations of all the aspects of that document. Based on this, we propose the AF-DSC method to explicitly model such sentiment composition in reviews. AF-DSC first learns sentiment representations for all potential aspects and then aggregates aspect-level sentiments into a document-level one to perform document-level sentiment classification. In this way, we obtain the aspect-level sentiment classifier as the by-product of the document-level sentiment classifier. Experimental results on aspect- level sentiment classification benchmarks demonstrate the effectiveness of explicit utilization of sentiment composition in document-level sentiment classification. Our model with only 30k training data outperforms previous work utilizing millions of data.",https://arxiv.org/abs/2209.02279
Multi-class Classifier based Failure Prediction with Artificial andAnonymous Training for Data Privacy,"DibakarDas, VikramSeshasai, Vineet SudhirBhat, PushkalJuneja, JyotsnaBapat, Debabrata Das",06-sep-22,Artificial Intelligence (cs.AI)," This paper proposes a novel non-intrusive system failure prediction technique using available information from developers and minimal information from raw logs (rather than mining entire logs) but keeping the data entirely private with the data owners. A neural network based multi- class classifier is developed for failure prediction, using artificially generated anonymous data set, applying a combination of techniques, viz., genetic algorithm (steps), pattern repetition, etc., to train and test the network. The proposed mechanism completely decouples the data set used for training process from the actual data which is kept private. Moreover, multi-criteria decision making (MCDM) schemes are used to prioritize failures meeting business requirements. Results show high accuracy in failure prediction under different parameter configurations. On a broader context, any classification problem, beyond failure prediction, can be performed using the proposed mechanism with artificially generated data set without looking into the actual data as long as the input features can be translated to binary values (e.g. output from private binary classifiers) and can provide classification-as-a-service.",https://arxiv.org/abs/2209.02276
An Indoor Localization Dataset and Data Collection Framework with HighPrecision Position Annotation,"F. SerhanDaniÅŸ, A. TeomanNaskali, A. TaylanCemgil, Cem Ersoy",06-sep-22,Machine Learning (cs.LG)," We introduce a novel technique and an associated high resolution dataset that aims to precisely evaluate wireless signal based indoor positioning algorithms. The technique implements an augmented reality (AR) based positioning system that is used to annotate the wireless signal parameter data samples with high precision position data. We track the position of a practical and low cost navigable setup of cameras and a Bluetooth Low Energy (BLE) beacon in an area decorated with AR markers. We maximize the performance of the AR-based localization by using a redundant number of markers. Video streams captured by the cameras are subjected to a series of marker recognition, subset selection and filtering operations to yield highly precise pose estimations. Our results show that we can reduce the positional error of the AR localization system to a rate under 0.05 meters. The position data are then used to annotate the BLE data that are captured simultaneously by the sensors stationed in the environment, hence, constructing a wireless signal data set with the ground truth, which allows a wireless signal based localization system to be evaluated accurately.",https://arxiv.org/abs/2209.02275
Entity Aware Syntax Tree Based Data Augmentation for Natural LanguageUnderstanding,"JiaxingXu, JianbinCui, JiangnengLi, WengeRong, NoboruMatsuda",06-sep-22,Computation and Language (cs.CL)," Understanding the intention of the users and recognizing the semantic entities from their sentences, aka natural language understanding (NLU), is the upstream task of many natural language processing tasks. One of the main challenges is to collect a sufficient amount of annotated data to train a model. Existing research about text augmentation does not abundantly consider entity and thus performs badly for NLU tasks. To solve this problem, we propose a novel NLP data augmentation technique, Entity Aware Data Augmentation (EADA), which applies a tree structure, Entity Aware Syntax Tree (EAST), to represent sentences combined with attention on the entity. Our EADA technique automatically constructs an EAST from a small amount of annotated data, and then generates a large number of training instances for intent detection and slot filling. Experimental results on four datasets showed that the proposed technique significantly outperforms the existing data augmentation methods in terms of both accuracy and generalization ability.",https://arxiv.org/abs/2209.02270
The Simulator-in-the-Loop approach for vehicle dynamics control,"FedericoDettÃ¹, SimoneFormentin, Sergio MatteoSavaresi",06-sep-22,Systems and Control (eess.SY)," In vehicle dynamics control, engineering a suitable regulator is a long and costly process. The starting point is usually the design of a nominal controller based on a simple controloriented model and its testing on a full-fledged simulator. Then, many driving hours are required during the End-of-Line (EoL) tuning phase to calibrate the controller for the real vehicle. Given the recent technological advances, in this paper we consider the pioneering perspective where the simulator can be run on-board in the electronic control unit, so as to calculate the nominal control action in real-time. In this way, it can be shown that, in the EoL phase, we only need to tune a simple compensator of the mismatch between the expected and the measured outputs. The resulting approach not only exploits the already available simulator and nominal controller and significantly simplifies the design process, but also outperforms the state-of-the-art in terms of tracking accuracy and robustness within a challenging active braking control case study.",https://arxiv.org/abs/2209.02267
Computing the linear hull: Deciding Sequential? and Unambiguous? forweighted automata over fields,"Jason P.Bell, DanielSmertnig",06-sep-22,Formal Languages and Automata Theory (cs.FL)," The (left) linear hull of a weighted automaton over a field is a topological invariant. If the automaton is minimal, the linear hull can be used to determine whether or not the automaton is equivalent to a sequential (deterministic) automaton, respectively, an unambiguous automaton. We show how to compute the linear hull, and thus prove that it is decidable whether or not a given automaton over a (finitely generated) field is equivalent to a sequential [unambiguous] one. In these cases we are also able to compute an equivalent sequential [unambiguous] automaton. The results extend to some commutative domains, in particular PIDs. This resolves a problem posed in a 2006 survey by Lombardy and Sakarovitch.",https://arxiv.org/abs/2209.02263
Faster federated optimization under second-order similarity,"AhmedKhaled, ChiJin",06-sep-22,Machine Learning (cs.LG)," Federated learning (FL) is a subfield of machine learning where multiple clients try to collaboratively learn a model over a network under communication constraints. We consider finite-sum federated optimization under a second-order function similarity condition and strong convexity, and propose two new algorithms: SVRP and Catalyzed SVRP. This second-order similarity condition has grown popular recently, and is satisfied in many applications including distributed statistical learning and differentially private empirical risk minimization. The first algorithm, SVRP, combines approximate stochastic proximal point evaluations, client sampling, and variance reduction. We show that SVRP is communication efficient and achieves superior performance to many existing algorithms when function similarity is high enough. Our second algorithm, Catalyzed SVRP, is a Catalyst-accelerated variant of SVRP that achieves even better performance and uniformly improves upon existing algorithms for federated optimization under second-order similarity and strong convexity. In the course of analyzing these algorithms, we provide a new analysis of the Stochastic Proximal Point Method (SPPM) that might be of independent interest. Our analysis of SPPM is simple, allows for approximate proximal point evaluations, does not require any smoothness assumptions, and shows a clear benefit in communication complexity over ordinary distributed stochastic gradient descent.",https://arxiv.org/abs/2209.02260
Making the black-box brighter: interpreting machine learning algorithmfor forecasting drilling accidents,"EkaterinaGurina, NikitaKlyuchnikov, KseniaAntipova, DmitryKoroteev",06-sep-22,Machine Learning (cs.LG)," We present an approach for interpreting a black-box alarming system for forecasting accidents and anomalies during the drilling of oil and gas wells. The interpretation methodology aims to explain the local behavior of the accident predictive model to drilling engineers. The explanatory model uses Shapley additive explanations analysis of features, obtained through Bag-of-features representation of telemetry logs used during the drilling accident forecasting phase. Validation shows that the explanatory model has 15% precision at 70% recall, and overcomes the metric values of a random baseline and multi-head attention neural network. These results justify that the developed explanatory model is better aligned with explanations of drilling engineers, than the state-of-the-art method. The joint performance of explanatory and Bag-of-features models allows drilling engineers to understand the logic behind the system decisions at the particular moment, pay attention to highlighted telemetry regions, and correspondingly, increase the trust level in the accident forecasting alarms.",https://arxiv.org/abs/2209.02257
External Knowledge Selection with Weighted Negative Sampling inKnowledge-grounded Task-oriented Dialogue Systems,"JanghoonHan, JoongboShin, HosungSong, HyunjikJo, GyeonghunKim, YireunKim, StanleyJungkyu Choi",06-sep-22,Computation and Language (cs.CL)," Constructing a robust dialogue system on spoken conversations bring more challenge than written conversation. In this respect, DSTC10-Track2-Task2 is proposed, which aims to build a task-oriented dialogue (TOD) system incorporating unstructured external knowledge on a spoken conversation, extending DSTC9-Track1. This paper introduces our system containing four advanced methods: data construction, weighted negative sampling, post-training, and style transfer. We first automatically construct a large training data because DSTC10-Track2 does not release the official training set. For the knowledge selection task, we propose weighted negative sampling to train the model more fine-grained manner. We also employ post-training and style transfer for the response generation task to generate an appropriate response with a similar style to the target response. In the experiment, we investigate the effect of weighted negative sampling, post-training, and style transfer. Our model ranked 7 out of 16 teams in the objective evaluation and 6 in human evaluation.",https://arxiv.org/abs/2209.02256
Spatio-Temporal Action Detection Under Large Motion,"GurkirtSingh, VasileiosChoutas, Suman Saha, Fisher Yu, LucVan Gool",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Current methods for spatiotemporal action tube detection often extend a bounding box proposal at a given keyframe into a 3D temporal cuboid and pool features from nearby frames. However, such pooling fails to accumulate meaningful spatiotemporal features if the position or shape of the actor shows large 2D motion and variability through the frames, due to large camera motion, large actor shape deformation, fast actor action and so on. In this work, we aim to study the performance of cuboid-aware feature aggregation in action detection under large action. Further, we propose to enhance actor feature representation under large motion by tracking actors and performing temporal feature aggregation along the respective tracks. We define the actor motion with intersection-over-union (IoU) between the boxes of action tubes/tracks at various fixed time scales. The action having a large motion would result in lower IoU over time, and slower actions would maintain higher IoU. We find that track-aware feature aggregation consistently achieves a large improvement in action detection performance, especially for actions under large motion compared to the cuboid-aware baseline. As a result, we also report state-of-the-art on the large-scale MultiSports dataset.",https://arxiv.org/abs/2209.02251
Machine Learning For Classification Of Antithetical Emotional States,"JeevanshiSharma, RajatMaheshwari, Yusuf UzzamanKhan",06-sep-22,Machine Learning (cs.LG)," Emotion Classification through EEG signals has achieved many advancements. However, the problems like lack of data and learning the important features and patterns have always been areas with scope for improvement both computationally and in prediction accuracy. This works analyses the baseline machine learning classifiers' performance on DEAP Dataset along with a tabular learning approach that provided state-of-the- art comparable results leveraging the performance boost due to its deep learning architecture without deploying heavy neural networks.",https://arxiv.org/abs/2209.02250
The mpEDMD Algorithm for Data-Driven Computations of Measure-Preserving Dynamical Systems,Matthew J.Colbrook,06-sep-22,Numerical Analysis (math.NA)," Koopman operators globally linearize nonlinear dynamical systems and their spectral information is a powerful tool for the analysis and decomposition of nonlinear dynamical systems. However, Koopman operators are infinite-dimensional, and computing their spectral information is a considerable challenge. We introduce measure-preserving extended dynamic mode decomposition ($\texttt{mpEDMD}$), the first truncation method whose eigendecomposition converges to the spectral quantities of Koopman operators for general measure-preserving dynamical systems. $\texttt{mpEDMD}$ is a data-driven algorithm based on an orthogonal Procrustes problem that enforces measure-preserving truncations of Koopman operators using a general dictionary of observables. It is flexible and easy to use with any pre- existing DMD-type method, and with different types of data. We prove convergence of $\texttt{mpEDMD}$ for projection-valued and scalar-valued spectral measures, spectra, and Koopman mode decompositions. For the case of delay embedding (Krylov subspaces), our results include the first convergence rates of the approximation of spectral measures as the size of the dictionary increases. We demonstrate $\texttt{mpEDMD}$ on a range of challenging examples, its increased robustness to noise compared with other DMD-type methods, and its ability to capture the energy conservation and cascade of experimental measurements of a turbulent boundary layer flow with Reynolds number $ 6\times 10^4$ and state-space dimension $10^5$.",https://arxiv.org/abs/2209.02249
PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer TowardsVideo Object Detection,"HanWang, JunTang, Xiaodong Liu, Shanyan Guan, Rong Xie, LiSong",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Recent years have witnessed a trend of applying context frames to boost the performance of object detection as video object detection. Existing methods usually aggregate features at one stroke to enhance the feature. These methods, however, usually lack spatial information from neighboring frames and suffer from insufficient feature aggregation. To address the issues, we perform a progressive way to introduce both temporal information and spatial information for an integrated enhancement. The temporal information is introduced by the temporal feature aggregation model (TFAM), by conducting an attention mechanism between the context frames and the target frame (i.e., the frame to be detected). Meanwhile, we employ a Spatial Transition Awareness Model (STAM) to convey the location transition information between each context frame and target frame. Built upon a transformer-based detector DETR, our PTSEFormer also follows an end-to-end fashion to avoid heavy post-processing procedures while achieving 88.1% mAP on the ImageNet VID dataset. Codes are available at [this https URL](https://github.com/Hon-Wong/PTSEFormer).",https://arxiv.org/abs/2209.02244
Real-Time Cattle Interaction Recognition via Triple-stream Network,"YangYang, MizukaKomatsu, Kenji Oyama, TakenaoOhkawa",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In stockbreeding of beef cattle, computer vision-based approaches have been widely employed to monitor cattle conditions (e.g. the physical, physiology, and health). To this end, the accurate and effective recognition of cattle action is a prerequisite. Generally, most existing models are confined to individual behavior that uses video-based methods to extract spatial-temporal features for recognizing the individual actions of each cattle. However, there is sociality among cattle and their interaction usually reflects important conditions, e.g. estrus, and also video-based method neglects the real-time capability of the model. Based on this, we tackle the challenging task of real-time recognizing interactions between cattle in a single frame in this paper. The pipeline of our method includes two main modules: Cattle Localization Network and Interaction Recognition Network. At every moment, cattle localization network outputs high-quality interaction proposals from every detected cattle and feeds them into the interaction recognition network with a triple-stream architecture. Such a triple-stream network allows us to fuse different features relevant to recognizing interactions. Specifically, the three kinds of features are a visual feature that extracts the appearance representation of interaction proposals, a geometric feature that reflects the spatial relationship between cattle, and a semantic feature that captures our prior knowledge of the relationship between the individual action and interaction of cattle. In addition, to solve the problem of insufficient quantity of labeled data, we pre-train the model based on self-supervised learning. Qualitative and quantitative evaluation evidences the performance of our framework as an effective method to recognize cattle interaction in real time.",https://arxiv.org/abs/2209.02242
Automatic Code Documentation Generation Using GPT-3,"Junaed YounusKhan, GiasUddin",06-sep-22,Software Engineering (cs.SE)," Source code documentation is an important artifact for efficient software development. Code documentation could greatly benefit from automation since manual documentation is often labouring, resource and time- intensive. In this paper, we employed Codex for automatic code documentation creation. Codex is a GPT-3 based model pre-trained on both natural and programming languages. We find that Codex outperforms existing techniques even with basic settings like one-shot learning (i.e., providing only one example for training). Codex achieves an overall BLEU score of 20.6 for six different programming languages (11.2% improvement over earlier state-of- the-art techniques). Thus, Codex shows promise and warrants in-depth future studies for automatic code documentation generation to support diverse development tasks.",https://arxiv.org/abs/2209.02241
A Crypto-Assisted Approach for Publishing Graph Statistics with NodeLocal Differential Privacy,"ShangLiu, YangCao, TakaoMurakami, MasatoshiYoshikawa",06-sep-22,Cryptography and Security (cs.CR)," Publishing graph statistics under node differential privacy has attracted much attention since it provides a stronger privacy guarantee than edge differential privacy. Existing works related to node differential privacy assume a trusted data curator who holds the whole graph. However, in many applications, a trusted curator is usually not available due to privacy and security issues. In this paper, for the first time, we investigate the problem of publishing the graph degree distribution under Node Local Differential privacy (Node-LDP), which does not rely on a trusted server. We propose an algorithm to publish the degree distribution with Node-LDP by exploring how to select the optimal graph projection parameter and how to execute the local graph projection. Specifically, we propose a Crypto- assisted local projection method that combines LDP and cryptographic primitives, achieving higher accuracy than our baseline PureLDP local projection method. On the other hand, we improve our baseline Node-level parameter selection by proposing an Edge-level parameter selection that preserves more neighboring information and provides better utility. Finally, extensive experiments on real-world graphs show that Edge-level local projection provides higher accuracy than Node-level local projection, and Crypto-assisted parameter selection owns the better utility than PureLDP parameter selection, improving by up to 79.8% and 57.2% respectively.",https://arxiv.org/abs/2209.02235
Compression Optimality of Asymmetric Numeral Systems,"JosefPieprzyk, Jarek Duda, MarcinPawlowski, SeyitCamtepe, ArashMahboubi, PawelMorawiecki",06-sep-22,Information Theory (cs.IT)," Compression also known as entropy coding has a rich and long history. However, a recent explosion of multimedia Internet applications (such as teleconferencing and video streaming for instance) renews an interest in fast compression that also squeezes out as much redundancy as possible. In 2009 Jarek Duda invented his asymmetric numeral system (ANS). Apart from a beautiful mathematical structure, it is very efficient and offers compression with a very low residual redundancy. ANS works well for any symbol source statistics. Besides, ANS has become a preferred compression algorithm in the IT industry. However, designing ANS instance requires a random selection of its symbol spread function. Consequently, each ANS instance offers compression with a slightly different compression rate.   The paper investigates compression optimality of ANS. It shows that ANS is optimal (i.e. the entropies of encoding and source are equal) for any symbol sources whose probability distribution is described by natural powers of 1/2. We use Markov chains to calculate ANS state probabilities. This allows us to determine ANS compression rate precisely. We present two algorithms for finding ANS instances with high compression rates. The first explores state probability approximations in order to choose ANS instances with better compression rates. The second algorithm is a probabilistic one. It finds ANS instances, whose compression rate can be made as close to the best rate as required. This is done at the expense of the number $\theta$ of internal random ``coin'' tosses. The algorithm complexity is ${\cal O}(\theta L^3)$, where $L$ is the number of ANS states. The complexity can be reduced to ${\cal O}(\theta L\log{L})$ if we use a fast matrix inversion. If the algorithm is implemented on quantum computer, its complexity becomes ${\cal O}(\theta (\log{L})^3)$.",https://arxiv.org/abs/2209.02231
Adaptive Machine Learning for Cooperative Manipulators,FarhadAghili,06-sep-22,Robotics (cs.RO)," The problem of self-tuning control of cooperative manipulators forming a closed kinematic chain in the presence of an inaccurate kinematics model is addressed using adaptive machine learning. The kinematic parameters pertaining to the relative position/orientation uncertainties of the interconnected manipulators are updated online by two cascaded estimators in order to tune a cooperative controller for achieving accurate motion tracking with minimum-norm actuation force. This technique permits accurate calibration of the relative kinematics of the involved manipulators without needing high precision end-point sensing or force measurements, and hence it is economically justified. Investigating the stability of the entire real- time estimator/controller system reveals that the convergence and stability of the adaptive control process can be ensured if i) the direction of the angular velocity vector does not remain constant over time, and ii) the initial kinematic parameter error is upper bounded by a scaler function of some known parameters. The adaptive controller is proved to be singularity- free even though the control law involves inverting the approximation of a matrix computed at the estimated parameters. Experimental results demonstrate the sensitivity of the tracking performance of the conventional inverse dynamic control scheme to kinematic inaccuracies, while the tracking error is significantly reduced by the self-tuning cooperative controller.",https://arxiv.org/abs/2209.02228
Understanding Skills for OSS Communities on GitHub,"Jenny T.Liang, ThomasZimmermann, Denae Ford",06-sep-22,Software Engineering (cs.SE)," The development of open source software (OSS) is a broad field which requires diverse skill sets. For example, maintainers help lead the project and promote its longevity, technical writers assist with documentation, bug reporters identify defects in software, and developers program the software. However, it is unknown which skills are used in OSS development as well as OSS contributors' general attitudes towards skills in OSS. In this paper, we address this gap by administering a survey to a diverse set of 455 OSS contributors. Guided by these responses as well as prior literature on software development expertise and social factors of OSS, we develop a model of skills in OSS that considers the many contexts OSS contributors work in. This model has 45 skills in the following 9 categories: technical skills, working styles, problem solving, contribution types, project-specific skills, interpersonal skills, external relations, management, and characteristics. Through a mix of qualitative and quantitative analyses, we find that OSS contributors are actively motivated to improve skills and perceive many benefits in sharing their skills with others. We then use this analysis to derive a set of design implications and best practices for those who incorporate skills into OSS tools and platforms, such as GitHub.",https://arxiv.org/abs/2209.02223
USLN: A statistically guided lightweight network for underwater imageenhancement via dual-statistic white balance and multi-color space stretch,"ZiyuanXiao, YinaHan, SusantoRahardja, Yuanliang Ma",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Underwater images are inevitably affected by color distortion and reduced contrast. Traditional statistic-based methods such as white balance and histogram stretching attempted to adjust the imbalance of color channels and narrow distribution of intensities a priori thus with limited performance. Recently, deep-learning-based methods have achieved encouraging results. However, the involved complicate architecture and high computational costs may hinder their deployment in practical constrained platforms. Inspired by above works, we propose a statistically guided lightweight underwater image enhancement network (USLN). Concretely, we first develop a dual-statistic white balance module which can learn to use both average and maximum of images to compensate the color distortion for each specific pixel. Then this is followed by a multi-color space stretch module to adjust the histogram distribution in RGB, HSI, and Lab color spaces adaptively. Extensive experiments show that, with the guidance of statistics, USLN significantly reduces the required network capacity (over98%) and achieves state-of-the-art performance. The code and relevant resources are available at [this https URL](https://github.com/deepxzy/USLN).",https://arxiv.org/abs/2209.02222
"Dynamic optical path provisioning for alien access links:architecture, demonstration, and challenges","HidekiNishizawa, Takeo Sasai, TakeruInoue, KazuyaAnazawa, Toru Mano, KeiKitamura, YoshiakiSone, TetsuroInui, KoichiTakasugi",06-sep-22,Networking and Internet Architecture (cs.NI)," With the spread of Data Center Interconnect (DCI) and local 5G, there is a growing need for dynamically established connections between customer locations through high-capacity optical links. However, link parameters such as signal power profile and amplifier gains are often unknown and have to be measured by experts, preventing dynamic path provisioning due to the time-consuming manual measurements. Although several techniques for estimating the unknown parameters of such alien access links have been proposed, no work has presented architecture and protocol that drive the estimation techniques to establish an optical path between the customer locations. Our study aims to automatically correct the capability information of customer-owned transceivers via alien access links with optimal quality of transmission (QoT). We first propose an architecture and protocol for cooperative optical path design between a customer and carrier, utilizing a state-of-the-art technique for estimating link parameters. We then implement the proposed protocol in a software-defined network (SDN) controller and white-box transponders using an open API. The experiments demonstrate that the optical path is dynamically established via alien access links in 137 seconds from the transceiver's cold start. Lastly, we discuss the QoT accuracy obtained with this method and the remaining issues.",https://arxiv.org/abs/2209.02221
Reference Resolution and Context Change in Multimodal SituatedDialogue for Exploring Data Visualizations,"AbhinavKumar, Barbara DiEugenio, AbariBhattacharya, JillianAurisano, AndrewJohnson",06-sep-22,Computation and Language (cs.CL)," Reference resolution, which aims to identify entities being referred to by a speaker, is more complex in real world settings: new referents may be created by processes the agents engage in and/or be salient only because they belong to the shared physical setting. Our focus is on resolving references to visualizations on a large screen display in multimodal dialogue; crucially, reference resolution is directly involved in the process of creating new visualizations. We describe our annotations for user references to visualizations appearing on a large screen via language and hand gesture and also new entity establishment, which results from executing the user request to create a new visualization. We also describe our reference resolution pipeline which relies on an information-state architecture to maintain dialogue context. We report results on detecting and resolving references, effectiveness of contextual information on the model, and under-specified requests for creating visualizations. We also experiment with conventional CRF and deep learning / transformer models (BiLSTM-CRF and BERT-CRF) for tagging references in user utterance text. Our results show that transfer learning significantly boost performance of the deep learning methods, although CRF still out-performs them, suggesting that conventional methods may generalize better for low resource data.",https://arxiv.org/abs/2209.02217
Deep Neural Network Augmented Wireless Channel Estimation on System onChip,"Syed Asrar ulhaq, AbdulKarimGizzini, ShaktiShrey, SumitJ. Darak, SnehSaurabh, MarwaChafii",06-sep-22,Hardware Architecture (cs.AR)," Reliable and fast channel estimation is crucial for next- generation wireless networks supporting a wide range of vehicular and low- latency services. Recently, deep learning (DL) based channel estimation is being explored as an efficient alternative to conventional least-square (LS) and linear minimum mean square error (LMMSE) based channel estimation. Unlike LMMSE, DL methods do not need prior knowledge of channel statistics. Most of these approaches have not been realized on system-on-chip (SoC), and preliminary study shows that their complexity exceeds the complexity of the entire physical layer (PHY). The high latency of DL is another concern. This paper considers the design and implementation of deep neural network (DNN) augmented LS-based channel estimation (LSDNN) on Zynq multi-processor SoC (ZMPSoC). We demonstrate the gain in performance compared to the conventional LS and LMMSE channel estimation schemes. Via software-hardware co-design, word-length optimization, and reconfigurable architectures, we demonstrate the superiority of the LSDNN architecture over the LS and LMMSE for a wide range of SNR, number of pilots, preamble types, and wireless channels. Further, we evaluate the performance, power, and area (PPA) of the LS and LSDNN application-specific integrated circuit (ASIC) implementations in 45 nm technology. We demonstrate that the word-length optimization can substantially improve PPA for the proposed architecture in ASIC implementations.",https://arxiv.org/abs/2209.02215
Multi-Armed Bandits with Self-Information Rewards,"NirWeinberger, MichalYemini",06-sep-22,Information Theory (cs.IT)," This paper introduces the informational multi-armed bandit (IMAB) model in which at each round, a player chooses an arm, observes a symbol, and receives an unobserved reward in the form of the symbol's self- information. Thus, the expected reward of an arm is the Shannon entropy of the probability mass function of the source that generates its symbols. The player aims to maximize the expected total reward associated with the entropy values of the arms played. Under the assumption that the alphabet size is known, two UCB-based algorithms are proposed for the IMAB model which consider the biases of the plug-in entropy estimator. The first algorithm optimistically corrects the bias term in the entropy estimation. The second algorithm relies on data-dependent confidence intervals that adapt to sources with small entropy values. Performance guarantees are provided by upper bounding the expected regret of each of the algorithms. Furthermore, in the Bernoulli case, the asymptotic behavior of these algorithms is compared to the Lai-Robbins lower bound for the pseudo regret. Additionally, under the assumption that the \textit{exact} alphabet size is unknown, and instead the player only knows a loose upper bound on it, a UCB- based algorithm is proposed, in which the player aims to reduce the regret caused by the unknown alphabet size in a finite time regime. Numerical results illustrating the expected regret of the algorithms presented in the paper are provided.",https://arxiv.org/abs/2209.02213
Factor Graph Accelerator for LiDAR-Inertial Odometry,"YuhuiHao, BoYu, QiangLiu, ShaoshanLiu, YuhaoZhu",06-sep-22,Robotics (cs.RO)," Factor graph is a graph representing the factorization of a probability distribution function, and has been utilized in many autonomous machine computing tasks, such as localization, tracking, planning and control etc. We are developing an architecture with the goal of using factor graph as a common abstraction for most, if not, all autonomous machine computing tasks. If successful, the architecture would provide a very simple interface of mapping autonomous machine functions to the underlying compute hardware. As a first step of such an attempt, this paper presents our most recent work of developing a factor graph accelerator for LiDAR-Inertial Odometry (LIO), an essential task in many autonomous machines, such as autonomous vehicles and mobile robots. By modeling LIO as a factor graph, the proposed accelerator not only supports multi-sensor fusion such as LiDAR, inertial measurement unit (IMU), GPS, etc., but solves the global optimization problem of robot navigation in batch or incremental modes. Our evaluation demonstrates that the proposed design significantly improves the real-time performance and energy efficiency of autonomous machine navigation systems. The initial success suggests the potential of generalizing the factor graph architecture as a common abstraction for autonomous machine computing, including tracking, planning, and control etc.",https://arxiv.org/abs/2209.02211
High Speed Rotation Estimation with Dynamic Vision Sensors,"GuangrongZhao, YiranShen, NingChen, PengfeiHu, LeiLiu, HongkaiWen",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Rotational speed is one of the important metrics to be measured for calibrating the electric motors in manufacturing, monitoring engine during car repairing, faults detection on electrical appliance and etc. However, existing measurement techniques either require prohibitive hardware (e.g., high-speed camera) or are inconvenient to use in real-world application scenarios. In this paper, we propose, EV-Tach, an event-based tachometer via efficient dynamic vision sensing on mobile devices. EV-Tach is designed as a high-fidelity and convenient tachometer by introducing dynamic vision sensor as a new sensing modality to capture the high-speed rotation precisely under various real-world scenarios. By designing a series of signal processing algorithms bespoke for dynamic vision sensing on mobile devices, EV-Tach is able to extract the rotational speed accurately from the event stream produced by dynamic vision sensing on rotary targets. According to our extensive evaluations, the Relative Mean Absolute Error (RMAE) of EV- Tach is as low as 0.03% which is comparable to the state-of-the-art laser tachometer under fixed measurement mode. Moreover, EV-Tach is robust to subtle movement of user's hand, therefore, can be used as a handheld device, where the laser tachometer fails to produce reasonable results.",https://arxiv.org/abs/2209.02207
Exploiting and Guiding User Interaction in Interactive MachineTeaching,ZhongyiZhou,06-sep-22,Human-Computer Interaction (cs.HC)," Humans are talented with the ability to perform diverse interactions in the teaching process. However, when humans want to teach AI, existing interactive systems only allow humans to perform repetitive labeling, causing an unsatisfactory teaching experience. My Ph.D. research studies Interactive Machine Teaching (IMT), an emerging field of HCI research that aims to enhance humans' teaching experience in the AI creation process. My research builds IMT systems that exploit and guide user interaction and shows that such in-depth integration of human interaction can benefit both AI models and user experience.",https://arxiv.org/abs/2209.02205
Few-Shot Document-Level Event Argument Extraction,"XianjunYang, YujieLu, LindaPetzold",06-sep-22,Computation and Language (cs.CL)," Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works mainly assume full access to rich document supervision, ignoring the fact that the argument supervision is limited in documents. To fill this gap, we present FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based on the largest document-level event extraction dataset DocEE. We first define the new problem and reconstruct the corpus by a novel N-Way-D-Doc sampling instead of the traditional N-Way-K-Shot strategy. Then we adjust the advanced document- level neural models into the few-shot setting to provide baseline results under in- and cross-domain settings. Since the argument extraction depends on the context from multiple sentences and the learning process is limited to very few examples, we find the task to be very challenging with substantively low performance. Considering FewDocAE is closely related to practical use under low-resource regimes, we hope this benchmark encourages more research in this direction. Our data and codes will be available online.",https://arxiv.org/abs/2209.02204
What to Prune and What Not to Prune at Initialization,MahamHaroon,06-sep-22,Neural and Evolutionary Computing (cs.NE)," Post-training dropout based approaches achieve high sparsity and are well established means of deciphering problems relating to computational cost and overfitting in Neural Network architectures. Contrastingly, pruning at initialization is still far behind. Initialization pruning is more efficacious when it comes to scaling computation cost of the network. Furthermore, it handles overfitting just as well as post training dropout.   In approbation of the above reasons, the paper presents two approaches to prune at initialization. The goal is to achieve higher sparsity while preserving performance. 1) K-starts, begins with k random p-sparse matrices at initialization. In the first couple of epochs the network then determines the ""fittest"" of these p-sparse matrices in an attempt to find the ""lottery ticket"" p-sparse network. The approach is adopted from how evolutionary algorithms find the best individual. Depending on the Neural Network architecture, fitness criteria can be based on magnitude of network weights, magnitude of gradient accumulation over an epoch or a combination of both. 2) Dissipating gradients approach, aims at eliminating weights that remain within a fraction of their initial value during the first couple of epochs. Removing weights in this manner despite their magnitude best preserves performance of the network. Contrarily, the approach also takes the most epochs to achieve higher sparsity. 3) Combination of dissipating gradients and kstarts outperforms either methods and random dropout consistently.   The benefits of using the provided pertaining approaches are: 1) They do not require specific knowledge of the classification task, fixing of dropout threshold or regularization parameters 2) Retraining of the model is neither necessary nor affects the performance of the p-sparse network.",https://arxiv.org/abs/2209.02203
Task-wise Sampling Convolutions for Arbitrary-Oriented ObjectDetection in Aerial Images,"ZhanchaoHuang, WeiLi, Xiang-GenXia, HaoWang, RanTao",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Arbitrary-oriented object detection (AOOD) has been widely applied to locate and classify objects with diverse orientations in remote sensing images. However, the inconsistent features for the localization and classification tasks in AOOD models may lead to ambiguity and low-quality object predictions, which constrains the detection performance. In this paper, an AOOD method called task-wise sampling convolutions (TS-Conv) is proposed. TS-Conv adaptively samples task-wise features from respective sensitive regions and maps these features together in alignment to guide a dynamic label assignment for better predictions. Specifically, sampling positions of the localization convolution in TS-Conv is supervised by the oriented bounding box (OBB) prediction associated with spatial coordinates. While sampling positions and convolutional kernel of the classification convolution are designed to be adaptively adjusted according to different orientations for improving the orientation robustness of features. Furthermore, a dynamic task-aware label assignment (DTLA) strategy is developed to select optimal candidate positions and assign labels dynamicly according to ranked task-aware scores obtained from TS-Conv. Extensive experiments on several public datasets covering multiple scenes, multimodal images, and multiple categories of objects demonstrate the effectiveness, scalability and superior performance of the proposed TS-Conv.",https://arxiv.org/abs/2209.02201
Carbon-Neutralized Task Scheduling for Green Computing Networks,"Chien-ShengYang, Chien-Chun Huang-Fu, I-KangFu",06-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," Climate change due to increasing carbon emissions by human activities has been identified as one of the most critical threat to Earth. Carbon neutralization, as a key approach to reverse climate change, has triggered the development of new regulations to enforce the economic activities toward low carbon solutions. Computing networks that enable users to process computation-intensive tasks contribute huge amount of carbon emissions due to rising energy consumption. To analyze the achievable reduction of carbon emissions by a scheduling policy, we first propose a novel virtual queueing network model that captures communication and computing procedures in networks. To adapt to highly variable and unpredictable nature of renewable energy utilized by computing networks (i.e., carbon intensity of grid varies by time and location), we propose a novel carbon-intensity based scheduling policy that dynamically schedules computation tasks over clouds via the drift-plus-penalty methodology in Lyapunov optimization. Our numerical analysis using real-world data shows that the proposed policy achieves 54% reduction on the cumulative carbon emissions for AI model training tasks compared to the queue-length based policy.",https://arxiv.org/abs/2209.02200
LRT: An Efficient Low-Light Restoration Transformer for Dark LightField Images,"ShansiZhang, NanMeng, EdmundY. Lam",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Light field (LF) images with the multi-view property have many applications, which can be severely affected by the low-light imaging. Recent learning-based methods for low-light enhancement have their own disadvantages, such as no noise suppression, complex training process and poor performance in extremely low-light conditions. Targeted on solving these deficiencies while fully utilizing the multi-view information, we propose an efficient Low-light Restoration Transformer (LRT) for LF images, with multiple heads to perform specific intermediate tasks, including denoising, luminance adjustment, refinement and detail enhancement, within a single network, achieving progressive restoration from small scale to full scale. We design an angular transformer block with a view-token scheme to model the global angular relationship efficiently, and a multi-scale window- based transformer block to encode the multi-scale local and global spatial information. To solve the problem of insufficient training data, we formulate a synthesis pipeline by simulating the major noise with the estimated noise parameters of LF camera. Experimental results demonstrate that our method can achieve superior performance on the restoration of extremely low-light and noisy LF images with high efficiency.",https://arxiv.org/abs/2209.02198
Solving the Maximum Popular Matching Problem with Matroid Constraints,"GergelyCsÃ¡ji, TamÃ¡sKirÃ¡ly, Yu Yokoi",06-sep-22,Computer Science and Game Theory (cs.GT)," We consider the problem of finding a maximum popular matching in a many-to-many matching setting with two-sided preferences and matroid constraints. This problem was proposed by Kamiyama [TCS 2020] and solved in the special case where matroids are base orderable. Utilizing a recently shown matroid exchange property, we show that the problem is tractable for arbitrary matroids.",https://arxiv.org/abs/2209.02197
Programming Autonomous Machines,"ShaoshanLiu, XiaomingLi, TongshengGeng, StephaneZuckerman, Jean-LucGaudiot",06-sep-22,Robotics (cs.RO)," One key technical challenge in the age of autonomous machines is the programming of autonomous machines, which demands the synergy across multiple domains, including fundamental computer science, computer architecture, and robotics, and requires expertise from both academia and industry. This paper discusses the programming theory and practices tied to producing real-life autonomous machines, and covers aspects from high-level concepts down to low-level code generation in the context of specific functional requirements, performance expectation, and implementation constraints of autonomous machines.",https://arxiv.org/abs/2209.02195
A Multitask Deep Learning Model for Parsing Bridge Elements andSegmenting Defect in Bridge Inspection Images,"ChenyuZhang, Muhammad MonjurulKarim, Ruwen Qin",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The vast network of bridges in the United States raises a high requirement for its maintenance and rehabilitation. The massive cost of manual visual inspection to assess the conditions of the bridges turns out to be a burden to some extent. Advanced robots have been leveraged to automate inspection data collection. Automating the segmentations of multiclass elements, as well as surface defects on the elements, in the large volume of inspection image data would facilitate an efficient and effective assessment of the bridge condition. Training separate single-task networks for element parsing (i.e., semantic segmentation of multiclass elements) and defect segmentation fails to incorporate the close connection between these two tasks in the inspection images where both recognizable structural elements and apparent surface defects are present. This paper is motivated to develop a multitask deep neural network that fully utilizes such interdependence between bridge elements and defects to boost the performance and generalization of the model. Furthermore, the effectiveness of the proposed network designs in improving the task performance was investigated, including feature decomposition, cross-talk sharing, and multi-objective loss function. A dataset with pixel-level labels of bridge elements and corrosion was developed for training and assessment of the models. Quantitative and qualitative results from evaluating the developed multitask deep neural network demonstrate that the recommended network outperforms the independent single-task networks not only in performance (2.59% higher mIoU on bridge parsing and 1.65% on corrosion segmentation) but also in computational time and implementation capability.",https://arxiv.org/abs/2209.02193
"Object-Oriented Requirements: a Unified Framework for Specifications,Scenarios and Tests","MariaNaumcheva, SophieEbersold, AlexandrNaumchev, Jean-MichelBruel, FlorianGalinier, BertrandMeyer",06-sep-22,Software Engineering (cs.SE)," A paradox of requirements specifications as dominantly practiced in the industry is that they often claim to be object-oriented (OO) but largely rely on procedural (non-OO) techniques. Use cases and user stories describe functional flows, not object types. To gain the benefits provided by object technology (such as extendibility, reusability, reliability), requirements should instead take advantage of the same data abstraction concepts -- classes, inheritance, information hiding -- as OO design and OO programs.   Many people find use cases and user stories appealing because of the simplicity and practicality of the concepts. Can we reconcile requirements with object-oriented principles and get the best of both worlds?   This article proposes a unified framework. It shows that the concept of class is general enough to describe not only ""objects"" in a narrow sense but also scenarios such as use cases and user stories and other important artifacts such as test cases and oracles.   Having a single framework opens the way to requirements that enjoy the benefits of both approaches: like use cases and user stories, they reflect the practical views of stakeholders; like object-oriented requirements, they lend themselves to evolution and reuse.",https://arxiv.org/abs/2209.02190
Being Automated or Not? Risk Identification of Occupations with GraphNeural Networks,"DaweiXu, HaoranYang, Marian-AndreiRizoiu, Guandong Xu",06-sep-22,Computers and Society (cs.CY)," The rapid advances in automation technologies, such as artificial intelligence (AI) and robotics, pose an increasing risk of automation for occupations, with a likely significant impact on the labour market. Recent social-economic studies suggest that nearly 50\% of occupations are at high risk of being automated in the next decade. However, the lack of granular data and empirically informed models have limited the accuracy of these studies and made it challenging to predict which jobs will be automated. In this paper, we study the automation risk of occupations by performing a classification task between automated and non-automated occupations. The available information is 910 occupations' task statements, skills and interactions categorised by Standard Occupational Classification (SOC). To fully utilize this information, we propose a graph-based semi-supervised classification method named \textbf{A}utomated \textbf{O}ccupation \textbf{C}lassification based on \textbf{G}raph \textbf{C}onvolutional \textbf{N}etworks (\textbf{AOC-GCN}) to identify the automated risk for occupations. This model integrates a heterogeneous graph to capture occupations' local and global contexts. The results show that our proposed method outperforms the baseline models by considering the information of both internal features of occupations and their external interactions. This study could help policymakers identify potential automated occupations and support individuals' decision-making before entering the job market.",https://arxiv.org/abs/2209.02189
Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by theBest of Both Students,"XuZheng, Yunhao Luo, Hao Wang, Chong Fu, LinWang",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The popular methods for semi-supervised semantic segmentation mostly adopt a unitary network model using convolutional neural networks (CNNs) and enforce consistency of the model predictions over small perturbations applied to the inputs or model. However, such a learning paradigm suffers from a) limited learning capability of the CNN-based model; b) limited capacity of learning the discriminative features for the unlabeled data; c) limited learning for both global and local information from the whole image. In this paper, we propose a novel Semi-supervised Learning approach, called Transformer-CNN Cohort (TCC), that consists of two students with one based on the vision transformer (ViT) and the other based on the CNN. Our method subtly incorporates the multi-level consistency regularization on the predictions and the heterogeneous feature spaces via pseudo labeling for the unlabeled data. First, as the inputs of the ViT student are image patches, the feature maps extracted encode crucial class- wise statistics. To this end, we propose class-aware feature consistency distillation (CFCD) that first leverages the outputs of each student as the pseudo labels and generates class-aware feature (CF) maps. It then transfers knowledge via the CF maps between the students. Second, as the ViT student has more uniform representations for all layers, we propose consistency- aware cross distillation to transfer knowledge between the pixel-wise predictions from the cohort. We validate the TCC framework on Cityscapes and Pascal VOC 2012 datasets, which significantly outperforms existing semi- supervised methods by a large margin.",https://arxiv.org/abs/2209.02182
CNSNet: A Cleanness-Navigated-Shadow Network for Shadow Removal,"QianhaoYu, NaishanZheng, JieHuang, FengZhao",06-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The key to shadow removal is recovering the contents of the shadow regions with the guidance of the non-shadow regions. Due to the inadequate long-range modeling, the CNN-based approaches cannot thoroughly investigate the information from the non-shadow regions. To solve this problem, we propose a novel cleanness-navigated-shadow network (CNSNet), with a shadow- oriented adaptive normalization (SOAN) module and a shadow-aware aggregation with transformer (SAAT) module based on the shadow mask. Under the guidance of the shadow mask, the SOAN module formulates the statistics from the non- shadow region and adaptively applies them to the shadow region for region- wise restoration. The SAAT module utilizes the shadow mask to precisely guide the restoration of each shadowed pixel by considering the highly relevant pixels from the shadow-free regions for global pixel-wise restoration. Extensive experiments on three benchmark datasets (ISTD, ISTD+, and SRD) show that our method achieves superior de-shadowing performance.",https://arxiv.org/abs/2209.02178
Impact analysis of recovery cases due to COVID19 using LSTM deeplearning model,"Md ErshadulHaque, Samiul Hoque",06-sep-22,Machine Learning (cs.LG)," The present world is badly affected by novel coronavirus (COVID-19). Using medical kits to identify the coronavirus affected persons are very slow. What happens in the next, nobody knows. The world is facing erratic problem and do not know what will happen in near future. This paper is trying to make prognosis of the coronavirus recovery cases using LSTM (Long Short Term Memory). This work exploited data of 258 regions, their latitude and longitude and the number of death of 403 days ranging from 22-01-2020 to 27-02-2021. Specifically, advanced deep learning-based algorithms known as the LSTM, play a great effect on extracting highly essential features for time series data (TSD) analysis.There are lots of methods which already use to analyze propagation prediction. The main task of this paper culminates in analyzing the spreading of Coronavirus across worldwide recovery cases using LSTM deep learning-based architectures.",https://arxiv.org/abs/2209.02174
White-Box Adversarial Policies in Deep Reinforcement Learning,"StephenCasper, Dylan Hadfield-Menell, GabrielKreiman",05-sep-22,Artificial Intelligence (cs.AI)," Adversarial examples against AI systems pose both risks via malicious attacks and opportunities for improving robustness via adversarial training. In multiagent settings, adversarial policies can be developed by training an adversarial agent to minimize a victim agent's rewards. Prior work has studied black-box attacks where the adversary only sees the state observations and effectively treats the victim as any other part of the environment. In this work, we experiment with white-box adversarial policies to study whether an agent's internal state can offer useful information for other agents. We make three contributions. First, we introduce white-box adversarial policies in which an attacker can observe a victim's internal state at each timestep. Second, we demonstrate that white-box access to a victim makes for better attacks in two-agent environments, resulting in both faster initial learning and higher asymptotic performance against the victim. Third, we show that training against white-box adversarial policies can be used to make learners in single-agent environments more robust to domain shifts.",https://arxiv.org/abs/2209.02173
To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing,"LucaBallotta, GiovanniPeserico, FrancescoZanini, Paolo Dini",05-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," We consider a network of smart sensors for edge computing application that sample a signal of interest and send updates to a base station for remote global monitoring. Sensors are equipped with sensing and compute, and can either send raw data or process them on-board before transmission. Limited hardware resources at the edge generate a fundamental latency-accuracy trade-off: raw measurements are inaccurate but timely, whereas accurate processed updates are available after computational delay. Also, if sensor on-board processing entails data compression, latency caused by wireless communication might be higher for raw measurements. Hence, one needs to decide when sensors should transmit raw measurements or rely on local processing to maximize overall network performance. To tackle this sensing design problem, we model an estimation-theoretic optimization framework that embeds computation and communication delays, and propose a Reinforcement Learning-based approach to dynamically allocate computational resources at each sensor. Effectiveness of our proposed approach is validated through numerical simulations with case studies motivated by the Internet of Drones and self-driving vehicles.",https://arxiv.org/abs/2209.02167
A Reinforcement Learning Approach to Sensing Design in Resource-Constrained Wireless Networked Control Systems,"LucaBallotta, GiovanniPeserico, FrancescoZanini","1 Apr 2022 (v1(https://arxiv.org/abs/2204.00703v1)), lastrevised 5 Aug 2022 (this version, v4)",Systems and Control (eess.SY)," In this paper, we consider a wireless network of smart sensors (agents) that monitor a dynamical process and send measurements to a base station that performs global monitoring and decision-making. Smart sensors are equipped with both sensing and computation, and can either send raw measurements or process them prior to transmission. Constrained agent resources raise a fundamental latency-accuracy trade-off. On the one hand, raw measurements are inaccurate but fast to produce. On the other hand, data processing on resource-constrained platforms generates accurate measurements at the cost of non-negligible computation latency. Further, if processed data are also compressed, latency caused by wireless communication might be higher for raw measurements. Hence, it is challenging to decide when and where sensors in the network should transmit raw measurements or leverage time-consuming local processing. To tackle this design problem, we propose a Reinforcement Learning approach to learn an efficient policy that dynamically decides when measurements are to be processed at each sensor. Effectiveness of our proposed approach is validated through a numerical simulation with case study on smart sensing motivated by the Internet of Drones.",https://arxiv.org/abs/2209.02166
Reinforcement learning-based optimised control for tracking ofnonlinear systems with adversarial attacks,"FarshadRahimi, SepidehZiaei",05-sep-22,Systems and Control (eess.SY)," This paper introduces a reinforcement learning-based tracking control approach for a class of nonlinear systems using neural networks. In this approach, adversarial attacks were considered both in the actuator and on the outputs. This approach incorporates a simultaneous tracking and optimization process. It is necessary to be able to solve the Hamilton- Jacobi-Bellman equation (HJB) in order to obtain optimal control input, but this is difficult due to the strong nonlinearity terms in the equation. In order to find the solution to the HJB equation, we used a reinforcement learning approach. In this online adaptive learning approach, three neural networks are simultaneously adapted: the critic neural network, the actor neural network, and the adversary neural network. Ultimately, simulation results are presented to demonstrate the effectiveness of the introduced method on a manipulator.",https://arxiv.org/abs/2204.00703
Comparative Study of AR Versus Image and Video for Exercise Learning,"JamieBurns, WengeXu, IanWilliams, IrfanKhawaja",05-sep-22,Human-Computer Interaction (cs.HC)," There is inadequate attention to using mobile Augmented Reality (AR) in fitness, despite mobile AR being easy to use, requiring no extra cost, and can be a powerful learning tool. In this work, we present a mobile AR application that can help users learn exercises with a virtual personal trainer. We conduct a user study with 10 participants to investigate the learning quality of the ARFit (i.e., the proposed mobile AR application) in comparison to traditional methods such as Image-based learning and Video- based learning. Our results indicate that participants have a higher learning quality of exercise with mobile AR than (1) Image-based learning among all exercises selected and (2) video-based learning with exercise that requires greater spatial knowledge, with the performance evaluated by a qualified personal trainer. In addition, ARFit has an excellent rating in usability, is deemed to be highly acceptable, and is the preferred exercise learning method by most participants (N=9)",https://arxiv.org/abs/2209.02165
Improving Assistive Robotics with Deep Reinforcement Learning,"YashJakhotiya, Iman Haque",05-sep-22,Robotics (cs.RO)," Assistive Robotics is a class of robotics concerned with aiding humans in daily care tasks that they may be inhibited from doing due to disabilities or age. While research has demonstrated that classical control methods can be used to design policies to complete these tasks, these methods can be difficult to generalize to a variety of instantiations of a task. Reinforcement learning can provide a solution to this issue, wherein robots are trained in simulation and their policies are transferred to real- world machines. In this work, we replicate a published baseline for training robots on three tasks in the Assistive Gym environment, and we explore the usage of a Recurrent Neural Network and Phasic Policy Gradient learning to augment the original work. Our baseline implementation meets or exceeds the baseline of the original work, however, we found that our explorations into the new methods was not as effective as we anticipated. We discuss the results of our baseline, and some thoughts on why our new methods were not successful.",https://arxiv.org/abs/2209.02161
Recommender Systems and Algorithmic Hate,"Jessie J.Smith, Lucia Jayne, Robin Burke",05-sep-22,Human-Computer Interaction (cs.HC)," Despite increasing reliance on personalization in digital platforms, many algorithms that curate content or information for users have been met with resistance. When users feel dissatisfied or harmed by recommendations, this can lead users to hate, or feel negatively towards these personalized systems. Algorithmic hate detrimentally impacts both users and the system, and can result in various forms of algorithmic harm, or in extreme cases can lead to public protests against ''the algorithm'' in question. In this work, we summarize some of the most common causes of algorithmic hate and their negative consequences through various case studies of personalized recommender systems. We explore promising future directions for the RecSys research community that could help alleviate algorithmic hate and improve the relationship between recommender systems and their users.",https://arxiv.org/abs/2209.02160
Spatial Parquet: A Column File Format for Geospatial Data Lakes[Extended Version],"MajidSaeedan, AhmedEldawy",05-sep-22,Databases (cs.DB)," Modern data analytics applications prefer to use column-storage formats due to their improved storage efficiency through encoding and compression. Parquet is the most popular file format for column data storage that provides several of these benefits out of the box. However, geospatial data is not readily supported by Parquet. This paper introduces Spatial Parquet, a Parquet extension that efficiently supports geospatial data. Spatial Parquet inherits all the advantages of Parquet for non-spatial data, such as rich data types, compression, and column/row filtering. Additionally, it adds three new features to accommodate geospatial data. First, it introduces a geospatial data type that can encode all standard spatial data types in a column format compatible with Parquet. Second, it adds a new lossless and efficient encoding method, termed FP-delta, that is customized to efficiently store geospatial coordinates stored in floating- point format. Third, it adds a light-weight spatial index that allows the reader to skip non-relevant parts of the file for increased read efficiency. Experiments on large-scale real data showed that SpatialParquet can reduce the data size by a factor of three, even without compression. Compression can further reduce the storage size. Additionally, Spatial Parquet can reduce the reading time by two orders of magnitude when the light-weight index is applied. This initial prototype can open new research directions to further improve geospatial data storage in column format.",https://arxiv.org/abs/2209.02159
A New Approach to Training Multiple Cooperative Agents for AutonomousDriving,"RuiyangYang, SihengLi, BeihongJin",05-sep-22,Artificial Intelligence (cs.AI)," Training multiple agents to perform safe and cooperative control in the complex scenarios of autonomous driving has been a challenge. For a small fleet of cars moving together, this paper proposes Lepus, a new approach to training multiple agents. Lepus adopts a pure cooperative manner for training multiple agents, featured with the shared parameters of policy networks and the shared reward function of multiple agents. In particular, Lepus pre-trains the policy networks via an adversarial process, improving its collaborative decision-making capability and further the stability of car driving. Moreover, for alleviating the problem of sparse rewards, Lepus learns an approximate reward function from expert trajectories by combining a random network and a distillation network. We conduct extensive experiments on the MADRaS simulation platform. The experimental results show that multiple agents trained by Lepus can avoid collisions as many as possible while driving simultaneously and outperform the other four methods, that is, DDPG-FDE, PSDDPG, MADDPG, and MAGAIL(DDPG) in terms of stability.",https://arxiv.org/abs/2209.02158
Adaptive Visual Servo Control for Autonomous Robots,FarhadAghili,05-sep-22,Robotics (cs.RO)," This paper focuses on an adaptive and fault-tolerant vision-guided robotic system that enables to choose the most appropriate control action if partial or complete failure of the vision system in the short term occurs. Moreover, the autonomous robotic system takes physical and operational constraints into account to perform the demands of a specific visual servoing task in a way to minimize a cost function. A hierarchical control architecture is developed based on interwoven integration of a variant of the iterative closest point (ICP) image registration, a constrained noise- adaptive Kalman filter, a fault detection logic and recovery, together with a constrained optimal path planner. The dynamic estimator estimates unknown states and uncertain parameters required for motion prediction while imposing a set of inequality constraints for consistency of the estimation process and adjusting adaptively the Kalman filter parameters in the face of unexpected vision errors. It is followed by the implementation of a fault recovery strategy based on a fault detection logic that monitors the health of the visual feedback using the metric fit error of the image registration. Subsequently, the estimated/predicted pose and parameters are passed to an optimal path planner in order to bring the robot end-effector to the grasping point of a moving target as quickly as possible subject to multiple constraints such as acceleration limit, smooth capture, and line-of-sight angle of the target.",https://arxiv.org/abs/2209.02157
Random Initialization Solves Shapley's Fictitious Play Counterexample,SamGanzfried,05-sep-22,Computer Science and Game Theory (cs.GT), In 1964 Shapley devised a family of games for which fictitious play fails to converge to Nash equilibrium. The games are two-player non- zero-sum with 3 pure strategies per player. Shapley assumed that each player played a specific pure strategy in the first round. We show that if we use random (mixed) strategy profile initializations we are able to converge to Nash equilibrium approximately 1/3 of the time for a representative game in this class.,https://arxiv.org/abs/2209.02156
Cooperative Predictive Cruise Control: A Bargaining Game Approach,"Miguel F. Arevalo-Castiblanco, JaimePachon, Duvan Tellez-Castro, Eduardo Mojica-Nava",05-sep-22,Systems and Control (eess.SY)," This paper considers a cooperative cruise control problem from a predictive control perspective. Online decision-making is used to be executed during the driving process based on the information obtained from the network. We formalize a synchronization problem approach from a predictive control theory using bargaining games to find an operating agreement between the vehicles. Finally, we test these results in an emulation environment in a hardware-in-the-loop system.",https://arxiv.org/abs/2209.02154
Learning Canonical Embeddings for Unsupervised Shape Correspondencewith Locally Linear Transformations,"Pan He, PatrickEmami, SanjayRanka, AnandRangarajan","5 Sep 2022 (v1(https://arxiv.org/abs/2209.02152v1)), lastrevised 7 Sep 2022 (this version, v2)",Computer Vision and Pattern Recognition (cs.CV)," We present a new approach to unsupervised shape correspondence learning between pairs of point clouds. We make the first attempt to adapt the classical locally linear embedding algorithm (LLE) -- originally designed for nonlinear dimensionality reduction -- for shape correspondence. The key idea is to find dense correspondences between shapes by first obtaining high-dimensional neighborhood-preserving embeddings of low- dimensional point clouds and subsequently aligning the source and target embeddings using locally linear transformations. We demonstrate that learning the embedding using a new LLE-inspired point cloud reconstruction objective results in accurate shape correspondences. More specifically, the approach comprises an end-to-end learnable framework of extracting high- dimensional neighborhood-preserving embeddings, estimating locally linear transformations in the embedding space, and reconstructing shapes via divergence measure-based alignment of probabilistic density functions built over reconstructed and target shapes. Our approach enforces embeddings of shapes in correspondence to lie in the same universal/canonical embedding space, which eventually helps regularize the learning process and leads to a simple nearest neighbors approach between shape embeddings for finding reliable correspondences. Comprehensive experiments show that the new method makes noticeable improvements over state-of-the-art approaches on standard shape correspondence benchmark datasets covering both human and nonhuman shapes.",https://arxiv.org/abs/2209.02153
A Unified Trapezoidal Quadrature Method for Singular and HypersingularBoundary Integral Operators on Curved Surfaces,"BoweiWu, Per-GunnarMartinsson",05-sep-22,Numerical Analysis (math.NA)," This paper describes a trapezoidal quadrature method for the discretization of singular and hypersingular boundary integral operators (BIOs) that arise in solving boundary value problems for elliptic partial differential equations. The quadrature is based on a uniform grid in parameter space coupled with the standard punctured Trapezoidal rule. A key observation is that the error incurred by the singularity in the kernel can be expressed exactly using generalized Euler-Maclaurin formulae that involve the Riemann zeta function in 2D and the Epstein zeta functions in 3D. These expansions are exploited to correct the errors via local stencils at the singular point using a novel systematic moment-fitting approach. This new method provides a unified treatment of all common BIOs (Laplace, Helmholtz, Stokes, etc.). We present numerical examples that show convergence of up to 32nd-order in 2D and 9th-order in 3D with respect to the mesh size.",https://arxiv.org/abs/2209.02152
Inferring Region Types via an Abstract Notion of EnvironmentTransformation,"UlrichSchÃ¶pp, Chuangjie Xu","5 Sep 2022 (v1(https://arxiv.org/abs/2209.02147v1)), lastrevised 7 Sep 2022 (this version, v2)",Programming Languages (cs.PL)," Region-based type systems are a powerful tool for various kinds of program analysis. We introduce a new inference algorithm for region types based on an abstract notion of environment transformation. It analyzes the code of a method only once, even when there are multiple invocations of the method of different region types in the program. Elements of such an abstract transformation are essentially constraints for equality and subtyping that capture flow information of the program. In particular, we work with access graphs in the definition of abstract transformations to guarantee the termination of the inference algorithm, because they provide a finite representation of field access paths.",https://arxiv.org/abs/2209.02150
Rare but Severe Neural Machine Translation Errors Induced by MinimalDeletion: An Empirical Study on Chinese and English,"RuikangShi, AlvinGrissom II, Duc MinhTrinh",05-sep-22,Computation and Language (cs.CL)," We examine the inducement of rare but severe errors in English- Chinese and Chinese-English in-domain neural machine translation by minimal deletion of the source text with character-based models. By deleting a single character, we find that we can induce severe errors in the translation. We categorize these errors and compare the results of deleting single characters and single words. We also examine the effect of training data size on the number and types of pathological cases induced by these minimal perturbations, finding significant variation.",https://arxiv.org/abs/2209.02147
A Deep Neural Network for Multiclass Bridge Element Parsing inInspection Image Analysis,"ChenyuZhang, Muhammad MonjurulKarim, Zhaozheng Yin, Ruwen Qin",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Aerial robots such as drones have been leveraged to perform bridge inspections. Inspection images with both recognizable structural elements and apparent surface defects can be collected by onboard cameras to provide valuable information for the condition assessment. This article aims to determine a suitable deep neural network (DNN) for parsing multiclass bridge elements in inspection images. An extensive set of quantitative evaluations along with qualitative examples show that High-Resolution Net (HRNet) possesses the desired ability. With data augmentation and a training sample of 130 images, a pre-trained HRNet is efficiently transferred to the task of structural element parsing and has achieved a 92.67% mean F1-score and 86.33% mean IoU.",https://arxiv.org/abs/2209.02145
Cross-Lingual and Cross-Domain Crisis Classification for Low-ResourceScenarios,"CinthiaSÃ¡nchez, HernanSarmiento, JorgePÃ©rez, AndresAbeliuk, BarbaraPoblete",05-sep-22,Computation and Language (cs.CL)," Social media data has emerged as a useful source of timely information about real-world crisis events. One of the main tasks related to the use of social media for disaster management is the automatic identification of crisis-related messages. Most of the studies on this topic have focused on the analysis of data for a particular type of event in a specific language. This limits the possibility of generalizing existing approaches because models cannot be directly applied to new types of events or other languages. In this work, we study the task of automatically classifying messages that are related to crisis events by leveraging cross- language and cross-domain labeled data. Our goal is to make use of labeled data from high-resource languages to classify messages from other (low- resource) languages and/or of new (previously unseen) types of crisis situations. For our study we consolidated from the literature a large unified dataset containing multiple crisis events and languages. Our empirical findings show that it is indeed possible to leverage data from crisis events in English to classify the same type of event in other languages, such as Spanish and Italian (80.0% F1-score). Furthermore, we achieve good performance for the cross-domain task (80.0% F1-score) in a cross-lingual setting. Overall, our work contributes to improving the data scarcity problem that is so important for multilingual crisis classification. In particular, mitigating cold-start situations in emergency events, when time is of essence.",https://arxiv.org/abs/2209.02141
"Granular Compensation, Information, and Carbon Pricing Promote DERDeployment","Hafiz Anwar UllahKhan, BurÃ§inÃœnel, YuryDvorkin",05-sep-22,Systems and Control (eess.SY)," The socially efficient deployment of Distributed Energy Resources (DERs), e.g., rooftop solar, depends on the underlying retail electricity policies. Current debates on DER policies, including Net Energy Metering (NEM) variants, center around developing value-reflective compensation policies that can expedite DER deployment while preventing potential cost shifts between DER adopters and non-adopters. However, these debates mostly ignore the temporally- and spatially- granular value of DERs, market failures (e.g., information asymmetry among DER stakeholders) and externalities (e.g., carbon-dioxide emissions). In this paper, we develop a game-theoretic approach with information asymmetry to examine efficiency implications of adopting granular DER compensation policies, e.g., value stacks and distributional locational marginal price, instead of NEM with flat retail rates. We show that granular compensation policies result in more efficient market outcomes than under NEM, even in the presence of information asymmetry, thus avoiding the need for interventions. Combined with granular DER compensation, carbon pricing provides the most accurate price signal to DER investors/aggregators, and leads to the highest social welfare.",https://arxiv.org/abs/2209.02139
Facial Expression Translation using Landmark Guided GANs,"HaoTang, NicuSebe",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We propose a simple yet powerful Landmark guided Generative Adversarial Network (LandmarkGAN) for the facial expression-to-expression translation using a single image, which is an important and challenging task in computer vision since the expression-to-expression translation is a non- linear and non-aligned problem. Moreover, it requires a high-level semantic understanding between the input and output images since the objects in images can have arbitrary poses, sizes, locations, backgrounds, and self- occlusions. To tackle this problem, we propose utilizing facial landmark information explicitly. Since it is a challenging problem, we split it into two sub-tasks, (i) category-guided landmark generation, and (ii) landmark- guided expression-to-expression translation. Two sub-tasks are trained in an end-to-end fashion that aims to enjoy the mutually improved benefits from the generated landmarks and expressions. Compared with current keypoint- guided approaches, the proposed LandmarkGAN only needs a single facial image to generate various expressions. Extensive experimental results on four public datasets demonstrate that the proposed LandmarkGAN achieves better results compared with state-of-the-art approaches only using a single image. The code is available at [this https URL](https://github.com/Ha0Tang/LandmarkGAN).",https://arxiv.org/abs/2209.02138
Impact of Scaled Image on Robustness of Deep Neural Networks,"ChengyinHu, WeiwenShi",02-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Deep neural networks (DNNs) have been widely used in computer vision tasks like image classification, object detection and segmentation. Whereas recent studies have shown their vulnerability to manual digital perturbations or distortion in the input images. The accuracy of the networks is remarkably influenced by the data distribution of their training dataset. Scaling the raw images creates out-of-distribution data, which makes it a possible adversarial attack to fool the networks. In this work, we propose a Scaling-distortion dataset ImageNet-CS by Scaling a subset of the ImageNet Challenge dataset by different multiples. The aim of our work is to study the impact of scaled images on the performance of advanced DNNs. We perform experiments on several state-of-the-art deep neural network architectures on the proposed ImageNet-CS, and the results show a significant positive correlation between scaling size and accuracy decline. Moreover, based on ResNet50 architecture, we demonstrate some tests on the performance of recent proposed robust training techniques and strategies like Augmix, Revisiting and Normalizer Free on our proposed ImageNet-CS. Experiment results have shown that these robust training techniques can improve networks' robustness to scaling transformation.",https://arxiv.org/abs/2209.02136
Evaluating the Susceptibility of Pre-Trained Language Models viaHandcrafted Adversarial Examples,"Hezekiah J.Branch, Jonathan RodriguezCefalu, JeremyMcHugh, Leyla Hujer, Aditya Bahl, Daniel del CastilloIglesias, RonHeichman, RameshDarwishi",05-sep-22,Computation and Language (cs.CL)," Recent advances in the development of large language models have resulted in public access to state-of-the-art pre-trained language models (PLMs), including Generative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT). However, evaluations of PLMs, in practice, have shown their susceptibility to adversarial attacks during the training and fine-tuning stages of development. Such attacks can result in erroneous outputs, model-generated hate speech, and the exposure of users' sensitive information. While existing research has focused on adversarial attacks during either the training or the fine-tuning of PLMs, there is a deficit of information on attacks made between these two development phases. In this work, we highlight a major security vulnerability in the public release of GPT-3 and further investigate this vulnerability in other state-of-the-art PLMs. We restrict our work to pre-trained models that have not undergone fine-tuning. Further, we underscore token distance-minimized perturbations as an effective adversarial approach, bypassing both supervised and unsupervised quality measures. Following this approach, we observe a significant decrease in text classification quality when evaluating for semantic similarity.",https://arxiv.org/abs/2209.02132
Design of the topology for contrastive visual-textual alignment,ZhunSun,05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Pre-training weakly related image-text pairs in the contrastive style shows great power in learning semantic aligning cross-modal models. The common choice to measure the distance between the feature representations of the image-text pairs is the cosine similarity, which can be considered as the negative inner product of features embedded on a sphere mathematically. While such topology benefits from the low computational resources consumption and a properly defined uniformity, typically, there are two major drawbacks when applied. First, it is vulnerable to the semantic ambiguity phenomenon resulting from the noise in the weakly-related image-text pairs. Second, the learning progress is unstable and fragile at the beginning. Although, in the practice of former studies, a learnable softmax temperature parameter and a long warmup scheme are employed to meliorate the training progress, still there lacks an in-depth analysis of these problems. In this work, we discuss the desired properties of the topology and its endowed distance function for the embedding vectors of feature representations from the view of optimization. We then propose a rather simple solution to improve the aforementioned problem. That is, we map the feature representations onto the oblique manifold endowed with the negative inner product as the distance function. In the experimental analysis, we show that we can improve the baseline performance by a large margin (e.g. 4% in the zero-shot image to text retrieval task) by changing only two lines of the training codes.",https://arxiv.org/abs/2209.02128
Utilizing Post-Hurricane Satellite Imagery to Identify Flooding Damagewith Convolutional Neural Networks,JimmyBao,05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Post-hurricane damage assessment is crucial towards managing resource allocations and executing an effective response. Traditionally, this evaluation is performed through field reconnaissance, which is slow, hazardous, and arduous. Instead, in this paper we furthered the idea of implementing deep learning through convolutional neural networks in order to classify post-hurricane satellite imagery of buildings as Flooded/Damaged or Undamaged. The experimentation was conducted employing a dataset containing post-hurricane satellite imagery from the Greater Houston area after Hurricane Harvey in 2017. This paper implemented three convolutional neural network model architectures paired with additional model considerations in order to achieve high accuracies (over 99%), reinforcing the effective use of machine learning in post-hurricane disaster assessment.",https://arxiv.org/abs/2209.02127
Class-Incremental Learning via Knowledge Amalgamation,"Marcus deCarvalho, MahardhikaPratama, Jie Zhang, Yajuan San",05-sep-22,Machine Learning (cs.LG)," Catastrophic forgetting has been a significant problem hindering the deployment of deep learning algorithms in the continual learning setting. Numerous methods have been proposed to address the catastrophic forgetting problem where an agent loses its generalization power of old tasks while learning new tasks. We put forward an alternative strategy to handle the catastrophic forgetting with knowledge amalgamation (CFA), which learns a student network from multiple heterogeneous teacher models specializing in previous tasks and can be applied to current offline methods. The knowledge amalgamation process is carried out in a single-head manner with only a selected number of memorized samples and no annotations. The teachers and students do not need to share the same network structure, allowing heterogeneous tasks to be adapted to a compact or sparse data representation. We compare our method with competitive baselines from different strategies, demonstrating our approach's advantages.",https://arxiv.org/abs/2209.02124
SR-GNN: Spatial Relation-aware Graph Neural Network for Fine-GrainedImage Categorization,"AsishBera, ZacharyWharton, Yonghuai Liu, Nik Bessis, ArdhenduBehera",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Over the past few years, a significant progress has been made in deep convolutional neural networks (CNNs)-based image recognition. This is mainly due to the strong ability of such networks in mining discriminative object pose and parts information from texture and shape. This is often inappropriate for fine-grained visual classification (FGVC) since it exhibits high intra-class and low inter-class variances due to occlusions, deformation, illuminations, etc. Thus, an expressive feature representation describing global structural information is a key to characterize an object/ scene. To this end, we propose a method that effectively captures subtle changes by aggregating context-aware features from most relevant image- regions and their importance in discriminating fine-grained categories avoiding the bounding-box and/or distinguishable part annotations. Our approach is inspired by the recent advancement in self-attention and graph neural networks (GNNs) approaches to include a simple yet effective relation-aware feature transformation and its refinement using a context- aware attention mechanism to boost the discriminability of the transformed feature in an end-to-end learning process. Our model is evaluated on eight benchmark datasets consisting of fine-grained objects and human-object interactions. It outperforms the state-of-the-art approaches by a significant margin in recognition accuracy.",https://arxiv.org/abs/2209.02112
Convexifying Market Clearing of SoC-Dependent Bids from MerchantStorage Participants,"CongChen, LangTong",05-sep-22,Systems and Control (eess.SY)," State-of-charge (SoC) dependent bidding allows merchant storage participants to incorporate SoC-dependent operation and opportunity costs in a bid-based market clearing process. However, such a bid results in a non- convex cost function in multi-interval economic dispatch and market clearing, limiting its implementation in practice. We show that a minor restriction on the bidding format removes non-convexity, making the multi- interval dispatch of SoC-dependent bids a standard convex piece-wise linear program.",https://arxiv.org/abs/2209.02109
Prediction Based Decision Making for Autonomous Highway Driving,"MustafaYildirim, SajjadMozaffari, LucMcCutcheon, MehrdadDianati, Alireza Tamaddoni-Nezhad SaberFallah",05-sep-22,Robotics (cs.RO)," Autonomous driving decision-making is a challenging task due to the inherent complexity and uncertainty in traffic. For example, adjacent vehicles may change their lane or overtake at any time to pass a slow vehicle or to help traffic flow. Anticipating the intention of surrounding vehicles, estimating their future states and integrating them into the decision-making process of an automated vehicle can enhance the reliability of autonomous driving in complex driving scenarios. This paper proposes a Prediction-based Deep Reinforcement Learning (PDRL) decision-making model that considers the manoeuvre intentions of surrounding vehicles in the decision-making process for highway driving. The model is trained using real traffic data and tested in various traffic conditions through a simulation platform. The results show that the proposed PDRL model improves the decision-making performance compared to a Deep Reinforcement Learning (DRL) model by decreasing collision numbers, resulting in safer driving.",https://arxiv.org/abs/2209.02107
Nearest-Neighbor Decompositions of Drawings,"JonasCleve, NicolasGrelier, KristinKnorr, MaartenLÃ¶ffler, WolfgangMulzer, Daniel Perz",05-sep-22,Computational Geometry (cs.CG)," Let $\mathcal{D}$ be a set of straight-line segments in the plane, potentially crossing, and let $c$ be a positive integer. We denote by $P$ the union of the endpoints of the straight-line segments of $\mathcal{D}$ and of the intersection points between pairs of segments. We say that $\mathcal{D}$ has a nearest-neighbor decomposition into $c$ parts if we can partition $P$ into $c$ point sets $P_1, \ldots, P_c$ such that $\mathcal{D}$ is the union of the nearest neighbor graphs on $P_1, \ldots, P_c$. We show that it is NP-complete to decide whether $\mathcal{D}$ can be drawn as the union of $c\geq 3$ nearest-neighbor graphs, even when no two segments cross. We show that for $c = 2$, it is NP-complete in the general setting and polynomial-time solvable when no two segments cross. We show the existence of an $O(\log n)$-approximation algorithm running in subexponential time for partitioning $\mathcal{D}$ into a minimum number of nearest-neighbor graphs.   As a main tool in our analysis, we establish the notion of the conflict graph for a drawing $\mathcal{D}$. The vertices of the conflict graph are the connected components of $\mathcal{D}$, with the assumption that each connected component is the nearest neighbor graph of its vertices, and there is an edge between two components $U$ and $V$ if and only if the nearest neighbor graph of $U \cup V$ contains an edge between a vertex in $U$ and a vertex in $V$. We show that string graphs are conflict graphs of certain planar drawings. For planar graphs and complete $k$-partite graphs, we give additional, more efficient constructions. We furthermore show that there are subdivisions of non-planar graphs that are not conflict graphs. Lastly, we show a separator lemma for conflict graphs.",https://arxiv.org/abs/2209.02106
Unique Sink Orientations of Grids is in Unique End of Potential Line,"MichaelaBorzechowski, WolfgangMulzer",05-sep-22,Computational Geometry (cs.CG)," The complexity classes Unique End of Potential Line (UEOPL) and its promise version PUEOPL were introduced in 2018 by Fearnly et al. UEOPL captures search problems where the instances are promised to have a unique solution. UEOPL captures total search versions of these promise problems. The promise problems can be made total by defining violations that are returned as a short certificate of an unfulfilled promise.   GridUSO is the problem of finding the sink in a grid with a unique sink orientation. It was introduced by GÃ¤rtner et al. We describe a promise preserving reduction from GridUSO to UniqueForwardEOPL, a UEOPL-complete problem. Thus, we show that GridUSO is in UEOPL and its promise version is in PUEOPL.",https://arxiv.org/abs/2209.02103
Compressing integer lists with Contextual Arithmetic Trits,"YannBarsamian, AndrÃ©Chailloux",05-sep-22,Databases (cs.DB)," Inverted indexes allow to query large databases without needing to search in the database at each query. An important line of research is to construct the most efficient inverted indexes, both in terms of compression ratio and time efficiency. In this article, we show how to use trit encoding, combined with contextual methods for computing inverted indexes. We perform an extensive study of different variants of these methods and show that our method consistently outperforms the Binary Interpolative Method -- which is one of the golden standards in this topic -- with respect to compression size. We apply our methods to a variety of datasets and make available the source code that produced the results, together with all our datasets.",https://arxiv.org/abs/2209.02101
Simulation of Heat Conduction in Complex Domains of Multi-materialComposites using a Meshless Method,"NamanBartwal, ShantanuShahane, Somnath Roy, Surya PratapVanka",05-sep-22,Numerical Analysis (math.NA)," Several engineering applications involve complex materials with significant and discontinuous variations in thermophysical properties. These include materials for thermal storage, biological tissues with blood capillaries, etc. For such applications, numerical simulations must exercise care in not smearing the interfaces by interpolating variables across the interfaces of the subdomains. In this paper, we describe a high accuracy meshless method that uses domain decomposition and cloud-based interpolation of scattered data to solve the heat conduction equation in such situations. The polyharmonic spline (PHS) function with appended polynomial of prescribed degree is used for discretization. A flux balance condition is satisfied at the interface points and the clouds of interpolation points are restricted to be within respective domains. Compared with previously proposed meshless algorithms with domain decomposition, the cloud-based interpolations are numerically better conditioned, and achieve high accuracy through the appended polynomial. The accuracy of the algorithm is demonstrated in several two and three dimensional problems using manufactured solutions to the heat conduction equation with sharp discontinuity in thermal conductivity. Subsequently, we demonstrate the applicability of the algorithm to solve heat conduction in complex domains with practical boundary conditions and internal heat generation. Systematic computations with varying conductivity ratios, interpoint spacing and degree of appended polynomial are performed to investigate the accuracy of the algorithm.",https://arxiv.org/abs/2209.02089
A Study on Representation Transfer for Few-Shot Learning,"Chun-NamYu, YiXie",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Few-shot classification aims to learn to classify new object categories well using only a few labeled examples. Transferring feature representations from other models is a popular approach for solving few-shot classification problems. In this work we perform a systematic study of various feature representations for few-shot classification, including representations learned from MAML, supervised classification, and several common self-supervised tasks. We find that learning from more complex tasks tend to give better representations for few-shot classification, and thus we propose the use of representations learned from multiple tasks for few-shot classification. Coupled with new tricks on feature selection and voting to handle the issue of small sample size, our direct transfer learning method offers performance comparable to state-of-art on several benchmark datasets.",https://arxiv.org/abs/2209.02082
CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingualRetrieval,"Kung-HsiangHuang, ChengXiangZhai, HengJi",05-sep-22,Computation and Language (cs.CL)," Fact-checking has gained increasing attention due to the widespread of falsified information. Most fact-checking approaches focus on claims made in English only due to the data scarcity issue in other languages. The lack of fact-checking datasets in low-resource languages calls for an effective cross-lingual transfer technique for fact-checking. Additionally, trustworthy information in different languages can be complementary and helpful in verifying facts. To this end, we present the first fact-checking framework augmented with cross-lingual retrieval that aggregates evidence retrieved from multiple languages through a cross- lingual retriever. Given the absence of cross-lingual information retrieval datasets with claim-like queries, we train the retriever with our proposed Cross-lingual Inverse Cloze Task (X-ICT), a self-supervised algorithm that creates training instances by translating the title of a passage. The goal for X-ICT is to learn cross-lingual retrieval in which the model learns to identify the passage corresponding to a given translated title. On the X-Fact dataset, our approach achieves 2.23% absolute F1 improvement in the zero-shot cross-lingual setup over prior systems. The source code and data are publicly available at [this https URL](https://github.com/khuangaf/CONCRETE).",https://arxiv.org/abs/2209.02073
Trust in Language Grounding: a new AI challenge for human-robot teams,"David M.Bossens, ChristineEvers",05-sep-22,Artificial Intelligence (cs.AI)," The challenge of language grounding is to fully understand natural language by grounding language in real-world referents. While AI techniques are available, the widespread adoption and effectiveness of such technologies for human-robot teams relies critically on user trust. This survey provides three contributions relating to the newly emerging field of trust in language grounding, including a) an overview of language grounding research in terms of AI technologies, data sets, and user interfaces; b) six hypothesised trust factors relevant to language grounding, which are tested empirically on a human-robot cleaning team; and c) future research directions for trust in language grounding.",https://arxiv.org/abs/2209.02071
"""Dummy Grandpa, do you know anything?"": Identifying and CharacterizingAd hominem Fallacy Usage in the Wild","UtkarshPatel, AnimeshMukherjee, MainackMondal",05-sep-22,Computation and Language (cs.CL)," Today, participating in discussions on online forums is extremely commonplace and these discussions have started rendering a strong influence on the overall opinion of online users. Naturally, twisting the flow of the argument can have a strong impact on the minds of naive users, which in the long run might have socio-political ramifications, for example, winning an election or spreading targeted misinformation. Thus, these platforms are potentially highly vulnerable to malicious players who might act individually or as a cohort to breed fallacious arguments with a motive to sway public opinion. Ad hominem arguments are one of the most effective forms of such fallacies. Although a simple fallacy, it is effective enough to sway public debates in offline world and can be used as a precursor to shutting down the voice of opposition by slander.   In this work, we take a first step in shedding light on the usage of ad hominem fallacies in the wild. First, we build a powerful ad hominem detector with high accuracy (F1 more than 83%, showing a significant improvement over prior work), even for datasets for which annotated instances constitute a very small fraction. We then used our detector on 265k arguments collected from the online debate forum - CreateDebate. Our crowdsourced surveys validate our in-the-wild predictions on CreateDebate data (94% match with manual annotation). Our analysis revealed that a surprising 31.23% of CreateDebate content contains ad hominem fallacy, and a cohort of highly active users post significantly more ad hominem to suppress opposing views. Then, our temporal analysis revealed that ad hominem argument usage increased significantly since the 2016 US Presidential election, not only for topics like Politics, but also for Science and Law. We conclude by discussing important implications of our work to detect and defend against ad hominem fallacies.",https://arxiv.org/abs/2209.02066
Low-rank nonnegative tensor approximation via alternating projectionsand sketching,"AzamatSultonov, SergeyMatveev, StanislavBudzinskiy",05-sep-22,Numerical Analysis (math.NA)," We show how to construct nonnegative low-rank approximations of nonnegative tensors in Tucker and tensor train formats. We use alternating projections between the nonnegative orthant and the set of low-rank tensors, using STHOSVD and TTSVD algorithms, respectively, and further accelerate the alternating projections using randomized sketching. The numerical experiments on both synthetic data and hyperspectral images show the decay of the negative elements and that the error of the resulting approximation is close to the initial error obtained with STHOSVD and TTSVD. The proposed method for the Tucker case is superior to the previous ones in terms of computational complexity and decay of negative elements. The tensor train case, to the best of our knowledge, has not been studied before.",https://arxiv.org/abs/2209.02062
Full Kullback-Leibler-Divergence Loss for Hyperparameter-free LabelDistribution Learning,"MauriceGÃ¼nder, NicoPiatkowski, ChristianBauckhage",05-sep-22,Machine Learning (cs.LG)," The concept of Label Distribution Learning (LDL) is a technique to stabilize classification and regression problems with ambiguous and/or imbalanced labels. A prototypical use-case of LDL is human age estimation based on profile images. Regarding this regression problem, a so called Deep Label Distribution Learning (DLDL) method has been developed. The main idea is the joint regression of the label distribution and its expectation value. However, the original DLDL method uses loss components with different mathematical motivation and, thus, different scales, which is why the use of a hyperparameter becomes necessary. In this work, we introduce a loss function for DLDL whose components are completely defined by Kullback- Leibler (KL) divergences and, thus, are directly comparable to each other without the need of additional hyperparameters. It generalizes the concept of DLDL with regard to further use-cases, in particular for multi- dimensional or multi-scale distribution learning tasks.",https://arxiv.org/abs/2209.02060
RX-ADS: Interpretable Anomaly Detection using Adversarial ML forElectric Vehicle CAN data,"Chathurika S.Wickramasinghe, Daniel L.Marino, Harindra S.Mavikumbure, VictorCobilean, Timothy D.Pennington, Benny J.Varghese, CraigRieger, Milos Manic",05-sep-22,Machine Learning (cs.LG)," Recent year has brought considerable advancements in Electric Vehicles (EVs) and associated infrastructures/communications. Intrusion Detection Systems (IDS) are widely deployed for anomaly detection in such critical infrastructures. This paper presents an Interpretable Anomaly Detection System (RX-ADS) for intrusion detection in CAN protocol communication in EVs. Contributions include: 1) window based feature extraction method; 2) deep Autoencoder based anomaly detection method; and 3) adversarial machine learning based explanation generation methodology. The presented approach was tested on two benchmark CAN datasets: OTIDS and Car Hacking. The anomaly detection performance of RX-ADS was compared against the state-of-the-art approaches on these datasets: HIDS and GIDS. The RX-ADS approach presented performance comparable to the HIDS approach (OTIDS dataset) and has outperformed HIDS and GIDS approaches (Car Hacking dataset). Further, the proposed approach was able to generate explanations for detected abnormal behaviors arising from various intrusions. These explanations were later validated by information used by domain experts to detect anomalies. Other advantages of RX-ADS include: 1) the method can be trained on unlabeled data; 2) explanations help experts in understanding anomalies and root course analysis, and also help with AI model debugging and diagnostics, ultimately improving user trust in AI systems.",https://arxiv.org/abs/2209.02055
Visualization Of Class Activation Maps To Explain AI Classification OfNetwork Packet Captures,"IgorCherepanov, Alex Ulmer, Jonathan GeraldiJoewono, JÃ¶rnKohlhammer",05-sep-22,Machine Learning (cs.LG)," The classification of internet traffic has become increasingly important due to the rapid growth of today's networks and applications. The number of connections and the addition of new applications in our networks causes a vast amount of log data and complicates the search for common patterns by experts. Finding such patterns among specific classes of applications is necessary to fulfill various requirements in network analytics. Deep learning methods provide both feature extraction and classification from data in a single system. However, these networks are very complex and are used as black-box models, which weakens the experts' trust in the classifications. Moreover, by using them as a black-box, new knowledge cannot be obtained from the model predictions despite their excellent performance. Therefore, the explainability of the classifications is crucial. Besides increasing trust, the explanation can be used for model evaluation gaining new insights from the data and improving the model. In this paper, we present a visual interactive tool that combines the classification of network data with an explanation technique to form an interface between experts, algorithms, and data.",https://arxiv.org/abs/2209.02052
4Ward: a Relayering Strategy for Efficient Training of ArbitrarilyComplex Directed Acyclic Graphs,"TommasoBoccato, MatteoFerrante, AndreaDuggento, NicolaToschi",05-sep-22,Neural and Evolutionary Computing (cs.NE)," Thanks to their ease of implementation, multilayer perceptrons (MLPs) have become ubiquitous in deep learning applications. The graph underlying an MLP is indeed multipartite, i.e. each layer of neurons only connects to neurons belonging to the adjacent layer. In constrast, in vivo brain connectomes at the level of individual synapses suggest that biological neuronal networks are characterized by scale-free degree distributions or exponentially truncated power law strength distributions, hinting at potentially novel avenues for the exploitation of evolution- derived neuronal networks. In this paper, we present ""4Ward"", a method and Python library capable of generating flexible and efficient neural networks (NNs) from arbitrarily complex directed acyclic graphs. 4Ward is inspired by layering algorithms drawn from the graph drawing discipline to implement efficient forward passes, and provides significant time gains in computational experiments with various ErdÅ‘s-RÃ©nyi graphs. 4Ward overcomes the sequential nature of the learning matrix method by parallelizing the computation of activations and provides the designer with freedom to customize weight initialization and activation functions. Our algorithm can be of aid for any investigator seeking to exploit complex topologies in a NN design framework at the microscale.",https://arxiv.org/abs/2209.02045
Fast geometric trim fitting using partial incremental sorting andaccumulation,"Min Li, LaurentKneip",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We present an algorithmic contribution to improve the efficiency of robust trim-fitting in outlier affected geometric regression problems. The method heavily relies on the quick sort algorithm, and we present two important insights. First, partial sorting is sufficient for the incremental calculation of the x-th percentile value. Second, the normal equations in linear fitting problems may be updated incrementally by logging swap operations across the x-th percentile boundary during sorting. Besides linear fitting problems, we demonstrate how the technique can be additionally applied to closed-form, non-linear energy minimization problems, thus enabling efficient trim fitting under geometrically optimal objectives. We apply our method to two distinct camera resectioning algorithms, and demonstrate highly efficient and reliable, geometric trim fitting.",https://arxiv.org/abs/2209.02037
Distilling the Knowledge of BERT for CTC-based ASR,"HayatoFutami, HirofumiInaguma, MasatoMimura, ShinsukeSakai, TatsuyaKawahara",05-sep-22,Computation and Language (cs.CL)," Connectionist temporal classification (CTC) -based models are attractive because of their fast inference in automatic speech recognition (ASR). Language model (LM) integration approaches such as shallow fusion and rescoring can improve the recognition accuracy of CTC-based ASR by taking advantage of the knowledge in text corpora. However, they significantly slow down the inference of CTC. In this study, we propose to distill the knowledge of BERT for CTC-based ASR, extending our previous study for attention-based ASR. CTC-based ASR learns the knowledge of BERT during training and does not use BERT during testing, which maintains the fast inference of CTC. Different from attention-based models, CTC-based models make frame-level predictions, so they need to be aligned with token-level predictions of BERT for distillation. We propose to obtain alignments by calculating the most plausible CTC paths. Experimental evaluations on the Corpus of Spontaneous Japanese (CSJ) and TED-LIUM2 show that our method improves the performance of CTC-based ASR without the cost of inference speed.",https://arxiv.org/abs/2209.02034
Evaluation of the Region of Attractions of Higher DimensionalHyperbolic Systems using the Extended Dynamic Mode Decomposition,"Camilo Garcia-Tenorio, Duvan Tellez-Castro, Eduardo Mojica-Nava, Alain VandeWouwer",05-sep-22,Systems and Control (eess.SY)," This paper proposes an original methodology to compute the regions of attraction in hyperbolic and polynomial nonlinear dynamical systems using the eigenfunctions of the discrete-time approximation of the Koopman operator given by the extended dynamic mode decomposition algorithm. The proposed method relies on the spectral decomposition of the Koopman operator to build eigenfunctions that capture the boundary of the region of attraction. The algorithm relies solely on data that can be collected in experimental studies and does not require a mathematical model of the system. Two examples of dynamical systems, a population model and a higher dimensional chemical reaction system, allows demonstrating the reliability of the results.",https://arxiv.org/abs/2209.02030
How Much User Context Do We Need? Privacy by Design in Mental HealthNLP Application,"RamitSawhney, Atula TejaswiNeerkaje, IvanHabernal, Lucie Flek",05-sep-22,Computation and Language (cs.CL)," Clinical NLP tasks such as mental health assessment from text, must take social constraints into account - the performance maximization must be constrained by the utmost importance of guaranteeing privacy of user data. Consumer protection regulations, such as GDPR, generally handle privacy by restricting data availability, such as requiring to limit user data to 'what is necessary' for a given purpose. In this work, we reason that providing stricter formal privacy guarantees, while increasing the volume of user data in the model, in most cases increases benefit for all parties involved, especially for the user. We demonstrate our arguments on two existing suicide risk assessment datasets of Twitter and Reddit posts. We present the first analysis juxtaposing user history length and differential privacy budgets and elaborate how modeling additional user context enables utility preservation while maintaining acceptable user privacy guarantees.",https://arxiv.org/abs/2209.02028
When Robotics Meets Wireless Communications: An Introductory Tutorial,"Bonilla LiceaDaniel, GhoghoMounir, SaskaMartin",05-sep-22,Robotics (cs.RO)," The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles (UAVs) within the research community, industry, and society is growing fast. Many of these agents are nowadays equipped with communication systems that are, in some cases, essential to successfully achieve certain tasks. In this context, we have begun to witness the development of a new interdisciplinary research field at the intersection of robotics and communications. This research field has been boosted by the intention of integrating UAVs within the 5G and 6G communication networks. This research will undoubtedly lead to many important applications in the near future. Nevertheless, one of the main obstacles to the development of this research area is that most researchers address these problems by oversimplifying either the robotics or the communications aspect. This impedes the ability of reaching the full potential of this new interdisciplinary research area. In this tutorial, we present some of the modelling tools necessary to address problems involving both robotics and communication from an interdisciplinary perspective. As an illustrative example of such problems, we focus in this tutorial on the issue of communication-aware trajectory planning.",https://arxiv.org/abs/2209.02022
Resolving Infeasibility of Linear Systems: A Parameterized Approach,"KristÃ³fBÃ©rczi, AlexanderGÃ¶ke, Lydia Mirabel Mendoza-Cadena, MatthiasMnich",05-sep-22,Data Structures and Algorithms (cs.DS)," Deciding feasibility of large systems of linear equations and inequalities is one of the most fundamental algorithmic tasks. However, due to data inaccuracies or modeling errors, in practical applications one often faces linear systems that are infeasible. Extensive theoretical and practical methods have been proposed for post-infeasibility analysis of linear systems. This generally amounts to detecting a feasibility blocker of small size $k$, which is a set of equations and inequalities whose removal or perturbation from the large system of size $m$ yields a feasible system. This motivates a parameterized approach towards post-infeasibility analysis, where we aim to find a feasibility blocker of size at most $k$ in fixed- parameter time $f(k) \cdot m^{O(1)}$. We establish parameterized intractability ($W[1]$- and $NP$-hardness) results already in very restricted settings for different choices of the parameters maximum size of a deletion set, number of positive/negative right-hand sides, treewidth, pathwidth and treedepth. Additionally, we rule out a polynomial compression for MinFB parameterized by the size of a deletion set and the number of negative right-hand sides. Furthermore, we develop fixed-parameter algorithms parameterized by various combinations of these parameters when every row of the system corresponds to a difference constraint. Our algorithms capture the case of Directed Feedback Arc Set, a fundamental parameterized problem whose fixed-parameter tractability was shown by Chen et al. (STOC 2008).",https://arxiv.org/abs/2209.02021
Human and social capital strategies for Mafia network disruption,"AnnamariaFicara, FrancescoCurreri, GiacomoFiumara, Pasquale DeMeo","5 Sep 2022 (v1(https://arxiv.org/abs/2209.02012v1)), lastrevised 7 Sep 2022 (this version, v2)",Social and Information Networks (cs.SI)," Social Network Analysis (SNA) is an interdisciplinary science that focuses on discovering the patterns of individuals interactions. In particular, practitioners have used SNA to describe and analyze criminal networks to highlight subgroups, key actors, strengths and weaknesses in order to generate disruption interventions and crime prevention systems. In this paper, the effectiveness of a total of seven disruption strategies for two real Mafia networks is investigated adopting SNA tools. Three interventions targeting actors with a high level of social capital and three interventions targeting those with a high human capital are put to the test and compared between each other and with random node removal. Human and social capital approaches were also applied on the BarabÃ¡si-Albert models which are the one which better represent criminal networks. Simulations showed that actor removal based on social capital proved to be the most effective strategy, by leading to the total disruption of the criminal network in the least number of steps. The removal of a specific figure of a Mafia family such as the Caporegime seemed also promising in the network disruption.",https://arxiv.org/abs/2209.02017
On the Origins of Self-Modeling,"RobertKwiatkowski, Yuhang Hu, Boyuan Chen, Hod Lipson",05-sep-22,Robotics (cs.RO)," Self-Modeling is the process by which an agent, such as an animal or machine, learns to create a predictive model of its own dynamics. Once captured, this self-model can then allow the agent to plan and evaluate various potential behaviors internally using the self-model, rather than using costly physical experimentation. Here, we quantify the benefits of such self-modeling against the complexity of the robot. We find a R2 =0.90 correlation between the number of degrees of freedom a robot has, and the added value of self-modeling as compared to a direct learning baseline. This result may help motivate self modeling in increasingly complex robotic systems, as well as shed light on the origins of self-modeling, and ultimately self-awareness, in animals and humans.",https://arxiv.org/abs/2209.02012
Zero Shot Learning on Simulated Robots,"RobertKwiatkowski, Hod Lipson",04-oct-19,Robotics (cs.RO)," In this work we present a method for leveraging data from one source to learn how to do multiple new tasks. Task transfer is achieved using a self-model that encapsulates the dynamics of a system and serves as an environment for reinforcement learning. To study this approach, we train a self-models on various robot morphologies, using randomly sampled actions. Using a self-model, an initial state and corresponding actions, we can predict the next state. This predictive self-model is then used by a standard reinforcement learning algorithm to accomplish tasks without ever seeing a state from the ""real"" environment. These trained policies allow the robots to successfully achieve their goals in the ""real"" environment. We demonstrate that not only is training on the self-model far more data efficient than learning even a single task, but also that it allows for learning new tasks without necessitating any additional data collection, essentially allowing zero-shot learning of new tasks.",https://arxiv.org/abs/2209.02010
Online Decision Making for Trading Wind Energy,"Miguel AngelMuÃ±oz, PierrePinson, JalalKazempour",05-sep-22,Machine Learning (cs.LG)," This paper proposes and develops a new algorithm for trading wind energy in electricity markets, within an online learning and optimization framework. In particular, we combine a component-wise adaptive variant of the gradient descent algorithm with recent advances in the feature-driven newsvendor model. This results in an online offering approach capable of leveraging data-rich environments, while adapting to non-stationary characteristics of energy generation and electricity markets, and with a minimal computational burden. The performance of our approach is analyzed based on several numerical experiments, showing both better adaptability to non-stationary uncertain parameters and significant economic gains.",https://arxiv.org/abs/1910.01994
Stop the [Image] Steal: The Role and Dynamics of Visual Content in the2020 U.S. Election Misinformation Campaign,"HanaMatatov, Mor Naaman, Ofra Amir",05-sep-22,Social and Information Networks (cs.SI)," Images are powerful. Visual information can attract attention, improve persuasion, trigger stronger emotions, and is easy to share and spread. We examine the characteristics of the popular images shared on Twitter as part of ""Stop the Steal"", the widespread misinformation campaign during the 2020 U.S. election. We analyze the spread of the forty most popular images shared on Twitter as part of this campaign. Using a coding process, we categorize and label the images according to their type, content, origin, and role, and perform a mixed-method analysis of these images' spread on Twitter. Our results show that popular images include both photographs and text rendered as image. Only very few of these popular images included alleged photographic evidence of fraud; and none of the popular photographs had been manipulated. Most images reached a significant portion of their total spread within several hours from their first appearance, and both popular- and less-popular accounts were involved in various stages of their spread.",https://arxiv.org/abs/2209.02009
Classical and Quantum Random Walks to Identify Leaders in CriminalNetworks,"AnnamariaFicara, GiacomoFiumara, Pasquale DeMeo, SalvatoreCatanese",05-sep-22,Social and Information Networks (cs.SI)," Random walks simulate the randomness of objects, and are key instruments in various fields such as computer science, biology and physics. The counter part of classical random walks in quantum mechanics are the quantum walks. Quantum walk algorithms provide an exponential speedup over classical algorithms. Classical and quantum random walks can be applied in social network analysis, and can be used to define specific centrality metrics in terms of node occupation on single-layer and multilayer networks. In this paper, we applied these new centrality measures to three real criminal networks derived from an anti-mafia operation named Montagna and a multilayer network derived from them. Our aim is to (i) identify leaders in our criminal networks, (ii) study the dependence between these centralities and the degree, (iii) compare the results obtained for the real multilayer criminal network with those of a synthetic multilayer network which replicates its structure.",https://arxiv.org/abs/2209.02007
Neuromorphic Visual Odometry with Resonator Networks,"AlphaRenner, Lazar Supic, AndreeaDanielescu, GiacomoIndiveri, E. PaxonFrady, Friedrich T.Sommer, YuliaSandamirskaya",05-sep-22,Robotics (cs.RO)," Autonomous agents require self-localization to navigate in unknown environments. They can use Visual Odometry (VO) to estimate self-motion and localize themselves using visual sensors. This motion-estimation strategy is not compromised by drift as inertial sensors or slippage as wheel encoders. However, VO with conventional cameras is computationally demanding, limiting its application in systems with strict low-latency, -memory, and -energy requirements. Using event-based cameras and neuromorphic computing hardware offers a promising low-power solution to the VO problem. However, conventional algorithms for VO are not readily convertible to neuromorphic hardware. In this work, we present a VO algorithm built entirely of neuronal building blocks suitable for neuromorphic implementation. The building blocks are groups of neurons representing vectors in the computational framework of Vector Symbolic Architecture (VSA) which was proposed as an abstraction layer to program neuromorphic hardware. The VO network we propose generates and stores a working memory of the presented visual environment. It updates this working memory while at the same time estimating the changing location and orientation of the camera. We demonstrate how VSA can be leveraged as a computing paradigm for neuromorphic robotics. Moreover, our results represent an important step towards using neuromorphic computing hardware for fast and power-efficient VO and the related task of simultaneous localization and mapping (SLAM). We validate this approach experimentally in a robotic task and with an event- based dataset, demonstrating state-of-the-art performance.",https://arxiv.org/abs/2209.02005
Bridging Music and Text with Crowdsourced Music Comments: A Sequence-to-Sequence Framework for Thematic Music Comments Generation,"PeiningZhang, Junliang Guo, Linli Xu, MuYou, JunmingYin",05-sep-22,Sound (cs.SD)," We consider a novel task of automatically generating text descriptions of music. Compared with other well-established text generation tasks such as image caption, the scarcity of well-paired music and text datasets makes it a much more challenging task. In this paper, we exploit the crowd-sourced music comments to construct a new dataset and propose a sequence-to-sequence model to generate text descriptions of music. More concretely, we use the dilated convolutional layer as the basic component of the encoder and a memory based recurrent neural network as the decoder. To enhance the authenticity and thematicity of generated texts, we further propose to fine-tune the model with a discriminator as well as a novel topic evaluator. To measure the quality of generated texts, we also propose two new evaluation metrics, which are more aligned with human evaluation than traditional metrics such as BLEU. Experimental results verify that our model is capable of generating fluent and meaningful comments while containing thematic and content information of the original music.",https://arxiv.org/abs/2209.02000
Federated Zero-Shot Learning for Visual Recognition,"ZhiChen, YadanLuo, SenWang, Jingjing Li, Zi Huang",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Zero-shot learning is a learning regime that recognizes unseen classes by generalizing the visual-semantic relationship learned from the seen classes. To obtain an effective ZSL model, one may resort to curating training samples from multiple sources, which may inevitably raise the privacy concerns about data sharing across different organizations. In this paper, we propose a novel Federated Zero-Shot Learning FedZSL framework, which learns a central model from the decentralized data residing on edge devices. To better generalize to previously unseen classes, FedZSL allows the training data on each device sampled from the non-overlapping classes, which are far from the i.i.d. that traditional federated learning commonly assumes. We identify two key challenges in our FedZSL protocol: 1) the trained models are prone to be biased to the locally observed classes, thus failing to generalize to the unseen classes and/or seen classes appeared on other devices; 2) as each category in the training data comes from a single source, the central model is highly vulnerable to model replacement (backdoor) attacks. To address these issues, we propose three local objectives for visual-semantic alignment and cross-device alignment through relation distillation, which leverages the normalized class-wise covariance to regularize the consistency of the prediction logits across devices. To defend against the backdoor attacks, a feature magnitude defending technique is proposed. As malicious samples are less correlated to the given semantic attributes, the visual features of low magnitude will be discarded to stabilize model updates. The effectiveness and robustness of FedZSL are demonstrated by extensive experiments conducted on three zero-shot benchmark datasets.",https://arxiv.org/abs/2209.01996
Aspects of Modelling Requirements in Very-Large Agile SystemsEngineering,"GrischaLiebel, Eric Knauss",05-sep-22,Software Engineering (cs.SE)," Using models for requirements engineering (RE) is uncommon in systems engineering, despite the widespread use of model-based engineering in general. One reason for this lack of use is that formal models do not match well the trend to move towards agile developing methods. While there exists work that investigates challenges in the adoption of requirements modeling and agile methods in systems engineering, there is a lack of work studying successful approaches of using requirements modelling in agile systems engineering. To address this gap, we conducted a case study investigating the application of requirements models at Ericsson AB, a Swedish telecommunications company. We studied a department using requirements models to bridge agile development and plan-driven development aspects. We find that models are used to understand how requirements relate to each other, and to keep track with the product's evolution. To cope with the effort to maintain models over time, study participants suggest to rely on text-based notations that bring the models closer to developers and allow integration into existing software development workflows. This results in tool trade-offs, e.g., losing the possibility to control diagram layout.",https://arxiv.org/abs/2209.01994
TFN: An Interpretable Neural Network with Time-Frequency TransformEmbedded for Intelligent Fault Diagnosis,"QianChen, XingjianDong, GuoweiTu, DongWang, BaoxuanZhao, ZhikePeng",05-sep-22,Artificial Intelligence (cs.AI)," Convolutional Neural Networks (CNNs) are widely used in fault diagnosis of mechanical systems due to their powerful feature extraction and classification capabilities. However, the CNN is a typical black-box model, and the mechanism of CNN's decision-making are not clear, which limits its application in high-reliability-required fault diagnosis scenarios. To tackle this issue, we propose a novel interpretable neural network termed as Time-Frequency Network (TFN), where the physically meaningful time-frequency transform (TFT) method is embedded into the traditional convolutional layer as an adaptive preprocessing layer. This preprocessing layer named as time- frequency convolutional (TFconv) layer, is constrained by a well-designed kernel function to extract fault-related time-frequency information. It not only improves the diagnostic performance but also reveals the logical foundation of the CNN prediction in the frequency domain. Different TFT methods correspond to different kernel functions of the TFconv layer. In this study, four typical TFT methods are considered to formulate the TFNs and their effectiveness and interpretability are proved through three mechanical fault diagnosis experiments. Experimental results also show that the proposed TFconv layer can be easily generalized to other CNNs with different depths. The code of TFN is available on [this https URL](https://github.com/ChenQian0618/TFN).",https://arxiv.org/abs/2209.01993
A Benchmark for Weakly Semi-Supervised Abnormality Localization inChest X-Rays,"HaoqinJi, HaozheLiu, YuexiangLi, JinhengXie, NanjunHe, YawenHuang, DongWei, XinrongChen, LinlinShen, YefengZheng",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Accurate abnormality localization in chest X-rays (CXR) can benefit the clinical diagnosis of various thoracic diseases. However, the lesion-level annotation can only be performed by experienced radiologists, and it is tedious and time-consuming, thus difficult to acquire. Such a situation results in a difficulty to develop a fully-supervised abnormality localization system for CXR. In this regard, we propose to train the CXR abnormality localization framework via a weakly semi-supervised strategy, termed Point Beyond Class (PBC), which utilizes a small number of fully annotated CXRs with lesion-level bounding boxes and extensive weakly annotated samples by points. Such a point annotation setting can provide weakly instance-level information for abnormality localization with a marginal annotation cost. Particularly, the core idea behind our PBC is to learn a robust and accurate mapping from the point annotations to the bounding boxes against the variance of annotated points. To achieve that, a regularization term, namely multi-point consistency, is proposed, which drives the model to generate the consistent bounding box from different point annotations inside the same abnormality. Furthermore, a self- supervision, termed symmetric consistency, is also proposed to deeply exploit the useful information from the weakly annotated data for abnormality localization. Experimental results on RSNA and VinDr-CXR datasets justify the effectiveness of the proposed method. When less than 20% box-level labels are used for training, an improvement of ~5 in mAP can be achieved by our PBC, compared to the current state-of-the-art method (i.e., Point DETR). Code is available at [this https URL](https://github.com/HaozheLiu-ST/Point-Beyond-Class).",https://arxiv.org/abs/2209.01992
ScalSALE: Scalable SALE Benchmark Framework for Supercomputers,"Re'emHarel, MatanRusanovsky, Ron Wagner, Harel Levin, Gal Oren",05-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," Supercomputers worldwide provide the necessary infrastructure for groundbreaking research. However, most supercomputers are not designed equally due to different desired figure of merit, which is derived from the computational bounds of the targeted scientific applications' portfolio. In turn, the design of such computers becomes an optimization process that strives to achieve the best performances possible in a multi-parameters search space. Therefore, verifying and evaluating whether a supercomputer can achieve its desired goal becomes a tedious and complex task. For this purpose, many full, mini, proxy, and benchmark applications have been introduced in the attempt to represent scientific applications. Nevertheless, as these benchmarks are hard to expand, and most importantly, are over-simplified compared to scientific applications that tend to couple multiple scientific domains, they fail to represent the true scaling capabilities. We suggest a new physical scalable benchmark framework, namely ScalSALE, based on the well-known SALE scheme. ScalSALE's main goal is to provide a simple, flexible, scalable infrastructure that can be easily expanded to include multi-physical schemes while maintaining scalable and efficient execution times. By expanding ScalSALE, the gap between the over- simplified benchmarks and scientific applications can be bridged. To achieve this goal, ScalSALE is implemented in Modern Fortran with simple OOP design patterns and supported by transparent MPI-3 blocking and non-blocking communication that allows such a scalable framework. ScalSALE is compared to LULESH via simulating the Sedov-Taylor blast wave problem using strong and weak scaling tests. ScalSALE is executed and evaluated with both rezoning options - Lagrangian and Eulerian.",https://arxiv.org/abs/2209.01988
Volumetric video streaming: Current approaches and implementations,"IreneViola, PabloCesar",05-sep-22,Multimedia (cs.MM)," The rise of capturing systems for objects and scenes in 3D with increased fidelity and immersion has led to the popularity of volumetric video contents that can be seen from any position and angle in 6 degrees of freedom navigation. Such contents need large volumes of data to accurately represent the real world. Thus, novel optimization solutions and delivery systems are needed to enable volumetric video streaming over bandwidth- limited networks. In this chapter, we discuss theoretical approaches to volumetric video streaming optimization, through compression solutions, as well as network and user adaptation, for high-end and low-powered devices. Moreover, we present an overview of existing end-to-end systems, and we point to the future of volumetric video streaming.",https://arxiv.org/abs/2209.01983
Few-shot Incremental Event Detection,"HaoWang, HanwenShi, JianyongDuan",05-sep-22,Computation and Language (cs.CL)," Event detection tasks can help people quickly determine the domain from complex texts. It can also provides powerful support for downstream tasks of natural language processing.Existing methods implement fixed-type learning only based on large amounts of data. When extending to new classes, it is often required to retain the original data and retrain the model.Incremental event detection tasks enables lifelong learning of new classes, but most existing methods need to retain a large number of original data or face the problem of catastrophic forgetting. Apart from that, it is difficult to obtain enough data for model training due to the lack of high- quality data in [this http URL](http://practical.To) address the above problems, we define a new task in the domain of event detection, which is few-shot incremental event detection.This task require that the model should retain previous type when learning new event type in each round with limited input. We recreate and release a benchmark dataset in the few-shot incremental event detection task based on FewEvent.The dataset we published is more appropriate than other in this new task. In addition, we propose two benchmark approaches, IFSED-K and IFSED-KP, which can address the task in different ways. Experiments results have shown that our approach has a higher F1 score and is more stable than baseline.",https://arxiv.org/abs/2209.01982
A new collision avoidance model with random batch resolution strategy,"TianluChen, ChangYang, LÃ©onMatarTine, Zhichang Guo","5 Sep 2022 (v1(https://arxiv.org/abs/2209.01977v1)), lastrevised 7 Sep 2022 (this version, v2)",Numerical Analysis (math.NA)," Research on crowd simulation has important and wide range of applications. The main difficulty is how to lead all particles with a same and simple rule, especially when particles are numerous. In this paper, we firstly propose a two dimensional agent-based collision avoidance model, which is a $N$-particles Newtonian system. The collision interaction force, imminent interaction force and following interaction force are designed, so that particles can be guided to their respective destinations without collisions. The proposed agent-based model is then extended to the corresponding mean field limit model as $N\to\infty$. Secondly, notice that direct simulation of the $N$-particles Newtonian system is very time- consuming, since the computational complexity is of order $\mathcal{O}(N^2)$. In contrast, we propose an efficient hybrid resolution strategy to reduce the computational complexity. It is a combination of the Random Batch method (Shi Jin, Lei Li, and Jian-Guo Liu. Random batch methods (RBM) for interacting particle systems. Journal of Computational Physics, 400:108877, 2020.) and the method based on local particles Newtonian system. Thanks to this hybrid resolution strategy, the computational complexity is reduced to $\mathcal{O}(N)$. Finally, various tests are presented to show robustness and efficiency of our collision avoidance model and the hybrid resolution strategy.",https://arxiv.org/abs/2209.01979
Selective Annotation Makes Language Models Better Few-Shot Learners,"HongjinSu, JungoKasai, ChenHenry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, MariOstendorf, LukeZettlemoyer, Noah A.Smith, TaoYu",05-sep-22,Computation and Language (cs.CL)," Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of- the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at [this https URL](https://github.com/HKUNLP/icl-selective-annotation).",https://arxiv.org/abs/2209.01977
Orthogonal layers of parallelism in large-scale eigenvaluecomputations,"AndreasAlvermann, Georg Hager, HolgerFehske",05-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," We address the communication overhead of distributed sparse matrix-(multiple)-vector multiplication in the context of large-scale eigensolvers, using filter diagonalization as an example. The basis of our study is a performance model which includes a communication metric that is computed directly from the matrix sparsity pattern without running any code. The performance model quantifies to which extent scalability and parallel efficiency are lost due to communication overhead. To restore scalability, we identify two orthogonal layers of parallelism in the filter diagonalization technique. In the horizontal layer the rows of the sparse matrix are distributed across individual processes. In the vertical layer bundles of multiple vectors are distributed across separate process groups. An analysis in terms of the communication metric predicts that scalability can be restored if, and only if, one implements the two orthogonal layers of parallelism via different distributed vector layouts. Our theoretical analysis is corroborated by benchmarks for application matrices from quantum and solid state physics. We finally demonstrate the benefits of using orthogonal layers of parallelism with two exemplary application cases -- an exciton and a strongly correlated electron system -- which incur either small or large communication overhead.",https://arxiv.org/abs/2209.01975
FIRED: a fine-grained robust performance diagnosis framework for cloudapplications,"RuyueXin, HongyunLiu, PengChen, PaolaGrosso, Zhiming Zhao",05-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," To run a cloud application with the required service quality, operators have to continuously monitor the cloud application's run-time status, detect potential performance anomalies, and diagnose the root causes of anomalies. However, existing models of performance anomaly detection often suffer from low re-usability and robustness due to the diversity of system-level metrics being monitored and the lack of high-quality labeled monitoring data for anomalies. Moreover, the current coarse-grained analysis models make it difficult to locate system-level root causes of the application performance anomalies for effective adaptation decisions. We provide a FIne-grained Robust pErformance Diagnosis (FIRED) framework to tackle those challenges. The framework offers an ensemble of several well- selected base models for anomaly detection using a deep neural network, which adopts weakly-supervised learning considering fewer labels exist in reality. The framework also employs a real-time fine-grained analysis model to locate dependent system metrics of the anomaly. Our experiments show that the framework can achieve the best detection accuracy and algorithm robustness, and it can predict anomalies in four minutes with F1 score higher than 0.8. In addition, the framework can accurately localize the first root causes, and with an average accuracy higher than 0.7 of locating first four root causes.",https://arxiv.org/abs/2209.01974
HAGCN : Network Decentralization Attention Based Heterogeneity-AwareSpatiotemporal Graph Convolution Network for Traffic Signal Forecasting,"JunKyuJang, Sung-Hyuk Park",05-sep-22,Machine Learning (cs.LG)," The construction of spatiotemporal networks using graph convolution networks (GCNs) has become one of the most popular methods for predicting traffic signals. However, when using a GCN for traffic speed prediction, the conventional approach generally assumes the relationship between the sensors as a homogeneous graph and learns an adjacency matrix using the data accumulated by the sensors. However, the spatial correlation between sensors is not specified as one but defined differently from various viewpoints. To this end, we aim to study the heterogeneous characteristics inherent in traffic signal data to learn the hidden relationships between sensors in various ways. Specifically, we designed a method to construct a heterogeneous graph for each module by dividing the spatial relationship between sensors into static and dynamic modules. We propose a network decentralization attention based heterogeneity-aware graph convolution network (HAGCN) method that aggregates the hidden states of adjacent nodes by considering the importance of each channel in a heterogeneous graph. Experimental results on real traffic datasets verified the effectiveness of the proposed method, achieving a 6.35% improvement over the existing model and realizing state-of-the-art prediction performance.",https://arxiv.org/abs/2209.01970
Modeling User Repeat Consumption Behavior for Online NovelRecommendation,"YuncongLi, CunxiangYin, YanchengHe, GuoqiangXu, JingCai, LeevenLuo, Sheng-huaZhong",05-sep-22,Information Retrieval (cs.IR)," Given a user's historical interaction sequence, online novel recommendation suggests the next novel the user may be interested in. Online novel recommendation is important but underexplored. In this paper, we concentrate on recommending online novels to new users of an online novel reading platform, whose first visits to the platform occurred in the last seven days. We have two observations about online novel recommendation for new users. First, repeat novel consumption of new users is a common phenomenon. Second, interactions between users and novels are informative. To accurately predict whether a user will reconsume a novel, it is crucial to characterize each interaction at a fine-grained level. Based on these two observations, we propose a neural network for online novel recommendation, called NovelNet. NovelNet can recommend the next novel from both the user's consumed novels and new novels simultaneously. Specifically, an interaction encoder is used to obtain accurate interaction representation considering fine-grained attributes of interaction, and a pointer network with a pointwise loss is incorporated into NovelNet to recommend previously- consumed novels. Moreover, an online novel recommendation dataset is built from a well-known online novel reading platform and is released for public use as a benchmark. Experimental results on the dataset demonstrate the effectiveness of NovelNet.",https://arxiv.org/abs/2209.01967
Adversarial Detection: Attacking Object Detection in Real Time,"Han Wu, Syed Yunas, SarehRowlands, Wenjie Ruan, JohanWahlstrom",05-sep-22,Artificial Intelligence (cs.AI)," Intelligent robots hinge on accurate object detection models to perceive the environment. Advances in deep learning security unveil that object detection models are vulnerable to adversarial attacks. However, prior research primarily focuses on attacking static images or offline videos. It is still unclear if such attacks could jeopardize real-world robotic applications in dynamic environments. There is still a gap between theoretical discoveries and real-world applications. We bridge the gap by proposing the first real-time online attack against object detection models. We devised three attacks that fabricate bounding boxes for nonexistent objects at desired locations.",https://arxiv.org/abs/2209.01963
Exponential convergence of a generalized FEM for heterogeneousreaction-diffusion equations,"ChupengMa, JensMarkusMelenk","5 Sep 2022 (v1(https://arxiv.org/abs/2209.01957v1)), lastrevised 8 Sep 2022 (this version, v2)",Numerical Analysis (math.NA)," A generalized finite element method is proposed for solving a heterogeneous reaction-diffusion equation with a singular perturbation parameter $\varepsilon$, based on locally approximating the solution on each subdomain by solution of a local reaction-diffusion equation and eigenfunctions of a local eigenproblem. These local problems are posed on some domains slightly larger than the subdomains with oversampling size $\delta^{\ast}$. The method is formulated at the continuous level as a direct discretization of the continuous problem and at the discrete level as a coarse-space approximation for its standard FE discretizations. Exponential decay rates for local approximation errors with respect to $\delta^{\ast}/\varepsilon$ and $\delta^{\ast}/h$ (at the discrete level with $h$ denoting the fine FE mesh size) and with the local degrees of freedom are established. In particular, it is shown that the method at the continuous level converges uniformly with respect to $\varepsilon$ in the standard $H^{1}$ norm, and that if the oversampling size is relatively large with respect to $\varepsilon$ and $h$ (at the discrete level), the solutions of the local reaction-diffusion equations provide good local approximations for the solution and thus the local eigenfunctions are not needed. Numerical results are provided to verify the theoretical results.",https://arxiv.org/abs/2209.01962
Moderately-Balanced Representation Learning for Treatment Effects withOrthogonality Information,"YiyanHuang, CheukHang Leung, Shumin Ma, QiWu, DongdongWang, ZhixiangHuang",05-sep-22,Machine Learning (cs.LG)," Estimating the average treatment effect (ATE) from observational data is challenging due to selection bias. Existing works mainly tackle this challenge in two ways. Some researchers propose constructing a score function that satisfies the orthogonal condition, which guarantees that the established ATE estimator is ""orthogonal"" to be more robust. The others explore representation learning models to achieve a balanced representation between the treated and the controlled groups. However, existing studies fail to 1) discriminate treated units from controlled ones in the representation space to avoid the over-balanced issue; 2) fully utilize the ""orthogonality information"". In this paper, we propose a moderately-balanced representation learning (MBRL) framework based on recent covariates balanced representation learning methods and orthogonal machine learning theory. This framework protects the representation from being over-balanced via multi- task learning. Simultaneously, MBRL incorporates the noise orthogonality information in the training and validation stages to achieve a better ATE estimation. The comprehensive experiments on benchmark and simulated datasets show the superiority and robustness of our method on treatment effect estimations compared with existing state-of-the-art methods.",https://arxiv.org/abs/2209.01957
New degrees of freedom for differential forms on cubical meshes,JonniLohi,05-sep-22,Numerical Analysis (math.NA)," We consider new degrees of freedom for higher order differential forms on cubical meshes. The approach is inspired by the idea of Rapetti and Bossavit to define higher order Whitney forms and their degrees of freedom using small simplices. We show that higher order differential forms on cubical meshes can be defined analogously using small cubes and prove that these small cubes yield unisolvent degrees of freedom. Significantly, this approach is compatible with discrete exterior calculus and expands the framework to cover higher order methods on cubical meshes, complementing the earlier strategy based on simplices.",https://arxiv.org/abs/2209.01956
Authentication of Underwater Assets,"BÃ¡lint Z.TÃ©glÃ¡sy, EmilWengle, John R.Potter, SokratisKatsikas",05-sep-22,Cryptography and Security (cs.CR)," Secure digital wireless communication underwater has become a key issue as maritime operations shift towards employing a heterogeneous mix of robotic assets and as the security of digital systems becomes challenged across all domains. At the same time, a proliferation of underwater signal coding and physical layer options are delivering greater bandwidth and flexibility, but mostly without the standards necessary for interoperability. We address here an essential requirement for security, namely a confirmation of asset identities also known as authentication. We propose, implement, verify and validate an authentication protocol based on the first digital underwater communications standard. Our scheme is applicable primarily to AUVs operating around offshore oil and gas facilities, but also to other underwater devices that may in the future have acoustic modems. It makes communication including command and control significantly more secure and provides a foundation for the development of more sophisticated security mechanisms.",https://arxiv.org/abs/2209.01954
MO2: Model-Based Offline Options,"SashaSalter, MarkusWulfmeier, DhruvaTirumala, NicolasHeess, MartinRiedmiller, RaiaHadsell, Dushyant Rao",05-sep-22,Machine Learning (cs.LG)," The ability to discover useful behaviours from past experience and transfer them to new tasks is considered a core component of natural embodied intelligence. Inspired by neuroscience, discovering behaviours that switch at bottleneck states have been long sought after for inducing plans of minimum description length across tasks. Prior approaches have either only supported online, on-policy, bottleneck state discovery, limiting sample-efficiency, or discrete state-action domains, restricting applicability. To address this, we introduce Model-Based Offline Options (MO2), an offline hindsight framework supporting sample-efficient bottleneck option discovery over continuous state-action spaces. Once bottleneck options are learnt offline over source domains, they are transferred online to improve exploration and value estimation on the transfer domain. Our experiments show that on complex long-horizon continuous control tasks with sparse, delayed rewards, MO2's properties are essential and lead to performance exceeding recent option learning methods. Additional ablations further demonstrate the impact on option predictability and credit assignment.",https://arxiv.org/abs/2209.01952
BiRank vs PageRank: Using SNA on Company Register Data for Fiscal RiskPrediction,"BernhardGÃ¶schlberger, Dragos Deliu",05-sep-22,Social and Information Networks (cs.SI)," Efficient financial administrations need to ensure compliant behavior of all tax subjects without excessive personnel costs or obstruction of compliant companies. To do so, accurate prediction of non- compliance or fraud is crucial. Social Network Analysis (SNA) provides powerful tools for fraud prediction as fraudulence is often clustered in certain areas of real world social networks. In this paper we present our results of comparing PageRank and the more recent BiRank to infer risk-ranks based on network structure and prior fraud information. Specifically, we model our social network from company register data. We find that in this case study BiRank outperforms PageRank in both quality of the resulting ranks for fraud prediction and run time. The results show that this class of algorithms is generally useful for fraud and risk prediction and more specifically also illustrate the potential of BiRank in comparison, as it opens up new modeling opportunities. Our results show that selecting companies for tax audits based on BiRank yields a precision of 16.38% for the top 20.000 subjects selecting 83.4% of all fraud cases (recall).",https://arxiv.org/abs/2209.01947
Jamming Modulation: An Active Anti-Jamming Scheme,"JianhuiMa, QiangLi, ZilongLiu, LinsongDu, HongyangChen, NirwanAnsari",05-sep-22,Information Theory (cs.IT)," Providing quality communications under adversarial electronic attacks, e.g., broadband jamming attacks, is a challenging task. Unlike state-of-the-art approaches which treat jamming signals as destructive interference, this paper presents a novel active anti-jamming (AAJ) scheme for a jammed channel to enhance the communication quality between a transmitter node (TN) and receiver node (RN), where the TN actively exploits the jamming signal as a carrier to send messages. Specifically, the TN is equipped with a programmable-gain amplifier, which is capable of re- modulating the jamming signals for jamming modulation. Considering four typical jamming types, we derive both the bit error rates (BER) and the corresponding optimal detection thresholds of the AAJ scheme. The asymptotic performances of the AAJ scheme are discussed under the high jamming-to-noise ratio (JNR) and sampling rate cases. Our analysis shows that there exists a BER floor for sufficiently large JNR. Simulation results indicate that the proposed AAJ scheme allows the TN to communicate with the RN reliably even under extremely strong and/or broadband jamming. Additionally, we investigate the channel capacity of the proposed AAJ scheme and show that the channel capacity of the AAJ scheme outperforms that of the direct transmission when the JNR is relatively high.",https://arxiv.org/abs/2209.01945
Incremental Permutation Feature Importance (iPFI): Towards OnlineExplanations on Data Streams,"FabianFumagalli, MaximilianMuschalik, EykeHÃ¼llermeier, BarbaraHammer","5 Sep 2022 (v1(https://arxiv.org/abs/2209.01939v1)), lastrevised 7 Sep 2022 (this version, v2)",Machine Learning (cs.LG)," Explainable Artificial Intelligence (XAI) has mainly focused on static learning scenarios so far. We are interested in dynamic scenarios where data is sampled progressively, and learning is done in an incremental rather than a batch mode. We seek efficient incremental algorithms for computing feature importance (FI) measures, specifically, an incremental FI measure based on feature marginalization of absent features similar to permutation feature importance (PFI). We propose an efficient, model- agnostic algorithm called iPFI to estimate this measure incrementally and under dynamic modeling conditions including concept drift. We prove theoretical guarantees on the approximation quality in terms of expectation and variance. To validate our theoretical findings and the efficacy of our approaches compared to traditional batch PFI, we conduct multiple experimental studies on benchmark data with and without concept drift.",https://arxiv.org/abs/2209.01943
MuCaSLAM: CNN-Based Frame Quality Assessment for Mobile Robot withOmnidirectional Visual SLAM,"PavelKarpyshev, EvgenyKruzhkov, EvgenyYudin, AlenaSavinykh, AndreiPotapov, MikhailKurenkov, AntonKolomeytsev, IvanKalinov, DzmitryTsetserukou",05-sep-22,Robotics (cs.RO)," In the proposed study, we describe an approach to improving the computational efficiency and robustness of visual SLAM algorithms on mobile robots with multiple cameras and limited computational power by implementing an intermediate layer between the cameras and the SLAM pipeline. In this layer, the images are classified using a ResNet18-based neural network regarding their applicability to the robot localization. The network is trained on a six-camera dataset collected in the campus of the Skolkovo Institute of Science and Technology (Skoltech). For training, we use the images and ORB features that were successfully matched with subsequent frame of the same camera (""good"" keypoints or features). The results have shown that the network is able to accurately determine the optimal images for ORB- SLAM2, and implementing the proposed approach in the SLAM pipeline can help significantly increase the number of images the SLAM algorithm can localize on, and improve the overall robustness of visual SLAM. The experiments on operation time state that the proposed approach is at least 6 times faster compared to using ORB extractor and feature matcher when operated on CPU, and more than 30 times faster when run on GPU. The network evaluation has shown at least 90% accuracy in recognizing images with a big number of ""good"" ORB keypoints. The use of the proposed approach allowed to maintain a high number of features throughout the dataset by robustly switching from cameras with feature-poor streams.",https://arxiv.org/abs/2209.01939
Forensicability Assessment of Questioned Images in RecapturingDetection,"ChangshengChen, LinZhao, RizhaoCai, ZitongYu, JiwuHuang, AlexC. Kot",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Recapture detection of face and document images is an important forensic task. With deep learning, the performances of face anti-spoofing (FAS) and recaptured document detection have been improved significantly. However, the performances are not yet satisfactory on samples with weak forensic cues. The amount of forensic cues can be quantified to allow a reliable forensic result. In this work, we propose a forensicability assessment network to quantify the forensicability of the questioned samples. The low-forensicability samples are rejected before the actual recapturing detection process to improve the efficiency of recapturing detection systems. We first extract forensicability features related to both image quality assessment and forensic tasks. By exploiting domain knowledge of the forensic application in image quality and forensic features, we define three task-specific forensicability classes and the initialized locations in the feature space. Based on the extracted features and the defined centers, we train the proposed forensic assessment network (FANet) with cross-entropy loss and update the centers with a momentum-based update method. We integrate the trained FANet with practical recapturing detection schemes in face anti-spoofing and recaptured document detection tasks. Experimental results show that, for a generic CNN-based FAS scheme, FANet reduces the EERs from 33.75% to 19.23% under ROSE to IDIAP protocol by rejecting samples with the lowest 30% forensicability scores. The performance of FAS schemes is poor in the rejected samples, with EER as high as 56.48%. Similar performances in rejecting low-forensicability samples have been observed for the state-of-the-art approaches in FAS and recaptured document detection tasks. To the best of our knowledge, this is the first work that assesses the forensicability of recaptured document images and improves the system efficiency.",https://arxiv.org/abs/2209.01936
Analysis of the Effect of Time Delay for Unmanned Aerial Vehicles withApplications to Vision Based Navigation,"Muhammad AhmedHumais, MohamadChehadeh, Igor Boiko, YahyaZweiri",05-sep-22,Robotics (cs.RO)," In this paper, we analyze the effect of time delay dynamics on controller design for Unmanned Aerial Vehicles (UAVs) with vision based navigation. Time delay is an inevitable phenomenon in cyber-physical systems, and has important implications on controller design and trajectory generation for UAVs. The impact of time delay on UAV dynamics increases with the use of the slower vision based navigation stack. We show that the existing models in the literature, which exclude time delay, are unsuitable for controller tuning since a trivial solution for minimizing an error cost functional always exists. The trivial solution that we identify suggests use of infinite controller gains to achieve optimal performance, which contradicts practical findings. We avoid such shortcomings by introducing a novel nonlinear time delay model for UAVs, and then obtain a set of linear decoupled models corresponding to each of the UAV control loops. The cost functional of the linearized time delay model of angular and altitude dynamics is analyzed, and in contrast to the delay-free models, we show the existence of finite optimal controller parameters. Due to the use of time delay models, we experimentally show that the proposed model accurately represents system stability limits. Due to time delay consideration, we achieved a tracking results of RMSE 5.01 cm when tracking a lemniscate trajectory with a peak velocity of 2.09 m/s using visual odometry (VO) based UAV navigation, which is on par with the state-of-the-art.",https://arxiv.org/abs/2209.01935
Gather -- a better way to codehack online,"RikaKobayashi, Sarah Jaffa, Jiachen Dong, Roger D.Amos, JeremyCohen, EmilyF.Kerrison",05-sep-22,Human-Computer Interaction (cs.HC)," A virtual hands-on computer laboratory has been designed within the Gather online meeting platform. Gather's features such as spatial audio, private spaces and interactable objects offer scope for great improvements over currently used platforms, especially for small-group based teaching. We describe our experience using this virtual computer laboratory for a recent 'Python for Beginners' workshop held as part of the Software Sustainability Institute's 2022 Research Software Camp.",https://arxiv.org/abs/2209.01933
Scene Text Recognition with Single-Point Decoding Network,"LeiChen, HaiboQin, Shi-XueZhang, ChunYang, XuchengYin",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In recent years, attention-based scene text recognition methods have been very popular and attracted the interest of many researchers. Attention-based methods can adaptively focus attention on a small area or even single point during decoding, in which the attention matrix is nearly one-hot distribution. Furthermore, the whole feature maps will be weighted and summed by all attention matrices during inference, causing huge redundant computations. In this paper, we propose an efficient attention- free Single-Point Decoding Network (dubbed SPDN) for scene text recognition, which can replace the traditional attention-based decoding network. Specifically, we propose Single-Point Sampling Module (SPSM) to efficiently sample one key point on the feature map for decoding one character. In this way, our method can not only precisely locate the key point of each character but also remove redundant computations. Based on SPSM, we design an efficient and novel single-point decoding network to replace the attention-based decoding network. Extensive experiments on publicly available benchmarks verify that our SPDN can greatly improve decoding efficiency without sacrificing performance.",https://arxiv.org/abs/2209.01927
Deciding a Graph Property by a Single Mobile Agent: One-Bit MemorySuffices,"TaisukeIzumi, KazukiKakizawa, YuyaKawabata, NaokiKitamura, ToshimitsuMasuzawa",05-sep-22,Data Structures and Algorithms (cs.DS)," We investigate the computational power of the deterministic single-agent model where the agent and each node are equipped with a limited amount of persistent memory. Tasks are formalized as decision problems on properties of input graphs, i.e., the task is defined as a subset $\mathcal{T}$ of all possible input graphs, and the agent must decide if the network belongs to $\mathcal{T}$ or not. We focus on the class of the decision problems which are solvable in a polynomial number of movements, and polynomial-time local computation. The contribution of this paper is the computational power of the very weak system with one-bit agent memory and $O(1)$-bit storage (i.e. node memory) is equivalent to the one with $O(n)$-bit agent memory and $O(1)$-bit storage. We also show that the one- bit agent memory is crucial to lead this equivalence: There exists a decision task which can be solved by the one-bit memory agent but cannot be solved by the zero-bit memory (i.e., oblivious) agent. Our result is deduced by the algorithm of simulating the $O(n)$-bit memory agent by the one-bit memory agent with polynomial-time overhead, which is developed by two novel technical tools. The first one is a dynamic $s$-$t$ path maintenance mechanism which uses only $O(1)$-bit storage per node. The second one is a new lexicographically-ordered DFS algorithm for the mobile agent system with $O(1)$-bit memory and $O(1)$-bit storage per node. These tools are of independent interest.",https://arxiv.org/abs/2209.01914
The Power of Uniform Sampling for Coresets,"VladimirBraverman, Vincent Cohen-Addad, Shaofeng H.-C.Jiang, RobertKrauthgamer, ChrisSchwiegelshohn, Mads BechToftrup, Xuan Wu",05-sep-22,Data Structures and Algorithms (cs.DS)," Motivated by practical generalizations of the classic $k$-median and $k$-means objectives, such as clustering with size constraints, fair clustering, and Wasserstein barycenter, we introduce a meta-theorem for designing coresets for constrained-clustering problems. The meta-theorem reduces the task of coreset construction to one on a bounded number of ring instances with a much-relaxed additive error. This reduction enables us to construct coresets using uniform sampling, in contrast to the widely-used importance sampling, and consequently we can easily handle constrained objectives. Notably and perhaps surprisingly, this simpler sampling scheme can yield coresets whose size is independent of $n$, the number of input points.   Our technique yields smaller coresets, and sometimes the first coresets, for a large number of constrained clustering problems, including capacitated clustering, fair clustering, Euclidean Wasserstein barycenter, clustering in minor-excluded graph, and polygon clustering under FrÃ©chet and Hausdorff distance. Finally, our technique yields also smaller coresets for $1$-median in low-dimensional Euclidean spaces, specifically of size $\tilde{O}(\varepsilon^{-1.5})$ in $\mathbb{R}^2$ and $\tilde{O}(\varepsilon^{-1.6})$ in $\mathbb{R}^3$.",https://arxiv.org/abs/2209.01906
A Robust Learning Methodology for Uncertainty-aware Scientific MachineLearning models,"Erbet CostaAlmeida, Carine de MenezesRebello, MarcioFontana, LeizerSchnitman, Idelfonso Bessa dos ReisNogueira",05-sep-22,Artificial Intelligence (cs.AI)," Robust learning is an important issue in Scientific Machine Learning (SciML). There are several works in the literature addressing this topic. However, there is an increasing demand for methods that can simultaneously consider all the different uncertainty components involved in SciML model identification. Hence, this work proposes a comprehensive methodology for uncertainty evaluation of the SciML that also considers several possible sources of uncertainties involved in the identification process. The uncertainties considered in the proposed method are the absence of theory and causal models, the sensitiveness to data corruption or imperfection, and the computational effort. Therefore, it was possible to provide an overall strategy for the uncertainty-aware models in the SciML field. The methodology is validated through a case study, developing a Soft Sensor for a polymerization reactor. The results demonstrated that the identified Soft Sensor are robust for uncertainties, corroborating with the consistency of the proposed approach.",https://arxiv.org/abs/2209.01901
Forward-Mode Automatic Differentiation of Compiled Programs,"MaxAehle, JohannesBlÃ¼hdorn, MaxSagebaum, Nicolas R.Gauger",05-sep-22,Mathematical Software (cs.MS)," Algorithmic differentiation (AD) is a set of techniques to obtain accurate derivatives of a computer-implemented function in an automatic fashion. State-of-the-art AD tools rely on the source code of the implementation or internal representations of compilers building it.   We present the new AD tool Derivgrind, which augments the machine code of compiled programs with forward AD logic. Derivgrind leverages the Valgrind instrumentation framework for a structured access to the machine code, and a shadow memory tool to store dot values. Depending on the application scenario, no access to the source code is required at all, or the access is restricted to the parts defining input and output variables.   Derivgrind's versatility comes at the price of scaling the running time by a factor between 60 and 140, measured on a benchmark based on a PDE solver. Results of our extensive test suite indicate that Derivgrind produces correct results on GCC- and Clang-compiled programs, including a Python interpreter, with a small number of exceptions. While we provide a list of scenarios that Derivgrind does not handle correctly, most of them are academic examples or originate from highly optimized math libraries. We will therefore further study the potential of our tool in more complex software projects.",https://arxiv.org/abs/2209.01900
Write Me and I'll Tell You Secrets -- Write-After-Write Effects OnIntel CPUs,"Jan PhilippThoma, TimGÃ¼neysu",05-sep-22,Cryptography and Security (cs.CR)," There is a long history of side channels in the memory hierarchy of modern CPUs. Especially the cache side channel is widely used in the context of transient execution attacks and covert channels. Therefore, many secure cache architectures have been proposed. Most of these architectures aim to make the construction of eviction sets infeasible by randomizing the address-to-cache mapping. In this paper, we investigate the peculiarities of write instructions in recent CPUs. We identify Write+Write, a new side channel on Intel CPUs that leaks whether two addresses contend for the same cache set. We show how Write+Write can be used for rapid construction of eviction sets on current cache architectures. Moreover, we replicate the Write+Write effect in gem5 and demonstrate on the example of ScatterCache how it can be exploited to efficiently attack state-of-the-art cache randomization schemes. In addition to the Write+Write side channel, we show how Write-After-Write effects can be leveraged to efficiently synchronize covert channel communication across CPU cores. This yields the potential for much more stealthy covert channel communication than before.",https://arxiv.org/abs/2209.01895
"A Survey on Open-Source-Defined Wireless Networks: Framework, KeyTechnology, and Implementation","LiqiangZhao, Muhammad MuhammadBala, WuGang, PanChengkang, YuanYannan, TianZhigang, Yu-CheeTseng, ChenXiang, BinShen, Chih-Lin I",05-sep-22,Networking and Internet Architecture (cs.NI)," The realization of open-source-defined wireless networks in the telecommunication domain is accomplished through the fifth-generation network (5G). In contrast to its predecessors (3G and 4G), the 5G network can support a wide variety of heterogeneous use cases with challenging requirements from both the Internet and the Internet of Things (IoT). The future sixth-generation (6G) network will not only extend 5G capabilities but also innovate new functionalities to address emerging academic and engineering challenges. The research community has identified these challenges could be overcome by open-source-defined wireless networks, which is based on open-source software and hardware. In this survey, we present an overview of different aspects of open-source-defined wireless networks, comprising motivation, frameworks, key technologies, and implementation. We start by introducing the motivation and explore several frameworks with classification into three different categories: black-box, grey-box, and white-box. We review research efforts related to open-source-defined Core Network (CN), Radio Access Network (RAN), Multi-access Edge Computing (MEC), the capabilities of security threats, open-source hardware, and various implementations, including testbeds. The last but most important in this survey, lessons learned, future research direction, open research issues, pitfalls, and limitations of existing surveys on open-source wireless networks are included to motivate and encourage future research.",https://arxiv.org/abs/2209.01894
Event-Triggered l2-Optimal Formation Control with State-Estimation forAgents Modeled as LPV Systems,"GeraldGebhardt, HamidehSaadabadi, HerbertWerner",05-sep-22,Systems and Control (eess.SY)," This paper proposes a distributed scheme with different estimators for the event-triggered formation control of polytopic homogeneously scheduled linear parameter-varying (LPV) multi-agent systems (MAS). Each agent consists of a time-triggered inner feedback loop and a larger event- triggered outer feedback loop to track a formation reference signal and reject input and output noise. If a local event-trigger condition is violated, the event-triggered outer feedback loop is closed through the communication network. The event-trigger condition is only based on locally available information. To design the controller, a synthesis problem is formulated as a linear matrix inequality of the size of a single agent under the assumption, that local estimators trigger intercommunication events with neighboring agents if the event-trigger condition is violated. The design procedure guarantees stability and bounded l2-performance. Furthermore, the estimators are interchangeable for a given controller. We compare in simulation zero-order hold, open-loop estimation, and closed-loop estimation strategies. Simulation trials are carried out with non-holonomic dynamic unicycles modeled as polytopic LPV systems.",https://arxiv.org/abs/2209.01891
A Formal Theory of Choreographic Programming,"LuÃ­s Cruz-Filipe, FabrizioMontesi, MarcoPeressotti",05-sep-22,Logic in Computer Science (cs.LO)," Choreographic programming is a paradigm for writing coordination plans for distributed systems from a global point of view, from which correct-by-construction decentralised implementations can be generated automatically.   Theory of choreographies typically includes a number of complex results that are proved by structural induction. The high number of cases and the subtle details in some of these proofs has led to important errors being found in published works.   In this work, we formalise the theory of a choreographic programming language in Coq. Our development includes the basic properties of this language, a proof of its Turing completeness, a compilation procedure to a process language, and an operational characterisation of the correctness of this procedure.   Our formalisation experience illustrates the benefits of using a theorem prover: we get both an additional degree of confidence from the mechanised proof, and a significant simplification of the underlying theory. Our results offer a foundation for the future formal development of choreographic languages.",https://arxiv.org/abs/2209.01888
PromptAttack: Prompt-based Attack for Language Models via GradientSearch,"YundiShi, PijiLi, ChangchunYin, ZhaoyangHan, LuZhou, ZheLiu",05-sep-22,Computation and Language (cs.CL)," As the pre-trained language models (PLMs) continue to grow, so do the hardware and data requirements for fine-tuning PLMs. Therefore, the researchers have come up with a lighter method called \textit{Prompt Learning}. However, during the investigations, we observe that the prompt learning methods are vulnerable and can easily be attacked by some illegally constructed prompts, resulting in classification errors, and serious security problems for PLMs. Most of the current research ignores the security issue of prompt-based methods. Therefore, in this paper, we propose a malicious prompt template construction method (\textbf{PromptAttack}) to probe the security performance of PLMs. Several unfriendly template construction approaches are investigated to guide the model to misclassify the task. Extensive experiments on three datasets and three PLMs prove the effectiveness of our proposed approach PromptAttack. We also conduct experiments to verify that our method is applicable in few-shot scenarios.",https://arxiv.org/abs/2209.01886
Semi-Supervised Domain Adaptation by Similarity based Pseudo-labelInjection,"AbhayRawat, IshaDua, SauravGupta, RahulTallamraju",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," One of the primary challenges in Semi-supervised Domain Adaptation (SSDA) is the skewed ratio between the number of labeled source and target samples, causing the model to be biased towards the source domain. Recent works in SSDA show that aligning only the labeled target samples with the source samples potentially leads to incomplete domain alignment of the target domain to the source domain. In our approach, to align the two domains, we leverage contrastive losses to learn a semantically meaningful and a domain agnostic feature space using the supervised samples from both domains. To mitigate challenges caused by the skewed label ratio, we pseudo- label the unlabeled target samples by comparing their feature representation to those of the labeled samples from both the source and target domains. Furthermore, to increase the support of the target domain, these potentially noisy pseudo-labels are gradually injected into the labeled target dataset over the course of training. Specifically, we use a temperature scaled cosine similarity measure to assign a soft pseudo-label to the unlabeled target samples. Additionally, we compute an exponential moving average of the soft pseudo-labels for each unlabeled sample. These pseudo-labels are progressively injected or removed) into the (from) the labeled target dataset based on a confidence threshold to supplement the alignment of the source and target distributions. Finally, we use a supervised contrastive loss on the labeled and pseudo-labeled datasets to align the source and target distributions. Using our proposed approach, we showcase state-of-the- art performance on SSDA benchmarks \- Office-Home, DomainNet and Office-31.",https://arxiv.org/abs/2209.01882
ScaleFace: Uncertainty-aware Deep Metric Learning,"RomanKail, KirillFedyanin, NikitaMuravev, AlexeyZaytsev, Maxim Panov",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The performance of modern deep learning-based systems dramatically depends on the quality of input objects. For example, face recognition quality would be lower for blurry or corrupted inputs. However, it is hard to predict the influence of input quality on the resulting accuracy in more complex scenarios. We propose an approach for deep metric learning that allows direct estimation of the uncertainty with almost no additional computational cost. The developed \textit{ScaleFace} algorithm uses trainable scale values that modify similarities in the space of embeddings. These input-dependent scale values represent a measure of confidence in the recognition result, thus allowing uncertainty estimation. We provide comprehensive experiments on face recognition tasks that show the superior performance of ScaleFace compared to other uncertainty-aware face recognition approaches. We also extend the results to the task of text-to- image retrieval showing that the proposed approach beats the competitors with significant margin.",https://arxiv.org/abs/2209.01881
A new T-compatibility condition and its application to thediscretization of the damped time-harmonic Galbrun's equation,"MartinHalla, ChristophLehrenfeld, PaulStocker",05-sep-22,Numerical Analysis (math.NA)," We consider the approximation of weakly T-coercive operators. The main property to ensure the convergence thereof is the regularity of the approximation (in the vocabulary of discrete approximation schemes). In a previous work the existence of discrete operators $T_n$ which converge to $T$ in a discrete norm was shown to be sufficient to obtain regularity. Although this framework proved usefull for many applications for some instances the former assumption is too strong. Thus in the present article we report a weaker criterium for which the discrete operators $T_n$ only have to converge point-wise, but in addition a weak T-coercivity condition has to be satisfied on the discrete level. We apply the new framework to prove the convergence of certain $H^1$-conforming finite element discretizations of the damped time-harmonic Galbrun's equation, which is used to model the oscillations of stars. A main ingredient in the latter analysis is the uniformly stable invertibility of the divergence operator on certain spaces, which is related to the topic of divergence free elements for the Stokes equation.",https://arxiv.org/abs/2209.01880
Performance optimization and analysis of the unstructuredDiscontinuous Galerkin solver on multi-core and many-core architectures,"ZheDai, LiangD, YueqinWang, FangWang, LiMing, JianZhang",05-sep-22,Mathematical Software (cs.MS)," The discontinuous Galerkin (DG) algorithm is a representative high order method in Computational Fluid Dynamics (CFD) area which possesses considerable mathematical advantages such as high resolution, low dissipation, and dispersion. However, DG is rather computationally intensive to demonstrate practical engineering problems. This paper discusses the implementation of our in-house practical DG application in three different programming models, as well as some optimization techniques, including grid renumbering and mixed precision to maximize the performance improvements in a single node system. The experiment on CPU and GPU shows that our CUDA, OpenACC, and OpenMP-based code obtains a maximum speedup of 42.9x, 35.3x, and 8.1x compared with serial execution by the original application, respectively. Besides, we systematically compare the programming models in two aspects: performance and productivity. Our empirical conclusions facilitate the programmers to select the right platform with a suitable programming model according to their target applications.",https://arxiv.org/abs/2209.01878
SlateFree: a Model-Free Decomposition for Reinforcement Learning withSlate Actions,AnastasiosGiovanidis,05-sep-22,Machine Learning (cs.LG)," We consider the problem of sequential recommendations, where at each step an agent proposes some slate of $N$ distinct items to a user from a much larger catalog of size $KN$. The user has unknown preferences towards the recommendations and the agent takes sequential actions that optimise (in our case minimise) some user-related cost, with the help of Reinforcement Learning. The possible item combinations for a slate is $\binom{K}{N}$, an enormous number rendering value iteration methods intractable. We prove that the slate-MDP can actually be decomposed using just $K$ item-related $Q$ functions per state, which describe the problem in a more compact and efficient way. Based on this, we propose a novel model- free SARSA and Q-learning algorithm that performs $N$ parallel iterations per step, without any prior user knowledge. We call this method \texttt{SlateFree}, i.e. free-of-slates, and we show numerically that it converges very fast to the exact optimum for arbitrary user profiles, and that it outperforms alternatives from the literature.",https://arxiv.org/abs/2209.01877
The Best Decisions Are Not the Best Advice: Making Adherence-AwareRecommendations,"Julien Grand-ClÃ©ment, JeanPauphilet","5 Sep 2022 (v1(https://arxiv.org/abs/2209.01874v1)), lastrevised 7 Sep 2022 (this version, v2)",Human-Computer Interaction (cs.HC)," Many high-stake decisions follow an expert-in-loop structure in that a human operator receives recommendations from an algorithm but is the ultimate decision maker. Hence, the algorithm's recommendation may differ from the actual decision implemented in practice. However, most algorithmic recommendations are obtained by solving an optimization problem that assumes recommendations will be perfectly implemented. We propose an adherence-aware optimization framework to capture the dichotomy between the recommended and the implemented policy and analyze the impact of partial adherence on the optimal recommendation. We show that overlooking the partial adherence phenomenon, as is currently being done by most recommendation engines, can lead to arbitrarily severe performance deterioration, compared with both the current human baseline performance and what is expected by the recommendation algorithm. Our framework also provides useful tools to analyze the structure and to compute optimal recommendation policies that are naturally immune against such human deviations, and are guaranteed to improve upon the baseline policy.",https://arxiv.org/abs/2209.01876
Induced Cycles and Paths Are Harder Than You Think,"MinaDalirrooyfard, Virginia VassilevskaWilliams",05-sep-22,Computational Complexity (cs.CC)," The goal of the paper is to give fine-grained hardness results for the Subgraph Isomorphism (SI) problem for fixed size induced patterns $H$, based on the $k$-Clique hypothesis that the current best algorithms for Clique are optimal.   Our first main result is that for any pattern graph $H$ that is a {\em core}, the SI problem for $H$ is at least as hard as $t$-Clique, where $t$ is the size of the largest clique minor of $H$. This improves (for cores) the previous known results [Dalirrooyfard-Vassilevska W. STOC'20] that the SI for $H$ is at least as hard as $k$-clique where $k$ is the size of the largest clique {\em subgraph} in $H$, or the chromatic number of $H$ (under the Hadwiger conjecture). For detecting \emph{any} graph pattern $H$, we further remove the dependency of the result of [Dalirrooyfard-Vassilevska W. STOC'20] on the Hadwiger conjecture at the cost of a sub-polynomial decrease in the lower bound.   The result for cores allows us to prove that the SI problem for induced $k$-Path and $k$-Cycle is harder than previously known. Previously [Floderus et al. Theor. CS 2015] had shown that $k$-Path and $k$-Cycle are at least as hard to detect as a $\lfloor k/2\rfloor$-Clique. We show that they are in fact at least as hard as $3k/4-O(1)$-Clique, improving the conditional lower bound exponent by a factor of $3/2$.   Finally, we provide a new conditional lower bound for detecting induced $4$-cycles: $n^{2-o(1)}$ time is necessary even in graphs with $n$ nodes and $O(n^{1.5})$ edges.",https://arxiv.org/abs/2209.01874
Impact of COVID-19 on human mobility and retail sales in the US,"Ayobami EstherOlanrewaju, Patrick E.McSharry",05-sep-22,Computers and Society (cs.CY)," Due to the COVID-19 pandemic, governments had to rapidly implement lockdown policies that restricted human mobility to suppress the spread of the disease and reduce mortality. Because of the movement restrictions resulting from government responses to the pandemic, US retail sales declined by -22% in April 2020 compared to the previous year. This study looks at the stringency of government policies, mobility patterns, and implied compliance levels. The relationships between these variables and the influence on retail sales serve to understand past human behavior and prepare for future pandemics. Retail losses varied dramatically across the US states, from -1.6% in Mississippi to -38.9% in Hawaii. States in the west and northeast were most affected, while those in the south were relatively resilient. Regression was used to identify statistically significant state- level characteristics. The greatest losses occurred in states with a high percentage of Democrat voters in the 2020 Presidential Election and those with large populations. A 10% increase in the Democrat vote is associated with a 2.4% increase in retail sales loss. States with a high percentage of adults with less than a high school diploma were most resilient. The number of trips of less than one-mile per capita is defined as the mobility index as it has the greatest influence on retail sales, on average, across the US states. An increase of 10% in this mobility index is associated with a 4.6% increase in retail sales. All states were generally compliant and exhibited reduced mobility with increasing stringency. A rise of 1% in the stringency index is associated with a decline of 1% in the mobility index. States with a high percentage of Democrat voters, large populations, and located in the west tend to be most compliant. A 10% rise in the proportion of people voting Democrat is associated with a 5% increase in compliance.",https://arxiv.org/abs/2209.01873
Unsupervised Domain Adaptation via Style-Aware Self-intermediateDomain,"LianyuWang, MengWang, DaoqiangZhang, Huazhu Fu",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Unsupervised domain adaptation (UDA) has attracted considerable attention, which transfers knowledge from a label-rich source domain to a related but unlabeled target domain. Reducing inter-domain differences has always been a crucial factor to improve performance in UDA, especially for tasks where there is a large gap between source and target domains. To this end, we propose a novel style-aware feature fusion method (SAFF) to bridge the large domain gap and transfer knowledge while alleviating the loss of class-discriminative information. Inspired by the human transitive inference and learning ability, a novel style-aware self-intermediate domain (SSID) is investigated to link two seemingly unrelated concepts through a series of intermediate auxiliary synthesized concepts. Specifically, we propose a novel learning strategy of SSID, which selects samples from both source and target domains as anchors, and then randomly fuses the object and style features of these anchors to generate labeled and style-rich intermediate auxiliary features for knowledge transfer. Moreover, we design an external memory bank to store and update specified labeled features to obtain stable class features and class-wise style features. Based on the proposed memory bank, the intra- and inter-domain loss functions are designed to improve the class recognition ability and feature compatibility, respectively. Meanwhile, we simulate the rich latent feature space of SSID by infinite sampling and the convergence of the loss function by mathematical theory. Finally, we conduct comprehensive experiments on commonly used domain adaptive benchmarks to evaluate the proposed SAFF, and the experimental results show that the proposed SAFF can be easily combined with different backbone networks and obtain better performance as a plug-in-plug-out module.",https://arxiv.org/abs/2209.01871
Texture image analysis based on joint of multi directions GLCM andlocal ternary patterns,"Akshakhi KumarPritoonka, Faeze Kiani",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Human visual brain use three main component such as color, texture and shape to detect or identify environment and objects. Hence, texture analysis has been paid much attention by scientific researchers in last two decades. Texture features can be used in many different applications in commuter vision or machine learning problems. Since now, many different approaches have been proposed to classify textures. Most of them consider the classification accuracy as the main challenge that should be improved. In this article, a new approach is proposed based on combination of two efficient texture descriptor, co-occurrence matrix and local ternary patterns (LTP). First of all, basic local binary pattern and LTP are performed to extract local textural information. Next, a subset of statistical features is extracted from gray-level co-occurrence matrixes. Finally, concatenated features are used to train classifiers. The performance is evaluated on Brodatz benchmark dataset in terms of accuracy. Experimental results show that proposed approach provide higher classification rate in comparison with some state-of-the-art approaches.",https://arxiv.org/abs/2209.01870
Power Allocation for Joint Communication and Sensing in Cell-FreeMassive MIMO,"ZinatBehdad, Ã–zlem TuÄŸfeDemir, Ki WonSung, EmilBjÃ¶rnson, CicekCavdar",05-sep-22,Information Theory (cs.IT)," This paper studies a joint communication and sensing (JCAS) system with downlink communication and multi-static sensing for single-target detection in a cloud radio access network architecture. A centralized operation of cell-free massive MIMO is considered for communication and sensing purposes. The JCAS transmit access points (APs) jointly serve the user equipments (UEs) and optionally steer a beam towards the target. A maximum a posteriori ratio test detector is derived to detect the target using signals received at distributed APs. We propose a power allocation algorithm to maximize the sensing signal-to-noise ratio under the condition that a minimal signal-to-interference-plus-noise ratio value for each UE is guaranteed. Numerical results show that, compared to the fully communication-centric power allocation, the detection probability under a certain false alarm probability can be increased significantly by the proposed algorithm for both JCAS setups: i) using additional sensing symbols or ii) using only existing communication symbols.",https://arxiv.org/abs/2209.01866
Online B-Matchings for Reconfigurable Datacenters: The Power ofRandomization,"MarcinBienkowski, DavidFuchssteiner, StefanSchmid",05-sep-22,Networking and Internet Architecture (cs.NI)," This paper studies the problem of how to dynamically optimize the topology of a reconfigurable datacenter network in a demand-aware manner: by accounting for the current traffic pattern and providing direct connectivity between frequently communicating racks, the datacenter's bandwidth efficiency can be improved. The underlying algorithmic challenge can be described as an online maximum weight $b$-matching problem, a generalization of maximum weight matching where each node has at most $b \geq 1$ incident matching edges. Recently, Bienkowski et al.~presented an $O(b)$-competitive deterministic algorithm and showed that this is asymptotically optimal. This paper initiates the study of randomized algorithms, and we present a $O(\log b)$-competitive solution and show that this is asymptotically optimal. We complement our theoretical results with extensive trace-driven simulations, based on real-world datacenter workloads.",https://arxiv.org/abs/2209.01864
A Brief History of Recommender Systems,"ZhenhuaDong, ZheWang, JunXu, RuimingTang, JirongWen",05-sep-22,Information Retrieval (cs.IR)," Soon after the invention of the Internet, the recommender system emerged and related technologies have been extensively studied and applied by both academia and industry. Currently, recommender system has become one of the most successful web applications, serving billions of people in each day through recommending different kinds of contents, including news feeds, videos, e-commerce products, music, movies, books, games, friends, jobs etc. These successful stories have proved that recommender system can transfer big data to high values. This article briefly reviews the history of web recommender systems, mainly from two aspects: (1) recommendation models, (2) architectures of typical recommender systems. We hope the brief review can help us to know the dots about the progress of web recommender systems, and the dots will somehow connect in the future, which inspires us to build more advanced recommendation services for changing the world better.",https://arxiv.org/abs/2209.01863
Private Simultaneous Messages Based on Quadratic Residues,"KazumasaShinagawa, ReoEriguchi, ShoheiSatake, Koji Nuida",05-sep-22,Cryptography and Security (cs.CR)," Private Simultaneous Messages (PSM) model is a minimal model for secure multiparty computation. Feige, Kilian, and Naor (STOC 1994) and Ishai (Cryptology and Information Security Series 2013) constructed PSM protocols based on quadratic residues. In this paper, we define QR-PSM protocols as a generalization of these protocols. A QR-PSM protocol is a PSM protocol whose decoding function outputs the quadratic residuosity of what is computed from messages. We design a QR-PSM protocol for any symmetric function $f: \\{0,1\\}^n \rightarrow \\{0,1\\}$ of communication complexity $O(n^2)$. As far as we know, it is the most efficient PSM protocol since the previously known best PSM protocol was of $O(n^2\log n)$ (Beimel et al., CRYPTO 2014). We also study the sizes of the underlying finite fields $\mathbb{F}_p$ in the protocols since the communication complexity of a QR-PSM protocol is proportional to the bit length of the prime $p$. In particular, we show that the $N$-th Peralta prime $P_N$, which is used for general QR-PSM protocols, can be taken as at most $(1+o(1))N^2 2^{2N-2}$, which improves the Peralta's known result (Mathematics of Computation 1992) by a constant factor $(1+\sqrt{2})^2$.",https://arxiv.org/abs/2209.01860
Consistency-Based Semi-supervised Evidential Active Learning forDiagnostic Radiograph Classification,"ShafaBalaram, Cuong M.Nguyen, AshrafKassim, PavitraKrishnaswamy",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Deep learning approaches achieve state-of-the-art performance for classifying radiology images, but rely on large labelled datasets that require resource-intensive annotation by specialists. Both semi-supervised learning and active learning can be utilised to mitigate this annotation burden. However, there is limited work on combining the advantages of semi- supervised and active learning approaches for multi-label medical image classification. Here, we introduce a novel Consistency-based Semi-supervised Evidential Active Learning framework (CSEAL). Specifically, we leverage predictive uncertainty based on theories of evidence and subjective logic to develop an end-to-end integrated approach that combines consistency-based semi-supervised learning with uncertainty-based active learning. We apply our approach to enhance four leading consistency-based semi-supervised learning methods: Pseudo-labelling, Virtual Adversarial Training, Mean Teacher and NoTeacher. Extensive evaluations on multi-label Chest X-Ray classification tasks demonstrate that CSEAL achieves substantive performance improvements over two leading semi-supervised active learning baselines. Further, a class-wise breakdown of results shows that our approach can substantially improve accuracy on rarer abnormalities with fewer labelled samples.",https://arxiv.org/abs/2209.01859
Leveraging Layout-based Effects for Locking Analog ICs,"Muayad J.Aljafar, FlorenceAzais, Marie-LiseFlottes, SamuelPagliarini",05-sep-22,Cryptography and Security (cs.CR)," While various obfuscation methods exist in the digital domain, techniques for protecting Intellectual Property (IP) in the analog domain are mostly overlooked. Understandably, analog components have a small footprint as most of the surface of an Integrated Circuit (IC) is digital. Yet, since they are challenging to design and tune, they constitute a valuable IP that ought to be protected. This paper is the first to show a method to secure analog IP by exploiting layout-based effects that are typically seen as undesirable detractors in IC design. Specifically, we make use of the effects of Length of Oxide Diffusion and Well Proximity Effect on transistors for tuning the devices' critical parameters (e.g., gm and Vth). Such parameters are hidden behind key inputs, akin to the logic locking approach for digital ICs. The proposed technique is applied for locking an Operational Transconductance Amplifier. In order to showcase the robustness of the achieved obfuscation, the case studied circuit is simulated for a large number of key sets, i.e., 50K and 300K, and the results show a wide range of degradation in open-loop gain (up to 130dB), phase margin (up to 50 deg), 3dB bandwidth (approx. 2.5MHz), and power (approx. 1mW) of the locked circuit when incorrect keys are applied. Our results show the benefit of the technique and the incurred overheads. We also justify the non-effectiveness of reverse engineering efforts for attacking the proposed approach. More importantly, our technique employs only regular transistors and requires neither changes to the IC fabrication process nor any foundry-level coordination or trust.",https://arxiv.org/abs/2209.01858
Recognizing Geometric Intersection Graphs Stabbed by a Line,"DibyayanChakraborty, KshitijGajjar, Irena Rusu",05-sep-22,Discrete Mathematics (cs.DM)," In this paper, we determine the computational complexity of recognizing two graph classes, grounded \textsc{L}-graphs and stabbable grid intersection graphs. An \textsc{L}-shape is made by joining the bottom end- point of a vertical ($\vert$) segment to the left end-point of a horizontal ($-$) segment. The top end-point of the vertical segment is known as the {\em anchor} of the \textsc{L}-shape. Grounded \textsc{L}-graphs are the intersection graphs of \textsc{L}-shapes such that all the \textsc{L}-shapes' anchors lie on the same horizontal line. We show that recognizing grounded \textsc{L}-graphs is NP-complete. This answers an open question asked by Jel{\'Ä±}nek \& T{Ã¶}pfer (Electron. J. Comb., 2019).   Grid intersection graphs are the intersection graphs of axis-parallel line segments in which two vertical (similarly, two horizontal) segments cannot intersect. We say that a (not necessarily axis-parallel) straight line $\ell$ stabs a segment $s$, if $s$ intersects $\ell$. A graph $G$ is a stabbable grid intersection graph (\textsc{StabGIG}) if there is a grid intersection representation of $G$ in which the same line stabs all its segments. We show that recognizing \textsc{StabGIG} graphs is NP-complete, even when the input graphs are restricted to be bipartite apex graphs of large (but constant) girth. This answers an open question asked by Chaplick \textit{et al.} (\textsc{O}rder, 2018).",https://arxiv.org/abs/2209.01856
A Fault Resilient Approach to Non-collective Communication Creation inMPI,"RobertoRocco, GianlucaPalermo",05-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," The increasing size of HPC architectures makes the faults' presence an eventuality more and more frequent. This is especially relevant since MPI, the de-facto standard for inter-process communication lacks proper fault management functionalities. The past efforts produced extensions to the MPI standard that enabled fault management, the most important one being ULFM. In this paper, we introduce the support for non- collective communication creation (MPI_Comm_create_group) in ULFM to improve the fault management capabilities. We integrate our solution into the Legio library and measure the overhead introduced in the application. The proposed solution removes the possibility of turning the execution into a deadlock after a fault and can be used as an inspiring effort to improve the ULFM repair capabilities.",https://arxiv.org/abs/2209.01851
A Principled Evaluation Protocol for Comparative Investigation of theEffectiveness of DNN Classification Models on Similar-but-non-identicalDatasets,"Esla TimothyAnzaku, Haohan Wang, Arnout VanMessem, Wesley DeNeve",05-sep-22,Machine Learning (cs.LG)," Deep Neural Network (DNN) models are increasingly evaluated using new replication test datasets, which have been carefully created to be similar to older and popular benchmark datasets. However, running counter to expectations, DNN classification models show significant, consistent, and largely unexplained degradation in accuracy on these replication test datasets. While the popular evaluation approach is to assess the accuracy of a model by making use of all the datapoints available in the respective test datasets, we argue that doing so hinders us from adequately capturing the behavior of DNN models and from having realistic expectations about their accuracy. Therefore, we propose a principled evaluation protocol that is suitable for performing comparative investigations of the accuracy of a DNN model on multiple test datasets, leveraging subsets of datapoints that can be selected using different criteria, including uncertainty-related information. By making use of this new evaluation protocol, we determined the accuracy of $564$ DNN models on both (1) the CIFAR-10 and ImageNet datasets and (2) their replication datasets. Our experimental results indicate that the observed accuracy degradation between established benchmark datasets and their replications is consistently lower (that is, models do perform better on the replication test datasets) than the accuracy degradation reported in published works, with these published works relying on conventional evaluation approaches that do not utilize uncertainty- related information.",https://arxiv.org/abs/2209.01849
Conflict-Aware Pseudo Labeling via Optimal Transport for EntityAlignment,"QijieDing, DaokunZhang, JieYin",05-sep-22,Machine Learning (cs.LG)," Entity alignment aims to discover unique equivalent entity pairs with the same meaning across different knowledge graphs (KG). It has been a compelling but challenging task for knowledge integration or fusion. Existing models have primarily focused on projecting KGs into a latent embedding space to capture inherent semantics between entities for entity alignment. However, the adverse impacts of alignment conflicts have been largely overlooked during training, thus limiting the entity alignment performance. To address this issue, we propose a novel Conflict-aware Pseudo Labeling via Optimal Transport model (CPL-OT) for entity alignment. The key idea of CPL-OT is to iteratively pseudo-label alignment pairs empowered with conflict-aware Optimal Transport modeling to boost the precision of entity alignment. CPL-OT is composed of two key components-entity embedding learning with global-local aggregation and iterative conflict-aware pseudo labeling-that mutually reinforce each other. To mitigate alignment conflicts during pseudo labeling, we propose to use optimal transport (OT) as an effective means to warrant one-to-one entity alignment between two KGs with the minimal overall transport cost. The transport cost is calculated as the rectified distance between entity embeddings obtained via graph convolution augmented with global-level semantics. Extensive experiments on benchmark datasets show that CPL-OT can markedly outperform state-of-the-art baselines under both settings with and without prior alignment seeds.",https://arxiv.org/abs/2209.01848
Evaluating Situated Visualization in AR with Eye Tracking,"KunoKurzhals, MichaelBecher, NelusaPathmanathan, Guido Reina",05-sep-22,Human-Computer Interaction (cs.HC)," Augmented reality (AR) technology provides means for embedding visualization in a real-world context. Such techniques allow situated analyses of live data in their spatial domain. However, as existing techniques have to be adapted for this context and new approaches will be developed, the evaluation thereof poses new challenges for researchers. Apart from established performance measures, eye tracking has proven to be a valuable means to assess visualizations qualitatively and quantitatively. We discuss the challenges and opportunities of eye tracking for the evaluation of situated visualizations. We envision that an extension of gaze-based evaluation methodology into this field will provide new insights on how people perceive and interact with visualizations in augmented reality.",https://arxiv.org/abs/2209.01847
RunPHI: Enabling Mixed-criticality Containers via PartitioningHypervisors in Industry 4.0,"MarcoBarletta, MarcelloCinque, Luigi DeSimone, Raffaele DellaCorte, GiorgioFarina, DanieleOttaviano",05-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," Orchestration systems are becoming a key component to automatically manage distributed computing resources in many fields with criticality requirements like Industry 4.0 (I4.0). However, they are mainly linked to OS-level virtualization, which is known to suffer from reduced isolation. In this paper, we propose RunPHI with the aim of integrating partitioning hypervisors, as a solution for assuring strong isolation, with OS-level orchestration systems. The purpose is to enable container orchestration in mixed-criticality systems with isolation requirements through partitioned containers.",https://arxiv.org/abs/2209.01846
Dynamics of Fourier Modes in Torus Generative Adversarial Networks,"Ãngel GonzÃ¡lez-Prieto, AlbertoMozo, EdgarTalavera, Sandra GÃ³mez-Canaval",05-sep-22,Machine Learning (cs.LG)," Generative Adversarial Networks (GANs) are powerful Machine Learning models capable of generating fully synthetic samples of a desired phenomenon with a high resolution. Despite their success, the training process of a GAN is highly unstable and typically it is necessary to implement several accessory heuristics to the networks to reach an acceptable convergence of the model. In this paper, we introduce a novel method to analyze the convergence and stability in the training of Generative Adversarial Networks. For this purpose, we propose to decompose the objective function of the adversary min-max game defining a periodic GAN into its Fourier series. By studying the dynamics of the truncated Fourier series for the continuous Alternating Gradient Descend algorithm, we are able to approximate the real flow and to identify the main features of the convergence of the GAN. This approach is confirmed empirically by studying the training flow in a $2$-parametric GAN aiming to generate an unknown exponential distribution. As byproduct, we show that convergent orbits in GANs are small perturbations of periodic orbits so the Nash equillibria are spiral attractors. This theoretically justifies the slow and unstable training observed in GANs.",https://arxiv.org/abs/2209.01843
Which structure of academic articles do referees pay more attentionto?: perspective of peer review and full-text of academic articles,"ChengleiQin, ChengzhiZhang",05-sep-22,Computation and Language (cs.CL)," Purpose   The purpose of this paper is to explore which structures of academic articles referees would pay more attention to, what specific content referees focus on, and whether the distribution of PRC is related to the citations. Design/methodology/approach   Firstly, utilizing the feature words of section title and hierarchical attention network model (HAN) to identify the academic article structures. Secondly, analyzing the distribution of PRC in different structures according to the position information extracted by rules in PRC. Thirdly, analyzing the distribution of feature words of PRC extracted by the Chi- square test and TF-IDF in different structures. Finally, four correlation analysis methods are used to analyze whether the distribution of PRC in different structures is correlated to the citations. Findings   The count of PRC distributed in Materials and Methods and Results section is significantly more than that in the structure of Introduction and Discussion, indicating that referees pay more attention to the Material and Methods and Results. The distribution of feature words of PRC in different structures is obviously different, which can reflect the content of referees' concern. There is no correlation between the distribution of PRC in different structures and the citations. Research limitations/implications   Due to the differences in the way referees write peer review reports, the rules used to extract position information cannot cover all PRC. Originality/value   The paper finds a pattern in the distribution of PRC in different academic article structures proving the long-term empirical understanding. It also provides insight into academic article writing: researchers should ensure the scientificity of methods and the reliability of results when writing academic article to obtain a high degree of recognition from referees.",https://arxiv.org/abs/2209.01842
Effective Estimation of the Dimensions of a Manifold from RandomSamples,"LucienGrillet, Juan Souto","5 Sep 2022 (v1(https://arxiv.org/abs/2209.01839v1)), lastrevised 7 Sep 2022 (this version, v2)",Computational Geometry (cs.CG)," We give explicit theoretical and heuristical bounds for how big does a data set sampled from a reach-1 submanifold M of euclidian space need to be, to be able to estimate the dimension of M with 90% confidence.",https://arxiv.org/abs/2209.01841
A Benchmark for Unsupervised Anomaly Detection in Multi-AgentTrajectories,"JulianWiederer, JulianSchmidt, UlrichKressel, KlausDietmayer, VasileiosBelagiannis",05-sep-22,Robotics (cs.RO)," Human intuition allows to detect abnormal driving scenarios in situations they never experienced before. Like humans detect those abnormal situations and take countermeasures to prevent collisions, self-driving cars need anomaly detection mechanisms. However, the literature lacks a standard benchmark for the comparison of anomaly detection algorithms. We fill the gap and propose the R-U-MAAD benchmark for unsupervised anomaly detection in multi-agent trajectories. The goal is to learn a representation of the normal driving from the training sequences without labels, and afterwards detect anomalies. We use the Argoverse Motion Forecasting dataset for the training and propose a test dataset of 160 sequences with human-annotated anomalies in urban environments. To this end we combine a replay of real- world trajectories and scene-dependent abnormal driving in the simulation. In our experiments we compare 11 baselines including linear models, deep auto-encoders and one-class classification models using standard anomaly detection metrics. The deep reconstruction and end-to-end one-class methods show promising results. The benchmark and the baseline models will be publicly available.",https://arxiv.org/abs/2209.01839
Multi-Figurative Language Generation,"HuiyuanLai, MalvinaNissim",05-sep-22,Computation and Language (cs.CL)," Figurative language generation is the task of reformulating a given text in the desired figure of speech while still being faithful to the original context. We take the first step towards multi-figurative language modelling by providing a benchmark for the automatic generation of five common figurative forms in English. We train mFLAG employing a scheme for multi-figurative language pre-training on top of BART, and a mechanism for injecting the target figurative information into the encoder; this enables the generation of text with the target figurative form from another figurative form without parallel figurative-figurative sentence pairs. Our approach outperforms all strong baselines. We also offer some qualitative analysis and reflections on the relationship between the different figures of speech.",https://arxiv.org/abs/2209.01838
Lattice-based shape tracking and servoing of elastic objects,"Mohammadreza Shetab-Bushehri, MiguelAranda, YoucefMezouar, Erol Ozgur","5 Sep 2022 (v1(https://arxiv.org/abs/2209.01832v1)), lastrevised 7 Sep 2022 (this version, v2)",Robotics (cs.RO)," In this paper, we propose a general unified tracking-servoing approach for controlling the shape of elastic deformable objects using robotic arms. Our approach works by forming a lattice around the object, binding the object to the lattice, and tracking and servoing the lattice instead of the object. This makes our approach have full 3D control over deformable objects of any general form (linear, thin-shell, volumetric). Furthermore, it decouples the runtime complexity of the approach from the objects' geometric complexity. Our approach is based on the As-Rigid-As- Possible (ARAP) deformation model. It requires no mechanical parameter of the object to be known and can drive the object toward desired shapes through large deformations. The inputs to our approach are the point cloud of the object's surface in its rest shape and the point cloud captured by a 3D camera in each frame. Overall, our approach is more broadly applicable than existing approaches. We validate the efficiency of our approach through numerous experiments with deformable objects of various shapes and materials (paper, rubber, plastic, foam). Experiment videos are available on the project website: [this https URL](https://sites.google.com/view/tracking- servoing-approach).",https://arxiv.org/abs/2209.01835
Minimization of differential equations and algebraic values of$E$-functions,"AlinBostan, TanguyRivoal, Bruno Salvy",05-sep-22,Symbolic Computation (cs.SC)," A power series being given as the solution of a linear differential equation with appropriate initial conditions, minimization consists in finding a non-trivial linear differential equation of minimal order having this power series as a solution. This problem exists in both homogeneous and inhomogeneous variants; it is distinct from, but related to, the classical problem of factorization of differential operators. Recently, minimization has found applications in Transcendental Number Theory, more specifically in the computation of non-zero algebraic points where Siegel's $E$-functions take algebraic values. We present algorithms for these questions and discuss implementation and experiments.",https://arxiv.org/abs/2209.01832
Detecting unjustified assumptions in subclasses via EO representation,"VitaliyKorbashov, NikolaiKudasov, MikhailOlokin, Violetta Sim",05-sep-22,Programming Languages (cs.PL)," Elegant Objects (EO) is a programming language based on ideas of pure objects and the Decorator pattern. It has been suggested by Bugayenko as an intermediate representation for object-oriented programs. This paper presents a version of dynamic dispatch modelled in EO and formulates a problem of unjustified assumptions in decorator objects, which parallels similar problem in subclasses. Then, we introduce an approach to detect such problems in EO programs via method inlining and limited property inference. Finally, we discuss prototype implementation of this approach in Scala programming language.",https://arxiv.org/abs/2209.01827
A Survey on Measuring and Mitigating Reasoning Shortcuts in MachineReading Comprehension,"XanhHo, JohannesMarioMeissner, SakuSugawara, AkikoAizawa",05-sep-22,Computation and Language (cs.CL)," The issue of shortcut learning is widely known in NLP and has been an important research focus in recent years. Unintended correlations in the data enable models to easily solve tasks that were meant to exhibit advanced language understanding and reasoning capabilities. In this survey paper, we focus on the field of machine reading comprehension (MRC), an important task for showcasing high-level language understanding that also suffers from a range of shortcuts. We summarize the available techniques for measuring and mitigating shortcuts and conclude with suggestions for further progress in shortcut research. Most importantly, we highlight two main concerns for shortcut mitigation in MRC: the lack of public challenge sets, a necessary component for effective and reusable evaluation, and the lack of certain mitigation techniques that are prominent in other areas.",https://arxiv.org/abs/2209.01825
Natural Policy Gradients In Reinforcement Learning Explained,W.J.A. vanHeeswijk,05-sep-22,Machine Learning (cs.LG)," Traditional policy gradient methods are fundamentally flawed. Natural gradients converge quicker and better, forming the foundation of contemporary Reinforcement Learning such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO). This lecture note aims to clarify the intuition behind natural policy gradients, focusing on the thought process and the key mathematical constructs.",https://arxiv.org/abs/2209.01824
An Exploratory Study on the Predominant Programming Paradigms inPython Code,"RobertDyer, JigyasaChauhan",05-sep-22,Software Engineering (cs.SE)," Python is a multi-paradigm programming language that fully supports object-oriented (OO) programming. The language allows writing code in a non-procedural imperative manner, using procedures, using classes, or in a functional style. To date, no one has studied what paradigm(s), if any, are predominant in Python code and projects. In this work, we first define a technique to classify Python files into predominant paradigm(s). We then automate our approach and evaluate it against human judgements, showing over 80% agreement. We then analyze over 100k open-source Python projects, automatically classifying each source file and investigating the paradigm distributions. The results indicate Python developers tend to heavily favor OO features. We also observed a positive correlation between OO and procedural paradigms and the size of the project. And despite few files or projects being predominantly functional, we still found many functional feature uses.",https://arxiv.org/abs/2209.01820
ADTR: Anomaly Detection Transformer with Feature Reconstruction,"ZhiyuanYou, KaiYang, WenhanLuo, LeiCui, YuZheng, XinyiLe",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Anomaly detection with only prior knowledge from normal samples attracts more attention because of the lack of anomaly samples. Existing CNN-based pixel reconstruction approaches suffer from two concerns. First, the reconstruction source and target are raw pixel values that contain indistinguishable semantic information. Second, CNN tends to reconstruct both normal samples and anomalies well, making them still hard to distinguish. In this paper, we propose Anomaly Detection TRansformer (ADTR) to apply a transformer to reconstruct pre-trained features. The pre-trained features contain distinguishable semantic information. Also, the adoption of transformer limits to reconstruct anomalies well such that anomalies could be detected easily once the reconstruction fails. Moreover, we propose novel loss functions to make our approach compatible with the normal-sample-only case and the anomaly-available case with both image-level and pixel-level labeled anomalies. The performance could be further improved by adding simple synthetic or external irrelevant anomalies. Extensive experiments are conducted on anomaly detection datasets including MVTec-AD and CIFAR-10. Our method achieves superior performance compared with all baselines.",https://arxiv.org/abs/2209.01817
Query-focused Extractive Summarisation for Biomedical and COVID-19Complex Question Answering,DiegoMollÃ¡,05-sep-22,Computation and Language (cs.CL)," This paper presents Macquarie University's participation to the two most recent BioASQ Synergy Tasks (as per June 2022), and to the BioASQ10 Task~B (BioASQ10b), Phase~B. In these tasks, participating systems are expected to generate complex answers to biomedical questions, where the answers may contain more than one sentence. We apply query-focused extractive summarisation techniques. In particular, we follow a sentence classification-based approach that scores each candidate sentence associated to a question, and the $n$ highest-scoring sentences are returned as the answer. The Synergy Task corresponds to an end-to-end system that requires document selection, snippet selection, and finding the final answer, but it has very limited training data. For the Synergy task, we selected the candidate sentences following two phases: document retrieval and snippet retrieval, and the final answer was found by using a DistilBERT/ALBERT classifier that had been trained on the training data of BioASQ9b. Document retrieval was achieved as a standard search over the CORD-19 data using the search API provided by the BioASQ organisers, and snippet retrieval was achieved by re-ranking the sentences of the top retrieved documents, using the cosine similarity of the question and candidate sentence. We observed that vectors represented via sBERT have an edge over tf.idf. BioASQ10b Phase B focuses on finding the specific answers to biomedical questions. For this task, we followed a data-centric approach. We hypothesised that the training data of the first BioASQ years might be biased and we experimented with different subsets of the training data. We observed an improvement of results when the system was trained on the second half of the BioASQ10b training data.",https://arxiv.org/abs/2209.01816
RLIP: Relational Language-Image Pre-training for Human-ObjectInteraction Detection,"HangjieYuan, JianwenJiang, SamuelAlbanie, Tao Feng, ZiyuanHuang, DongNi, MingqianTang",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The task of Human-Object Interaction (HOI) detection targets fine- grained visual parsing of humans interacting with their environment, enabling a broad range of applications. Prior work has demonstrated the benefits of effective architecture design and integration of relevant cues for more accurate HOI detection. However, the design of an appropriate pre- training strategy for this task remains underexplored by existing approaches. To address this gap, we propose Relational Language-Image Pre- training (RLIP), a strategy for contrastive pre-training that leverages both entity and relation descriptions. To make effective use of such pre- training, we make three technical contributions: (1) a new Parallel entity detection and Sequential relation inference (ParSe) architecture that enables the use of both entity and relation descriptions during holistically optimized pre-training; (2) a synthetic data generation framework, Label Sequence Extension, that expands the scale of language data available within each minibatch; (3) mechanisms to account for ambiguity, Relation Quality Labels and Relation Pseudo-Labels, to mitigate the influence of ambiguous/noisy samples in the pre-training data. Through extensive experiments, we demonstrate the benefits of these contributions, collectively termed RLIP-ParSe, for improved zero-shot, few-shot and fine- tuning HOI detection performance as well as increased robustness to learning from noisy annotations. Code will be available at \url{[this https URL](https://github.com/JacobYuan7/RLIP)}.",https://arxiv.org/abs/2209.01815
Automatic Estimation of Self-Reported Pain by Trajectory Analysis inthe Manifold of Fixed Rank Positive Semi-Definite Matrices,"BenjaminSzczapa, MohamedDaoudi, StefanoBerretti, Pietro Pala, Alberto DelBimbo, ZakiaHammal",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," We propose an automatic method to estimate self-reported pain based on facial landmarks extracted from videos. For each video sequence, we decompose the face into four different regions and the pain intensity is measured by modeling the dynamics of facial movement using the landmarks of these regions. A formulation based on Gram matrices is used for representing the trajectory of landmarks on the Riemannian manifold of symmetric positive semi-definite matrices of fixed rank. A curve fitting algorithm is used to smooth the trajectories and temporal alignment is performed to compute the similarity between the trajectories on the manifold. A Support Vector Regression classifier is then trained to encode extracted trajectories into pain intensity levels consistent with self-reported pain intensity measurement. Finally, a late fusion of the estimation for each region is performed to obtain the final predicted pain level. The proposed approach is evaluated on two publicly available datasets, the UNBCMcMaster Shoulder Pain Archive and the Biovid Heat Pain dataset. We compared our method to the state-of-the-art on both datasets using different testing protocols, showing the competitiveness of the proposed approach.",https://arxiv.org/abs/2209.01814
Automatic Estimation of Self-Reported Pain by InterpretableRepresentations of Motion Dynamics,"BenjaminSzczapa, MohamedDaoudi, StefanoBerretti, Pietro Pala, Alberto DelBimbo, ZakiaHammal",24-jun-20,Computer Vision and Pattern Recognition (cs.CV)," We propose an automatic method for pain intensity measurement from video. For each video, pain intensity was measured using the dynamics of facial movement using 66 facial points. Gram matrices formulation was used for facial points trajectory representations on the Riemannian manifold of symmetric positive semi-definite matrices of fixed rank. Curve fitting and temporal alignment were then used to smooth the extracted trajectories. A Support Vector Regression model was then trained to encode the extracted trajectories into ten pain intensity levels consistent with the Visual Analogue Scale for pain intensity measurement. The proposed approach was evaluated using the UNBC McMaster Shoulder Pain Archive and was compared to the state-of-the-art on the same data. Using both 5-fold cross-validation and leave-one-subject-out cross-validation, our results are competitive with respect to state-of-the-art methods.",https://arxiv.org/abs/2209.01813
Detecting unanticipated mutual recursion using Elegant Objectsrepresentation of object-oriented programs,"NikolaiKudasov, MikhailOlokin, OleksiiPotyomkin, NikolayShilov, MaximStepanov",05-sep-22,Programming Languages (cs.PL)," Elegant Objects (EO) is a variation of the object-oriented programming paradigm that favors pure objects and decoration. EO programming language is based on these ideas and has been suggested by Bugayenko as an intermediate representation for object-oriented programs. This paper provides plausible representations in EO of some class-based constructions from Java, C++, and Python. We then reformulate the classical fragile base class problem in the context of these representations. Finally, we discuss an algorithm for detecting a subset of fragile base class patterns in EO programs. We show that using EO as an intermediate language is plausible and discuss possible improvements to the language to assist in richer static analysis.",https://arxiv.org/abs/2006.13882
Towards Zero Touch Networks: From the Perspective of HierarchicalLanguage Systems,"GuozhiLin, JingguoGe, YuleiWu",05-sep-22,Networking and Internet Architecture (cs.NI)," With ever-increasing complexity and dynamicity of communication networks, intelligent network operation and maintenance has become more important to network operators. With the fast development of artificial intelligence, concepts such as ""Zero Touch"", ""Intent-based"", ""Knowledge- defined"" and ""Self-driving"" networks have become well-known in networking community for a great vision of making networks automatically manageable and responsive to user demands. This article discusses how to achieve Zero Touch Networks from the perspective of language-like systems. We propose a novel hierarchical `language' framework dedicated for networks to enable the Zero Touch Network, which covers from symbolizing network components, a unified framework for understanding network systems, to the logical description with network semantics. A case study based on the proposed language framework is provided. Finally, we discuss the challenges and open issues of intelligence models for zero touch networks.",https://arxiv.org/abs/2209.01803
Impact of 4ir technology and its impact on the current deployment,"BandarAlsulaimani, Amanul Islam",05-sep-22,Cryptography and Security (cs.CR)," The Fourth Industrial Revolution represents a fundamental change in how we live, work, and relate to one another. It is a new chapter in human development with remarkable technological advancements comparable to those of the first, second, and third industrial revolutions. These developments are fusing the physical, digital, and biological worlds in ways that hold great promise as well as the possibility of great danger. The way that modern people live and work is changing as a result of disruptive technologies and trends including the Internet of Things (IoT), robotics, virtual reality (VR), and artificial intelligence (AI). This is known as the fourth industrial revolution. Industry 4.0 refers to the incorporation of these technologies into production processes. In this article, we discussed the history of 4IR technology, its impact of 4IR technology, and its impact on the current deployment.",https://arxiv.org/abs/2209.01794
ProcessorFuzz: Guiding Processor Fuzzing using Control and StatusRegisters,"SadullahCanakci, ChathuraRajapaksha, Anoop MysoreNataraja, LeilaDelshadtehrani, MichaelTaylor, ManuelEgele, AjayJoshi",05-sep-22,Hardware Architecture (cs.AR)," As the complexity of modern processors has increased over the years, developing effective verification strategies to identify bugs prior to manufacturing has become critical. Undiscovered micro-architectural bugs in processors can manifest as severe security vulnerabilities in the form of side channels, functional bugs, etc. Inspired by software fuzzing, a technique commonly used for software testing, multiple recent works use hardware fuzzing for the verification of Register-Transfer Level (RTL) designs. However, these works suffer from several limitations such as lack of support for widely-used Hardware Description Languages (HDLs) and misleading coverage-signals that misidentify ""interesting"" inputs. Towards overcoming these shortcomings, we present ProcessorFuzz, a processor fuzzer that guides the fuzzer with a novel CSR-transition coverage metric. ProcessorFuzz monitors the transitions in Control and Status Registers (CSRs) as CSRs are in charge of controlling and holding the state of the processor. Therefore, transitions in CSRs indicate a new processor state, and guiding the fuzzer based on this feedback enables ProcessorFuzz to explore new processor states. ProcessorFuzz is agnostic to the HDL and does not require any instrumentation in the processor design. Thus, it supports a wide range of RTL designs written in different hardware languages. We evaluated ProcessorFuzz with three real-world open-source processors -- Rocket, BOOM, and BlackParrot. ProcessorFuzz triggered a set of ground-truth bugs 1.23$\times$ faster (on average) than DIFUZZRTL. Moreover, our experiments exposed 8 new bugs across the three RISC-V cores and one new bug in a reference model. All nine bugs were confirmed by the developers of the corresponding projects.",https://arxiv.org/abs/2209.01791
LKD-Net: Large Kernel Convolution Network for Single Image Dehazing,"PinjunLuo, GuoqiangXiao, XinboGao, SongWu",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The deep convolutional neural networks (CNNs)-based single image dehazing methods have achieved significant success. The previous methods are devoted to improving the network's performance by increasing the network's depth and width. The current methods focus on increasing the convolutional kernel size to enhance its performance by benefiting from the larger receptive field. However, directly increasing the size of the convolutional kernel introduces a massive amount of computational overhead and parameters. Thus, a novel Large Kernel Convolution Dehaze Block (LKD Block) consisting of the Decomposition deep-wise Large Kernel Convolution Block (DLKCB) and the Channel Enhanced Feed-forward Network (CEFN) is devised in this paper. The designed DLKCB can split the deep-wise large kernel convolution into a smaller depth-wise convolution and a depth-wise dilated convolution without introducing massive parameters and computational overhead. Meanwhile, the designed CEFN incorporates a channel attention mechanism into Feed-forward Network to exploit significant channels and enhance robustness. By combining multiple LKD Blocks and Up-Down sampling modules, the Large Kernel Convolution Dehaze Network (LKD-Net) is conducted. The evaluation results demonstrate the effectiveness of the designed DLKCB and CEFN, and our LKD- Net outperforms the state-of-the-art. On the SOTS indoor dataset, our LKD- Net dramatically outperforms the Transformer-based method Dehamer with only 1.79% #Param and 48.9% FLOPs. The source code of our LKD-Net is available at [this https URL](https://github.com/SWU-CS-MediaLab/LKD-Net).",https://arxiv.org/abs/2209.01789
"""Is your explanation stable?"": A Robustness Evaluation Framework forFeature Attribution","YuyouGan, YuhaoMao, XuhongZhang, Shouling Ji, Yuwen Pu, MengHan, JianweiYin, TingWang",05-sep-22,Artificial Intelligence (cs.AI)," Understanding the decision process of neural networks is hard. One vital method for explanation is to attribute its decision to pivotal features. Although many algorithms are proposed, most of them solely improve the faithfulness to the model. However, the real environment contains many random noises, which may leads to great fluctuations in the explanations. More seriously, recent works show that explanation algorithms are vulnerable to adversarial attacks. All of these make the explanation hard to trust in real scenarios.   To bridge this gap, we propose a model-agnostic method \emph{Median Test for Feature Attribution} (MeTFA) to quantify the uncertainty and increase the stability of explanation algorithms with theoretical guarantees. MeTFA has the following two functions: (1) examine whether one feature is significantly important or unimportant and generate a MeTFA-significant map to visualize the results; (2) compute the confidence interval of a feature attribution score and generate a MeTFA-smoothed map to increase the stability of the explanation. Experiments show that MeTFA improves the visual quality of explanations and significantly reduces the instability while maintaining the faithfulness. To quantitatively evaluate the faithfulness of an explanation under different noise settings, we further propose several robust faithfulness metrics. Experiment results show that the MeTFA-smoothed explanation can significantly increase the robust faithfulness. In addition, we use two scenarios to show MeTFA's potential in the applications. First, when applied to the SOTA explanation method to locate context bias for semantic segmentation models, MeTFA-significant explanations use far smaller regions to maintain 99\%+ faithfulness. Second, when tested with different explanation-oriented attacks, MeTFA can help defend vanilla, as well as adaptive, adversarial attacks against explanations.",https://arxiv.org/abs/2209.01788
Underwater Acoustic Ranging Between Smartphones,"TuochaoChen, JustinChan, ShyamnathGollakota",05-sep-22,Networking and Internet Architecture (cs.NI)," We present a novel underwater system that can perform acoustic ranging between commodity smartphones. To achieve this, we design a real- time underwater ranging protocol that computes the time-of-flight between smartphones. To address the severe underwater multipath, we present a dual- microphone optimization algorithm that can more reliably identify the direct path. Our underwater evaluations show that our system has median errors of 0.48-0.86 m at distances upto 35 m. Further, our system can operate across smartphone model pairs and works in the presence of clock drifts. While existing underwater localization research is targeted for custom hydrophone hardware, we believe that our work breaks new ground by demonstrating a path to bringing underwater ranging capabilities to billions of existing smartphones, without additional hardware.",https://arxiv.org/abs/2209.01782
Indoor Path Planning for Multiple Unmanned Aerial Vehicles viaCurriculum Learning,"JongminPark, KwansikPark",05-sep-22,Robotics (cs.RO)," Multi-agent reinforcement learning was performed in this study for indoor path planning of two unmanned aerial vehicles (UAVs). Each UAV performed the task of moving as fast as possible from a randomly paired initial position to a goal position in an environment with obstacles. To minimize training time and prevent the damage of UAVs, learning was performed by simulation. Considering the non-stationary characteristics of the multi-agent environment wherein the optimal behavior varies based on the actions of other agents, the action of the other UAV was also included in the state space of each UAV. Curriculum learning was performed in two stages to increase learning efficiency. A goal rate of 89.0% was obtained compared with other learning strategies that obtained goal rates of 73.6% and 79.9%.",https://arxiv.org/abs/2209.01780
ElasticROS: An Elastically Collaborative Robot Operation System forFog and Cloud Robotics,"BoyiLiu, LujiaWang, MingLiu",05-sep-22,Robotics (cs.RO)," Robots are integrating more huge-size models to enrich functions and improve accuracy, which leads to out-of-control computing pressure. And thus robots are encountering bottlenecks in computing power and battery capacity. Fog or cloud robotics is one of the most anticipated theories to address these issues. Approaches of cloud robotics have developed from system-level to node-level. However, the present node-level systems are not flexible enough to dynamically adapt to changing conditions. To address this, we present ElasticROS, which evolves the present node-level systems into an algorithm-level one. ElasticROS is based on ROS and ROS2. For fog and cloud robotics, it is the first robot operating system with algorithm- level collaborative computing. ElasticROS develops elastic collaborative computing to achieve adaptability to dynamic conditions. The collaborative computing algorithm is the core and challenge of ElasticROS. We abstract the problem and then propose an algorithm named ElasAction to address. It is a dynamic action decision algorithm based on online learning, which determines how robots and servers cooperate. The algorithm dynamically updates parameters to adapt to changes of conditions where the robot is currently in. It achieves elastically distributing of computing tasks to robots and servers according to configurations. In addition, we prove that the regret upper bound of the ElasAction is sublinear, which guarantees its convergence and thus enables ElasticROS to be stable in its elasticity. Finally, we conducted experiments with ElasticROS on common tasks of robotics, including SLAM, grasping and human-robot dialogue, and then measured its performances in latency, CPU usage and power consumption. The algorithm-level ElasticROS performs significantly better than the present node-level system.",https://arxiv.org/abs/2209.01776
Predict-and-Update Network: Audio-Visual Speech Recognition Inspiredby Human Speech Perception,"JiadongWang, XinyuanQian, HaizhouLi",05-sep-22,Multimedia (cs.MM)," Audio and visual signals complement each other in human speech perception, so do they in speech recognition. The visual hint is less evident than the acoustic hint, but more robust in a complex acoustic environment, as far as speech perception is concerned. It remains a challenge how we effectively exploit the interaction between audio and visual signals for automatic speech recognition. There have been studies to exploit visual signals as redundant or complementary information to audio input in a synchronous manner. Human studies suggest that visual signal primes the listener in advance as to when and on which frequency to attend to. We propose a Predict-and-Update Network (P&U net), to simulate such a visual cueing mechanism for Audio-Visual Speech Recognition (AVSR). In particular, we first predict the character posteriors of the spoken words, i.e. the visual embedding, based on the visual signals. The audio signal is then conditioned on the visual embedding via a novel cross-modal Conformer, that updates the character posteriors. We validate the effectiveness of the visual cueing mechanism through extensive experiments. The proposed P&U net outperforms the state-of-the-art AVSR methods on both LRS2-BBC and LRS3-BBC datasets, with the relative reduced Word Error Rate (WER)s exceeding 10% and 40% under clean and noisy conditions, respectively.",https://arxiv.org/abs/2209.01774
Exploring the Verifiability of Code Generated by GitHub Copilot,"DakotaWong, AustinKothig, Patrick Lam",05-sep-22,Software Engineering (cs.SE)," GitHub's Copilot generates code quickly. We investigate whether it generates good code. Our approach is to identify a set of problems, ask Copilot to generate solutions, and attempt to formally verify these solutions with Dafny. Our formal verification is with respect to hand- crafted specifications. We have carried out this process on 6 problems and succeeded in formally verifying 4 of the created solutions. We found evidence which corroborates the current consensus in the literature: Copilot is a powerful tool; however, it should not be ""flying the plane"" by itself.",https://arxiv.org/abs/2209.01768
Continuous Decomposition of Granularity for Neural ParaphraseGeneration,"XiaodongGu, ZhaoweiZhang, Sang-Woo Lee, KangMin Yoo, Jung-Woo Ha",05-sep-22,Computation and Language (cs.CL)," While Transformers have had significant success in paragraph generation, they treat sentences as linear sequences of tokens and often neglect their hierarchical information. Prior work has shown that decomposing the levels of granularity~(e.g., word, phrase, or sentence) for input tokens has produced substantial improvements, suggesting the possibility of enhancing Transformers via more fine-grained modeling of granularity. In this work, we propose a continuous decomposition of granularity for neural paraphrase generation (C-DNPG). In order to efficiently incorporate granularity into sentence encoding, C-DNPG introduces a granularity-aware attention (GA-Attention) mechanism which extends the multi-head self-attention with: 1) a granularity head that automatically infers the hierarchical structure of a sentence by neurally estimating the granularity level of each input token; and 2) two novel attention masks, namely, granularity resonance and granularity scope, to efficiently encode granularity into attention. Experiments on two benchmarks, including Quora question pairs and Twitter URLs have shown that C-DNPG outperforms baseline models by a remarkable margin and achieves state-of-the-art results in terms of many metrics. Qualitative analysis reveals that C-DNPG indeed captures fine-grained levels of granularity with effectiveness.",https://arxiv.org/abs/2209.01766
A negative imaginary approach to hybrid integrator-gain system control,"KanghongShi, NastaranNikooienejad, Ian R.Petersen, S. O. RezaMoheimani",05-sep-22,Systems and Control (eess.SY)," In this paper, we show that a hybrid integrator-gain system (HIGS) is a nonlinear negative imaginary (NNI) system. We prove that the positive feedback interconnection of a linear negative imaginary (NI) system and a HIGS is asymptotically stable. We apply the HIGS to a MEMS nanopositioner, as an example of a linear NI system, in a single-input single-output framework. We analyze the stability and the performance of the closed-loop interconnection in both time and frequency domains through simulations and demonstrate the applicability of HIGS as an NNI controller to a linear NI system.",https://arxiv.org/abs/2209.01765
Free material optimization of thermal conductivity tensors withasymmetric components,"YukiSato, TeppeiDeguchi, TsuyoshiNomura, AtsushiKawamoto",05-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," Free Material Optimization (FMO), a branch of topology optimization, in which the design variables are the full constitutive tensors, can provide the most general form of the design problems. Considering the microstructure composed of isotropic materials, the constitutive tensors are yet positive definite and symmetric. On the other hand, it has been reported that the symmetry of this constitutive tensor can be broken in appearance by considering other physical phenomena. In the present study, we focus on the thermal Hall effect, which is explained as the phenomena that induces the temperature gradient orthogonal to a given temperature gradient across a solid when a magnetic field is applied to the solid. This effect makes the thermal conductivity tensor asymmetric and justifies extending the space of the constitutive tensors to be an asymmetric domain. We propose the FMO for asymmetric constitutive tensors, parameterizing the design space so that the physically available property could be naturally satisfied. Several numerical experiments are provided to show the validity and the utility of the proposed method.",https://arxiv.org/abs/2209.01759
Exploiting Pre-trained Feature Networks for Generative AdversarialNetworks in Audio-domain Loop Generation,"Yen-TungYeh, Bo-YuChen, Yi-Hsuan Yang",05-sep-22,Sound (cs.SD)," While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.",https://arxiv.org/abs/2209.01755
Boost Decentralized Federated Learning in Vehicular Networks byDiversifying Data Sources,"DongyuanSu, YipengZhou, Laizhong Cui",05-sep-22,Machine Learning (cs.LG)," Recently, federated learning (FL) has received intensive research because of its ability in preserving data privacy for scattered clients to collaboratively train machine learning models. Commonly, a parameter server (PS) is deployed for aggregating model parameters contributed by different clients. Decentralized federated learning (DFL) is upgraded from FL which allows clients to aggregate model parameters with their neighbours directly. DFL is particularly feasible for vehicular networks as vehicles communicate with each other in a vehicle-to-vehicle (V2V) manner. However, due to the restrictions of vehicle routes and communication distances, it is hard for individual vehicles to sufficiently exchange models with others. Data sources contributing to models on individual vehicles may not diversified enough resulting in poor model accuracy. To address this problem, we propose the DFL-DDS (DFL with diversified Data Sources) algorithm to diversify data sources in DFL. Specifically, each vehicle maintains a state vector to record the contribution weight of each data source to its model. The Kullback-Leibler (KL) divergence is adopted to measure the diversity of a state vector. To boost the convergence of DFL, a vehicle tunes the aggregation weight of each data source by minimizing the KL divergence of its state vector, and its effectiveness in diversifying data sources can be theoretically proved. Finally, the superiority of DFL-DDS is evaluated by extensive experiments (with MNIST and CIFAR-10 datasets) which demonstrate that DFL-DDS can accelerate the convergence of DFL and improve the model accuracy significantly compared with state-of-the-art baselines.",https://arxiv.org/abs/2209.01751
Removing membrane locking in quadratic NURBS-based discretizations oflinear plane Kirchhoff rods: CAS elements,"HugoCasquero, MahmoudGolestanian",05-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," NURBS-based discretizations suffer from membrane locking when applied to primal formulations of curved thin-walled structures. We consider linear plane curved Kirchhoff rods as a model problem to study how to remove membrane locking from NURBS-based discretizations. In this work, we propose continuous-assumed-strain (CAS) elements, an assumed strain treatment that removes membrane locking from quadratic NURBS for an ample range of slenderness ratios. CAS elements take advantage of the C1 inter-element continuity of the displacement vector given by quadratic NURBS to interpolate the membrane strain using linear Lagrange polynomials while preserving the C0 inter-element continuity of the membrane strain. CAS elements are the first NURBS-based element type able to remove membrane locking for a broad range of slenderness ratios that combines the following characteristics: (1) No additional degrees of freedom are added, (2) No additional systems of algebraic equations need to be solved, and (3) The nonzero pattern of the stiffness matrix is preserved. Since the only additional computations required by the proposed element type are to evaluate the derivatives of the basis functions and the unit tangent vector at the knots, the proposed scheme barely increases the computational cost with respect to the locking-prone NURBS-based discretization of the primal formulation. The benchmark problems show that the convergence of CAS elements is independent of the slenderness ratio while the convergence of quadratic NURBS elements, local Bbar elements, and local ANS elements depends heavily on the slenderness ratio. The numerical examples also show how CAS elements remove the spurious oscillations in stress resultants caused by membrane locking while quadratic NURBS elements, local Bbar elements, and local ANS elements suffer from large-amplitude spurious oscillations in stress resultants.",https://arxiv.org/abs/2209.01750
SPCNet: Stepwise Point Cloud Completion Network,"Fei Hu, Honghua Chen, Xuequan Lu, Zhe Zhu, JunWang, WeimingWang, Fu LeeWang, Mingqiang Wei",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," How will you repair a physical object with large missings? You may first recover its global yet coarse shape and stepwise increase its local details. We are motivated to imitate the above physical repair procedure to address the point cloud completion task. We propose a novel stepwise point cloud completion network (SPCNet) for various 3D models with large missings. SPCNet has a hierarchical bottom-to-up network architecture. It fulfills shape completion in an iterative manner, which 1) first infers the global feature of the coarse result; 2) then infers the local feature with the aid of global feature; and 3) finally infers the detailed result with the help of local feature and coarse result. Beyond the wisdom of simulating the physical repair, we newly design a cycle loss %based training strategy to enhance the generalization and robustness of SPCNet. Extensive experiments clearly show the superiority of our SPCNet over the state-of-the-art methods on 3D point clouds with large missings.",https://arxiv.org/abs/2209.01747
SEFormer: Structure Embedding Transformer for 3D Object Detection,"XiaoyuFeng, HemingDu, YueqiDuan, YongpanLiu, HeheFan",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Effectively preserving and encoding structure features from objects in irregular and sparse LiDAR points is a key challenge to 3D object detection on point cloud. Recently, Transformer has demonstrated promising performance on many 2D and even 3D vision tasks. Compared with the fixed and rigid convolution kernels, the self-attention mechanism in Transformer can adaptively exclude the unrelated or noisy points and thus suitable for preserving the local spatial structure in irregular LiDAR point cloud. However, Transformer only performs a simple sum on the point features, based on the self-attention mechanism, and all the points share the same transformation for value. Such isotropic operation lacks the ability to capture the direction-distance-oriented local structure which is important for 3D object detection. In this work, we propose a Structure-Embedding transFormer (SEFormer), which can not only preserve local structure as traditional Transformer but also have the ability to encode the local structure. Compared to the self-attention mechanism in traditional Transformer, SEFormer learns different feature transformations for value points based on the relative directions and distances to the query point. Then we propose a SEFormer based network for high-performance 3D object detection. Extensive experiments show that the proposed architecture can achieve SOTA results on Waymo Open Dataset, the largest 3D detection benchmark for autonomous driving. Specifically, SEFormer achieves 79.02% mAP, which is 1.2% higher than existing works. We will release the codes.",https://arxiv.org/abs/2209.01746
Investigation on Principles for Cost Assignment in Motion Vector-basedVideo Steganography,"Jun Li, MinqingZhang, KeNiu, XiaoyuanYang",05-sep-22,Cryptography and Security (cs.CR)," Cost assignment in the motion vector domain remains a research focus in video steganography. Recent studies in image steganography have summarized many principles for cost assignment and achieved good results. But the basic principles for cost assignment in motion vector-based video steganography have not been fully discussed yet. Firstly, this paper proposes three principles for cost assignment in the motion vector domain, including the principle of local optimality, non-consistency in the block group, and complexity priority. Secondly, three corresponding novel practical distortion functions were designed according to the three principles. Finally, a joint distortion function is constructed based on all three principles to increase overall performance. The experimental results show that not only the three independent distortion functions can effectively resist the corresponding steganalysis attacks, but the final joint distortion can resist the three steganalysis features simultaneously. In addition, it can obtain good visual quality and coding efficiency, which can be applied to practical scenarios.",https://arxiv.org/abs/2209.01745
A repository of automatic GUI test patterns in Android applications:Specification and Analysis using Alloy modeling language,"FatemehMosayeb, ShohrehAjoudanian",05-sep-22,Software Engineering (cs.SE)," The software industry aims to provide customers with quality software. Testing software is a critical and sensitive stage in ensuring software quality. Due to the increasing popularity of mobile devices, the use of Android applications has increased. Almost all are equipped with Graphical User Interface to interact with users or systems. GUI is the most common tool to communicate with modern software. Therefore, the perfect GUI is a GUI that ensures the safety, strength, and usability of the whole software system. The GUI testing is a vital stage in ensuring the product quality because the GUI is the user s first impression and the final view of the final product. This paper has proposed a new technique to promote the model based test efficiency using Alloy modeling language. The findings showed that this approach needs less configuration and modeling time than previous methods. Moreover, using GUI patterns may decrease errors and violations.",https://arxiv.org/abs/2209.01744
Numerical analysis of growth-mediated autochemotactic patternformation in self-propelling bacteria,"JiansongZhang, MaoshengJiang, Jiang Zhu, Xijun Yu, LuizBevilacqua",05-sep-22,Numerical Analysis (math.NA)," In this paper, a decoupled characteristic Galerkin finite element procedure is provided for simulating growth-mediated autochemotactic pattern formation in self-propelling bacteria. In this procedure, a modified characteristic Galerkin method is established to solve the bacterial density equation, while the classical finite element procedure is considered for the self-secreted chemical density and polarization dynamics equations system. The convergence of this proposed method is considered under some regularity assumptions and the corresponding error estimate is derived. Numerical experiments are carried out to support the theoretical analysis. Furthermore, several new wave type pattern formations are found.",https://arxiv.org/abs/2209.01741
Using Consensual Biterms from Text Structures of Requirements and Codeto Improve IR-Based Traceability Recovery,"HuiGao, HongyuKuang, KexinSun, XiaoxingMa, AlexanderEgyed, PatrickMÃ¤der, Guoping Rong, Dong Shao, He Zhang",05-sep-22,Software Engineering (cs.SE)," Traceability approves trace links among software artifacts based on whether two artifacts are related by system functionalities. The traces are valuable for software development, but are difficult to obtain manually. To cope with the costly and fallible manual recovery, automated approaches are proposed to recover traces through textual similarities among software artifacts, such as those based on Information Retrieval (IR). However, the low quality & quantity of artifact texts negatively impact the calculated IR values, thus greatly hindering the performance of IR-based approaches. In this study, we propose to extract co-occurred word pairs from the text structures of both requirements and code (i.e., consensual biterms) to improve IR-based traceability recovery. We first collect a set of biterms based on the part-of-speech of requirement texts, and then filter them through the code texts. We then use these consensual biterms to both enrich the input corpus for IR techniques and enhance the calculations of IR values. A nine-system-based evaluation shows that in general, when solely used to enhance IR techniques, our approach can outperform pure IR-based approaches and another baseline by 21.9% & 21.8% in AP, and 9.3% & 7.2% in MAP, respectively. Moreover, when used to collaborate with another enhancing strategy from different perspectives, it can outperform this baseline by 5.9% in AP and 4.8% in MAP.",https://arxiv.org/abs/2209.01736
Prototype-Aware Heterogeneous Task for Point Cloud Completion,"JunshuTang, JiachenXu, JingyuGong, HaichuanSong, YuanXie, LizhuangMa",05-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Point cloud completion, which aims at recovering original shape information from partial point clouds, has attracted attention on 3D vision community. Existing methods usually succeed in completion for standard shape, while failing to generate local details of point clouds for some non- standard shapes. To achieve desirable local details, guidance from global shape information is of critical importance. In this work, we design an effective way to distinguish standard/non-standard shapes with the help of intra-class shape prototypical representation, which can be calculated by the proposed supervised shape clustering pretext task, resulting in a heterogeneous component w.r.t completion network. The representative prototype, defined as feature centroid of shape categories, can provide global shape guidance, which is referred to as soft-perceptual prior, to inject into downstream completion network by the desired selective perceptual feature fusion module in a multi-scale manner. Moreover, for effective training, we consider difficulty-based sampling strategy to encourage the network to pay more attention to some partial point clouds with fewer geometric information. Experimental results show that our method outperforms other state-of-the-art methods and has strong ability on completing complex geometric shapes.",https://arxiv.org/abs/2209.01734
Features Fusion Framework for Multimodal Irregular Time-series Events,"PeiwangTang, XianchaoZhang",05-sep-22,Artificial Intelligence (cs.AI)," Some data from multiple sources can be modeled as multimodal time- series events which have different sampling frequencies, data compositions, temporal relations and characteristics. Different types of events have complex nonlinear relationships, and the time of each event is irregular. Neither the classical Recurrent Neural Network (RNN) model nor the current state-of-the-art Transformer model can deal with these features well. In this paper, a features fusion framework for multimodal irregular time-series events is proposed based on the Long Short-Term Memory networks (LSTM). Firstly, the complex features are extracted according to the irregular patterns of different events. Secondly, the nonlinear correlation and complex temporal dependencies relationship between complex features are captured and fused into a tensor. Finally, a feature gate are used to control the access frequency of different tensors. Extensive experiments on MIMIC-III dataset demonstrate that the proposed framework significantly outperforms to the existing methods in terms of AUC (the area under Receiver Operating Characteristic curve) and AP (Average Precision).",https://arxiv.org/abs/2209.01733
An Adaptive Black-box Defense against Trojan Attacks (TrojDef),"GuanxiongLiu, AbdallahKhreishah, FatimaSharadgah, Issa Khalil",05-sep-22,Cryptography and Security (cs.CR)," Trojan backdoor is a poisoning attack against Neural Network (NN) classifiers in which adversaries try to exploit the (highly desirable) model reuse property to implant Trojans into model parameters for backdoor breaches through a poisoned training process. Most of the proposed defenses against Trojan attacks assume a white-box setup, in which the defender either has access to the inner state of NN or is able to run back- propagation through it. In this work, we propose a more practical black-box defense, dubbed TrojDef, which can only run forward-pass of the NN. TrojDef tries to identify and filter out Trojan inputs (i.e., inputs augmented with the Trojan trigger) by monitoring the changes in the prediction confidence when the input is repeatedly perturbed by random noise. We derive a function based on the prediction outputs which is called the prediction confidence bound to decide whether the input example is Trojan or not. The intuition is that Trojan inputs are more stable as the misclassification only depends on the trigger, while benign inputs will suffer when augmented with noise due to the perturbation of the classification features.   Through mathematical analysis, we show that if the attacker is perfect in injecting the backdoor, the Trojan infected model will be trained to learn the appropriate prediction confidence bound, which is used to distinguish Trojan and benign inputs under arbitrary perturbations. However, because the attacker might not be perfect in injecting the backdoor, we introduce a nonlinear transform to the prediction confidence bound to improve the detection accuracy in practical settings. Extensive empirical evaluations show that TrojDef significantly outperforms the-state-of-the-art defenses and is highly stable under different settings, even when the classifier architecture, the training process, or the hyper-parameters change.",https://arxiv.org/abs/2209.01728
A multi-scale framework for neural network enhanced methods to thesolution of partial differential equations,XiaodanRen,05-sep-22,Numerical Analysis (math.NA)," In the present work, a multi-scale framework for neural network enhanced methods is proposed for approximation of function and solution of partial differential equations (PDEs). By introducing the multi-scale concept, the total solution of the target problem could be decomposed into two parts, i.e. the coarse scale solution and the fine scale solution. In the coarse scale, the conventional numerical methods (e.g. finite element methods) are applied and the coarse scale solution could be obtained. In the fine scale, the neural networks is introduced to formulate the solution. The custom loss functions are developed by taking into account the governing equations and boundary conditions of PDEs, the constraints and the interaction from coarse scale. The proposed methods are illustrated and examined by various of testing cases.",https://arxiv.org/abs/2209.01721
On the Horizon: Interactive and Compositional Deepfakes,EricHorvitz,05-sep-22,Artificial Intelligence (cs.AI)," Over a five-year period, computing methods for generating high- fidelity, fictional depictions of people and events moved from exotic demonstrations by computer science research teams into ongoing use as a tool of disinformation. The methods, referred to with the portmanteau of ""deepfakes,"" have been used to create compelling audiovisual content. Here, I share challenges ahead with malevolent uses of two classes of deepfakes that we can expect to come into practice with costly implications for society: interactive and compositional deepfakes. Interactive deepfakes have the capability to impersonate people with realistic interactive behaviors, taking advantage of advances in multimodal interaction. Compositional deepfakes leverage synthetic content in larger disinformation plans that integrate sets of deepfakes over time with observed, expected, and engineered world events to create persuasive synthetic histories. Synthetic histories can be constructed manually but may one day be guided by adversarial generative explanation (AGE) techniques. In the absence of mitigations, interactive and compositional deepfakes threaten to move us closer to a post-epistemic world, where fact cannot be distinguished from fiction. I shall describe interactive and compositional deepfakes and reflect about cautions and potential mitigations to defend against them.",https://arxiv.org/abs/2209.01717
ChemBERTa-2: Towards Chemical Foundation Models,"WalidAhmad, ElanaSimon, SeyoneChithrananda, GabrielGrand, BharathRamsundar",05-sep-22,Machine Learning (cs.LG)," Large pretrained models such as GPT-3 have had tremendous impact on modern natural language processing by leveraging self-supervised learning to learn salient representations that can be used to readily finetune on a wide variety of downstream tasks. We investigate the possibility of transferring such advances to molecular machine learning by building a chemical foundation model, ChemBERTa-2, using the language of SMILES. While labeled data for molecular prediction tasks is typically scarce, libraries of SMILES strings are readily available. In this work, we build upon ChemBERTa by optimizing the pretraining process. We compare multi-task and self-supervised pretraining by varying hyperparameters and pretraining dataset size, up to 77M compounds from PubChem. To our knowledge, the 77M set constitutes one of the largest datasets used for molecular pretraining to date. We find that with these pretraining improvements, we are competitive with existing state-of-the-art architectures on the MoleculeNet benchmark suite. We analyze the degree to which improvements in pretraining translate to improvement on downstream tasks.",https://arxiv.org/abs/2209.01714
Hide & Seek: Seeking the (Un)-Hidden key in Provably-Secure LogicLocking Techniques,"SatwikPatnaik, NimishaLimaye, OzgurSinanoglu",05-sep-22,Cryptography and Security (cs.CR)," Logic locking protects an IC from threats such as piracy of design IP and unauthorized overproduction throughout the IC supply chain. Out of the several techniques proposed by the research community, provably-secure logic locking (PSLL) has acquired a foothold due to its algorithmic and provable-security guarantees. However, the security of these techniques is questioned by attackers that exploit the vulnerabilities arising from the hardware implementation. Such attacks (i) are predominantly specific to locking techniques and (ii) lack generality and scalability. This leads to a plethora of attacks, and defenders, find it challenging to ascertain the security of newly developed PSLL techniques. Additionally, there is no repository of locked circuits that attackers can use to benchmark (and compare) their attacks.   In this work, we develop a generalized attack that can recover the secret key across different PSLL techniques. To that end, we extract functional and structural properties depending on the hardware construction of the PSLL techniques and develop two attacks based on the concepts of VLSI testing and Boolean transformations. We evaluate our attacks on 30,000 locked circuits across 14 PSLL techniques, including nine unbroken techniques. Our attacks successfully recover the secret key (100% accuracy) for all the techniques. Our experimentation across different (I) technology libraries, (ii) synthesis tools, and (iii) logic optimization settings provide interesting insights. For instance, our attacks recover the secret key by only using the locked circuit when an academic synthesis tool is used. Additionally, designers can use our attacks as a verification tool to ascertain the lower- bound security achieved by hardware implementations. We shall release our artifacts, which could help foster the development of future attacks and defenses in the PSLL domain.",https://arxiv.org/abs/2209.01712
Synergistic Redundancy: Towards Verifiable Safety for AutonomousVehicles,"AyooshBansal, Simon Yu, Hunmin Kim, Bo Li, NairaHovakimyan, MarcoCaccamo, Lui Sha",04-sep-22,Robotics (cs.RO)," As Autonomous Vehicle (AV) development has progressed, concerns regarding the safety of passengers and agents in their environment have risen. Each real world traffic collision involving autonomously controlled vehicles has compounded this concern. Open source autonomous driving implementations show a software architecture with complex interdependent tasks, heavily reliant on machine learning and Deep Neural Networks (DNN), which are vulnerable to non deterministic faults and corner cases. These complex subsystems work together to fulfill the mission of the AV while also maintaining safety. Although significant improvements are being made towards increasing the empirical reliability and confidence in these systems, the inherent limitations of DNN verification create an, as yet, insurmountable challenge in providing deterministic safety guarantees in AV.   We propose Synergistic Redundancy (SR), a safety architecture for complex cyber physical systems, like AV. SR provides verifiable safety guarantees against specific faults by decoupling the mission and safety tasks of the system. Simultaneous to independently fulfilling their primary roles, the partially functionally redundant mission and safety tasks are able to aid each other, synergistically improving the combined system. The synergistic safety layer uses only verifiable and logically analyzable software to fulfill its tasks. Close coordination with the mission layer allows easier and early detection of safety critical faults in the system. SR simplifies the mission layer's optimization goals and improves its design. SR provides safe deployment of high performance, although inherently unverifiable, machine learning software. In this work, we first present the design and features of the SR architecture and then evaluate the efficacy of the solution, focusing on the crucial problem of obstacle existence detection faults in AV.",https://arxiv.org/abs/2209.01711
SFS: Smart OS Scheduling for Serverless Functions,"YuqiFu, LiLiu, HaoliangWang, YueCheng, Songqing Chen","4 Sep 2022 (v1(https://arxiv.org/abs/2209.01709v1)), lastrevised 7 Sep 2022 (this version, v2)",Operating Systems (cs.OS)," Serverless computing enables a new way of building and scaling cloud applications by allowing developers to write fine-grained serverless or cloud functions. The execution duration of a cloud function is typically short-ranging from a few milliseconds to hundreds of seconds. However, due to resource contentions caused by public clouds' deep consolidation, the function execution duration may get significantly prolonged and fail to accurately account for the function's true resource usage. We observe that the function duration can be highly unpredictable with huge amplification of more than 50x for an open-source FaaS platform (OpenLambda). Our experiments show that the OS scheduling policy of cloud functions' host server can have a crucial impact on performance. The default Linux scheduler, CFS (Completely Fair Scheduler), being oblivious to workloads, frequently context-switches short functions, causing a turnaround time that is much longer than their service time.   We propose SFS (Smart Function Scheduler),which works entirely in the user space and carefully orchestrates existing Linux FIFO and CFS schedulers to approximate Shortest Remaining Time First (SRTF). SFS uses two-level scheduling that seamlessly combines a new FILTER policy with Linux CFS, to trade off increased duration of long functions for significant performance improvement for short functions. We implement {\proj} in the Linux user space and port it to OpenLambda. Evaluation results show that SFS significantly improves short functions' duration with a small impact on relatively longer functions, compared to CFS.",https://arxiv.org/abs/2209.01710
Model Predictive Control Design of a 3-DOF Robot Arm Based onRecognition of Spatial Coordinates,"ZhangxiZhou, YuyaoZhang, Yezhang Li",04-sep-22,Robotics (cs.RO)," This paper uses Model Predictive Control (MPC) to optimise the input torques of a Three-Degrees-of-Freedom (DOF) robotic arm, enabling it to operate to the target position and grasp the object accurately. A monocular camera is firstly used to recognise the colour and depth of the object. Then, the inverse kinematics calculation and the spatial coordinates of the object through coordinate transformation are combined to get the required rotating angle of each servo. Finally, the dynamic model of the robotic arm structure is derived and the model predictive control is applied to simulate the optimal input torques of servos to minimize the cost function.",https://arxiv.org/abs/2209.01709
Recursive Gaussian Process over graphs for Integrating Multi-timescaleMeasurements in Low-Observable Distribution Systems,"ShwetaDahale, BalasubramaniamNatarajan",04-sep-22,Systems and Control (eess.SY)," The transition to a smarter grid is empowered by enhanced sensor deployments and smart metering infrastructure in the distribution system. Measurements from these sensors and meters can be used for many applications, including distribution system state estimation (DSSE). However, these measurements are typically sampled at different rates and could be intermittent due to losses during the aggregation process. These multi time-scale measurements should be reconciled in real-time to perform accurate grid monitoring. This paper tackles this problem by formulating a recursive multi-task Gaussian process (RGP-G) approach that sequentially aggregates sensor measurements. Specifically, we formulate a recursive multi-task GP with and without network connectivity information to reconcile the multi time-scale measurements in distribution systems. The proposed framework is capable of aggregating the multi-time scale measurements batch- wise or in real-time. Following the aggregation of the multi time-scale measurements, the spatial states of the consistent time-series are estimated using matrix completion based DSSE approach. Simulation results on IEEE 37 and IEEE 123 bus test systems illustrate the efficiency of the proposed methods from the standpoint of both multi time-scale data aggregation and DSSE.",https://arxiv.org/abs/2209.01706
Concatenated Classic and Neural (CCN) Codes: ConcatenatedAE,"OnurGÃ¼nlÃ¼, RickFritschek, Rafael F.Schaefer",04-sep-22,Information Theory (cs.IT)," Small neural networks (NNs) used for error correction were shown to improve on classic channel codes and to address channel model changes. We extend the code dimension of any such structure by using the same NN under one-hot encoding multiple times, which are serially-concatenated with an outer classic code. We design NNs with the same network parameters, where each Reed-Solomon codeword symbol is an input to a different NN. Significant improvements in block error probabilities for an additive Gaussian noise channel as compared to the small neural code are illustrated, as well as robustness to channel model changes.",https://arxiv.org/abs/2209.01703
"""iCub, We Forgive You!"" Investigating Trust in a Game Scenario withKids","FrancescaCocchella, GiuliaPusceddu, GiuliaBelgiovine, LindaLastrico, Francesco Rea, AlessandraSciutti",04-sep-22,Robotics (cs.RO)," This study presents novel strategies to investigate the mutual influence of trust and group dynamics in children-robot interaction. We implemented a game-like experimental activity with the humanoid robot iCub and designed a questionnaire to assess how the children perceived the interaction. We also aim to verify if the sensors, setups, and tasks are suitable for studying such aspects. The questionnaires' results demonstrate that youths perceive iCub as a friend and, typically, in a positive way. Other preliminary results suggest that, generally, children trusted iCub during the activity and, after its mistakes, they tried to reassure it with sentences such as: ""Don't worry iCub, we forgive you"". Furthermore, trust towards the robot in group cognitive activity appears to change according to gender: after two consecutive mistakes by the robot, girls tended to trust iCub more than boys. Finally, no significant difference has been evidenced between different age groups across points computed from the game and the self-reported scales. The tool we proposed is suitable for studying trust in human-robot interaction (HRI) across different ages and seems appropriate to understand the mechanism of trust in group interactions.",https://arxiv.org/abs/2209.01701
"IEEE Trust, Acceptance and Social Cues in Human-Robot Interaction --SCRITA 2022 Workshop","AlessandraRossi, PatrickHolthaus, SÃ¬lviaMoros, GabriellaLakatos","22 Aug 2022 (v1(https://arxiv.org/abs/2208.11090v1)), lastrevised 28 Aug 2022 (this version, v2)",Robotics (cs.RO)," The Trust, Acceptance and Social Cues in Human-Robot Interaction - SCRITA is the 5th edition of a series of workshops held in conjunction with the IEEE RO-MAN conference. This workshop focuses on addressing the challenges and development of the dynamics between people and robots in order to foster short interactions and long-lasting relationships in different fields, from educational, service, collaborative, companion, care- home and medical robotics. In particular, we aimed in investigating how robots can manipulate (i.e. creating, improving, and recovering) people's ability of accepting and trusting them for a fruitful and successful coexistence between humans and people. While advanced progresses are reached in studying and evaluating the factors affecting acceptance and trust of people in robots in controlled or short-term (repeated interactions) setting, developing service and personal robots, that are accepted and trusted by people where the supervision of operators is not possible, still presents an open challenge for scientists in robotics, AI and HRI fields. In such unstructured static and dynamic human-centred environments scenarios, robots should be able to learn and adapt their behaviours to the situational context, but also to people's prior experiences and learned associations, their expectations, and their and the robot's ability to predict and understand each other's behaviours. Although the previous editions valued the participation of leading researchers in the field and several exceptional invited speakers who tackled down some fundamental points in this research domains, we wish to continue to further explore the role of trust in robotics to present groundbreaking research to effectively design and develop socially acceptable and trustable robots to be deployed ""in the wild"".   Website: [this https URL](https://scrita.herts.ac.uk)",https://arxiv.org/abs/2209.01694
Variational Inference for Model-Free and Model-Based ReinforcementLearning,FelixLeibfried,04-sep-22,Machine Learning (cs.LG)," Variational inference (VI) is a specific type of approximate Bayesian inference that approximates an intractable posterior distribution with a tractable one. VI casts the inference problem as an optimization problem, more specifically, the goal is to maximize a lower bound of the logarithm of the marginal likelihood with respect to the parameters of the approximate posterior. Reinforcement learning (RL) on the other hand deals with autonomous agents and how to make them act optimally such as to maximize some notion of expected future cumulative reward. In the non- sequential setting where agents' actions do not have an impact on future states of the environment, RL is covered by contextual bandits and Bayesian optimization. In a proper sequential scenario, however, where agents' actions affect future states, instantaneous rewards need to be carefully traded off against potential long-term rewards. This manuscript shows how the apparently different subjects of VI and RL are linked in two fundamental ways. First, the optimization objective of RL to maximize future cumulative rewards can be recovered via a VI objective under a soft policy constraint in both the non-sequential and the sequential setting. This policy constraint is not just merely artificial but has proven as a useful regularizer in many RL tasks yielding significant improvements in agent performance. And second, in model-based RL where agents aim to learn about the environment they are operating in, the model-learning part can be naturally phrased as an inference problem over the process that governs environment dynamics. We are going to distinguish between two scenarios for the latter: VI when environment states are fully observable by the agent and VI when they are only partially observable through an observation distribution.",https://arxiv.org/abs/2208.11090
A Tutorial on Sparse Gaussian Processes and Variational Inference,"FelixLeibfried, VincentDutordoir, ST John, NicolasDurrande","27 Dec 2020 (v1(https://arxiv.org/abs/2012.13962v1)), lastrevised 4 Sep 2022 (this version, v12)",Machine Learning (cs.LG)," Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also problems with multidimensional labels. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as interdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.",https://arxiv.org/abs/2209.01693
On Kernel Regression with Data-Dependent Kernels,James B.Simon,04-sep-22,Machine Learning (cs.LG)," The primary hyperparameter in kernel regression (KR) is the choice of kernel. In most theoretical studies of KR, one assumes the kernel is fixed before seeing the training data. Under this assumption, it is known that the optimal kernel is equal to the prior covariance of the target function. In this note, we consider KR in which the kernel may be updated after seeing the training data. We point out that an analogous choice of kernel using the posterior of the target function is optimal in this setting. Connections to the view of deep neural networks as data-dependent kernel learners are discussed.",https://arxiv.org/abs/2012.13962
On the Privacy Risks of Cell-Based NAS Architectures,"HaiHuang, ZhikunZhang, YunShen, MichaelBackes, QiLi, YangZhang",04-sep-22,Cryptography and Security (cs.CR)," Existing studies on neural architecture search (NAS) mainly focus on efficiently and effectively searching for network architectures with better performance. Little progress has been made to systematically understand if the NAS-searched architectures are robust to privacy attacks while abundant work has already shown that human-designed architectures are prone to privacy attacks. In this paper, we fill this gap and systematically measure the privacy risks of NAS architectures. Leveraging the insights from our measurement study, we further explore the cell patterns of cell-based NAS architectures and evaluate how the cell patterns affect the privacy risks of NAS-searched architectures. Through extensive experiments, we shed light on how to design robust NAS architectures against privacy attacks, and also offer a general methodology to understand the hidden correlation between the NAS-searched architectures and other privacy risks.",https://arxiv.org/abs/2209.01691
Reconciling Individual Probability Forecasts,"AaronRoth, AlexanderTolbert, ScottWeinstein",04-sep-22,Machine Learning (cs.LG)," Individual probabilities refer to the probabilities of outcomes that are realized only once: the probability that it will rain tomorrow, the probability that Alice will die within the next 12 months, the probability that Bob will be arrested for a violent crime in the next 18 months, etc. Individual probabilities are fundamentally unknowable. Nevertheless, we show that two parties who agree on the data -- or on how to sample from a data distribution \-- cannot agree to disagree on how to model individual probabilities. This is because any two models of individual probabilities that substantially disagree can together be used to empirically falsify and improve at least one of the two models. This can be efficiently iterated in a process of ""reconciliation"" that results in models that both parties agree are superior to the models they started with, and which themselves (almost) agree on the forecasts of individual probabilities (almost) everywhere. We conclude that although individual probabilities are unknowable, they are contestable via a computationally and data efficient process that must lead to agreement. Thus we cannot find ourselves in a situation in which we have two equally accurate and unimprovable models that disagree substantially in their predictions -- providing an answer to what is sometimes called the predictive or model multiplicity problem.",https://arxiv.org/abs/2209.01688
ASTra: A Novel Algorithm-Level Approach to Imbalanced Classification,"DavidTwomey, Denise Gorse",04-sep-22,Machine Learning (cs.LG)," We propose a novel output layer activation function, which we name ASTra (Asymmetric Sigmoid Transfer function), which makes the classification of minority examples, in scenarios of high imbalance, more tractable. We combine this with a loss function that helps to effectively target minority misclassification. These two methods can be used together or separately, with their combination recommended for the most severely imbalanced cases. The proposed approach is tested on datasets with IRs from 588.24 to 4000 and very few minority examples (in some datasets, as few as five). Results using neural networks with from two to 12 hidden units are demonstrated to be comparable to, or better than, equivalent results obtained in a recent study that deployed a wide range of complex, hybrid data-level ensemble classifiers.",https://arxiv.org/abs/2209.01687
On the Risks of Collecting Multidimensional Data Under LocalDifferential Privacy,"HÃ©ber H.Arcolezi, SÃ©bastienGambs, Jean-FranÃ§oisCouchot, CatusciaPalamidessi",04-sep-22,Cryptography and Security (cs.CR)," The private collection of multiple statistics from a population is a fundamental statistical problem. One possible approach to realize this is to rely on the local model of differential privacy (LDP). Numerous LDP protocols have been developed for the task of frequency estimation of single and multiple attributes. These studies mainly focused on improving the utility of the algorithms to ensure the server performs the estimations accurately. In this paper, we investigate privacy threats (re-identification and attribute inference attacks) against LDP protocols for multidimensional data following two state-of-the-art solutions for frequency estimation of multiple attributes. To broaden the scope of our study, we have also experimentally assessed five widely used LDP protocols, namely, generalized randomized response, optimal local hashing, subset selection, RAPPOR and optimal unary encoding. Finally, we also proposed a countermeasure that improves both utility and robustness against the identified threats. Our contributions can help practitioners aiming to collect users' statistics privately to decide which LDP mechanism best fits their needs.",https://arxiv.org/abs/2209.01685
Learning to Predict Fitness for Duty using Near Infrared PeriocularIris Images,"JuanTapia, DanielBenalcazar, AndresValenzuela, LeonardoCausa, Enrique LopezDroguett, ChristophBusch",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," This research proposes a new database and method to detect the reduction of alertness conditions due to alcohol, drug consumption and sleepiness deprivation from Near-Infra-Red (NIR) periocular eye images. The study focuses on determining the effect of external factors on the Central Nervous System (CNS). The goal is to analyse how this impacts iris and pupil movement behaviours and if it is possible to classify these changes with a standard iris NIR capture device. This paper proposes a modified MobileNetV2 to classify iris NIR images taken from subjects under alcohol/drugs/sleepiness influences. The results show that the MobileNetV2-based classifier can detect the Unfit alertness condition from iris samples captured after alcohol and drug consumption robustly with a detection accuracy of 91.3% and 99.1%, respectively. The sleepiness condition is the most challenging with 72.4%. For two-class grouped images belonging to the Fit/Unfit classes, the model obtained an accuracy of 94.0% and 84.0%, respectively, using a smaller number of parameters than the standard Deep learning Network algorithm. This work is a step forward in biometric applications for developing an automatic system to classify ""Fitness for Duty"" and prevent accidents due to alcohol/drug consumption and sleepiness.",https://arxiv.org/abs/2209.01684
Behavioural Curves Analysis Using Near-Infrared-Iris Image Sequences,"L.Causa (1)J. E.Tapia, E. Lopez-Droguett (4) A.Valenzuela(2) D.Benalcazar(2) C.Busch (3)((1) TOC Biometrics Research and Development Centre Chile. (2) Universidadde Chile DIMEC Chile. (3) da/sec-Biometrics and Internet Security ResearchGroup Hochschule Darmstadt Germany. ",04-mar-22,Computer Vision and Pattern Recognition (cs.CV)," This paper proposes a new method to estimate behavioural curves from a stream of Near-Infra-Red (NIR) iris video frames. This method can be used in a Fitness For Duty system (FFD). The research focuses on determining the effect of external factors such as alcohol, drugs, and sleepiness on the Central Nervous System (CNS). The aim is to analyse how this behaviour is represented on iris and pupil movements and if it is possible to capture these changes with a standard NIR camera. The behaviour analysis showed essential differences in pupil and iris behaviour to classify the workers in ""Fit"" or ""Unfit"" conditions. The best results can distinguish subjects robustly under alcohol, drug consumption, and sleep conditions. The Multi- Layer-Perceptron and Gradient Boosted Machine reached the best results in all groups with an overall accuracy for Fit and Unfit classes of 74.0% and 75.5%, respectively. These results open a new application for iris capture devices.",https://arxiv.org/abs/2209.01683
Communication Efficient Distributed Learning over Wireless Channels,"IdanAchituve, Wenbo Wang, EthanFetaya, Amir Leshem",04-sep-22,Networking and Internet Architecture (cs.NI)," Vertical distributed learning exploits the local features collected by multiple learning workers to form a better global model. However, the exchange of data between the workers and the model aggregator for parameter training incurs a heavy communication burden, especially when the learning system is built upon capacity-constrained wireless networks. In this paper, we propose a novel hierarchical distributed learning framework, where each worker separately learns a low-dimensional embedding of their local observed data. Then, they perform communication efficient distributed max-pooling for efficiently transmitting the synthesized input to the aggregator. For data exchange over a shared wireless channel, we propose an opportunistic carrier sensing-based protocol to implement the max-pooling operation for the output data from all the learning workers. Our simulation experiments show that the proposed learning framework is able to achieve almost the same model accuracy as the learning model using the concatenation of all the raw outputs from the learning workers, while requiring a communication load that is independent of the number of workers.",https://arxiv.org/abs/2203.02488
FairSNA: Algorithmic Fairness in Social Network Analysis,"AkratiSaxena, GeorgeFletcher, MykolaPechenizkiy",04-sep-22,Social and Information Networks (cs.SI)," In recent years, designing fairness-aware methods has received much attention in various domains, including machine learning, natural language processing, and information retrieval. However, understanding structural bias and inequalities in social networks and designing fairness- aware methods for various research problems in social network analysis (SNA) have not received much attention. In this work, we highlight how the structural bias of social networks impacts the fairness of different SNA methods. We further discuss fairness aspects that should be considered while proposing network structure-based solutions for different SNA problems, such as link prediction, influence maximization, centrality ranking, and community detection. This paper clearly highlights that very few works have considered fairness and bias while proposing solutions; even these works are mainly focused on some research topics, such as link prediction, influence maximization, and PageRank. However, fairness has not yet been addressed for other research topics, such as influence blocking and community detection. We review state-of-the-art for different research topics in SNA, including the considered fairness constraints, their limitations, and our vision. This paper also covers evaluation metrics, available datasets, and synthetic network generating models used in such studies. Finally, we highlight various open research directions that require researchers' attention to bridge the gap between fairness and SNA.",https://arxiv.org/abs/2209.01682
National Power as Network Flow,MichaelPoulshock,04-sep-22,Social and Information Networks (cs.SI)," Political power in the international context can be characterized as a fluid-like substance that circulates through a network of nation states. States can possess it as a stock quantity, reflected by their material capacity or national wealth; and they can transfer it as a flow quantity, through constructive or destructive action. Constructive activities like trade increase a state's power, while destructive ones like violent conflict reduce it. In this paper, we quantify these assertions to a first approximation using economic and military data, parameterizing a mathematical model that can forecast the evolution of power in the international system.",https://arxiv.org/abs/2209.01678
One year of COVID-19 vaccine misinformation on Twitter,"FrancescoPierri, Matthew R.DeVerna, Kai-ChengYang, DavidAxelrod, JohnBryden, FilippoMenczer","4 Sep 2022 (v1(https://arxiv.org/abs/2209.01675v1)), lastrevised 7 Sep 2022 (this version, v2)",Social and Information Networks (cs.SI)," We collected almost 300M English-language tweets related to COVID-19 vaccines using a list of over 80 relevant keywords over a period of 12 months. We then extracted and labeled news articles at the source level, based on third-party lists of low-credibility and mainstream news sources, and measured the prevalence of different kinds of information. We also considered suspicious YouTube videos shared on Twitter. To identify spreaders of vaccine misinformation, we focused on verified Twitter accounts and employed a bot detection algorithm to identify accounts that are likely automated. Our findings show a low prevalence of low-credibility information compared to mainstream news. However, most popular low-credibility sources had reshare volumes comparable to many mainstream sources, and larger volumes than authoritative sources such as the U.S. Centers for Disease Control and Prevention and the World Health Organization. Throughout the year, we observed an increasing trend in the prevalence of low-credibility news relative to mainstream news about vaccines. We also observed a considerable amount of suspicious YouTube videos shared on Twitter. We found that tweets by a small group of about 800 superspreaders verified by Twitter accounted for approximately 35% of all reshares of misinformation on the average day, with the top superspreader (RobertKennedyJr) being responsible for over 13% of retweets. We also found that low-credibility news and suspicious YouTube videos were more likely to be shared by automated accounts.",https://arxiv.org/abs/2209.01677
Spatial motion planning with Pythagorean Hodograph curves,"JonArrizabalaga, Markus Ryll",04-sep-22,Robotics (cs.RO)," This paper presents a two-stage prediction-based control scheme for embedding the environment's geometric properties into a collision-free Pythagorean Hodograph spline, and subsequently finding the optimal path within the parameterized free space. The ingredients of this approach are twofold: First, we present a novel spatial path parameterization applicable to any arbitrary curve without prior assumptions in its adapted frame. Second, we identify the appropriateness of Pythagorean Hodograph curves for a compact and continuous definition of the path-parametric functions required by the presented spatial model. This dual-stage formulation results in a motion planning approach, where the geometric properties of the environment arise as states of the prediction model. Thus, the presented method is attractive for motion planning in dense environments. The efficacy of the approach is evaluated according to an illustrative example.",https://arxiv.org/abs/2209.01675
Extension of a Linear Controller Scheme to Non-Linear Systems and itsApplication on Inverted Pendulum,"JustinJacob, NavinKhaneja",04-sep-22,Systems and Control (eess.SY)," This paper presents the control and stabilization of the rotary inverted pendulum based on a general controller scheme. The proposed scheme has its foundation in classical control theory, and the importance of an integrator in disturbance rejection is emphasized. The system's dynamics are obtained by the Euler Lagrange method and are approximated for small-angle as balancing the pendulum is the objective. Experimental results demonstrate that the proposed control scheme can achieve the stabilization of a non- linear system. Also, the boundedness and convergence of the non-linear system with the controller subjected to the initial condition are validated.",https://arxiv.org/abs/2209.01673
A General Controller Scheme for Stabilization & Disturbance Rejectionwith Application to Non-Linear Systems and its Implementation on 2 DOFHelicopter,"JustinJacob, NavinKhaneja",07-jul-21,Systems and Control (eess.SY)," A general controller scheme for stabilizing a non-linear system, which has its origin from the linear system theory, is proposed in this paper. The proposed controller can stabilize the non-linear system subjected to initial conditions. An effective way to obtain the controller parameters is presented with the knowledge of the system model. The controller is designed for the linear time-invariant (LTI) system, which can reject any disturbance acting on it. Paper emphasis the idea of an integrator controller in disturbance rejection. The concept is extended to the application to non-linear systems where the non-linearities are assessed as the disturbance to the refined linear part of the system. Boundedness and convergence of the non-linear system with the controller are proved to justify system stabilization. Hardware implementation of the controller on the 2 dof helicopter model is presented with experimental results, which validates the proposed control scheme.",https://arxiv.org/abs/2209.01668
A Review of Sparse Expert Models in Deep Learning,"WilliamFedus, JeffDean, BarretZoph",04-sep-22,Machine Learning (cs.LG)," Sparse expert models are a thirty-year old concept re-emerging as a popular architecture in deep learning. This class of architecture encompasses Mixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, and others, all with the unifying idea that each example is acted on by a subset of the parameters. By doing so, the degree of sparsity decouples the parameter count from the compute per example allowing for extremely large, but efficient models. The resulting models have demonstrated significant improvements across diverse domains such as natural language processing, computer vision, and speech recognition. We review the concept of sparse expert models, provide a basic description of the common algorithms, contextualize the advances in the deep learning era, and conclude by highlighting areas for future work.",https://arxiv.org/abs/2107.03117
Exposure-Aware Recommendation using Contextual Bandits,"MasoudMansoury, BamshadMobasher, Herke vanHoof",04-sep-22,Information Retrieval (cs.IR)," Exposure bias is a well-known issue in recommender systems where items and suppliers are not equally represented in the recommendation results. This is especially problematic when bias is amplified over time as a few items (e.g., popular ones) are repeatedly over-represented in recommendation lists and users' interactions with those items will amplify bias towards those items over time resulting in a feedback loop. This issue has been extensively studied in the literature on model-based or neighborhood-based recommendation algorithms, but less work has been done on online recommendation models, such as those based on top-K contextual bandits, where recommendation models are dynamically updated with ongoing user feedback. In this paper, we study exposure bias in a class of well- known contextual bandit algorithms known as Linear Cascading Bandits. We analyze these algorithms on their ability to handle exposure bias and provide a fair representation for items in the recommendation results. Our analysis reveals that these algorithms tend to amplify exposure disparity among items over time. In particular, we observe that these algorithms do not properly adapt to the feedback provided by the users and frequently recommend certain items even when those items are not selected by users. To mitigate this bias, we propose an Exposure-Aware (EA) reward model that updates the model parameters based on two factors: 1) user feedback (i.e., clicked or not), and 2) position of the item in the recommendation list. This way, the proposed model controls the utility assigned to items based on their exposure in the recommendation list. Extensive experiments on two real-world datasets using three contextual bandit algorithms show that the proposed reward model reduces exposure bias amplification in long run while maintaining the recommendation accuracy.",https://arxiv.org/abs/2209.01667
Alcohol Consumption Detection from Periocular NIR Images Using CapsuleNetwork,"JuanTapia, Enrique LopezDroguett, ChristophBusch",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," This research proposes a method to detect alcohol consumption from Near-Infra-Red (NIR) periocular eye images. The study focuses on determining the effect of external factors such as alcohol on the Central Nervous System (CNS). The goal is to analyse how this impacts on iris and pupil movements and if it is possible to capture these changes with a standard iris NIR camera. This paper proposes a novel Fused Capsule Network (F-CapsNet) to classify iris NIR images taken under alcohol consumption subjects. The results show the F-CapsNet algorithm can detect alcohol consumption in iris NIR images with an accuracy of 92.3% using half of the parameters as the standard Capsule Network algorithm. This work is a step forward in developing an automatic system to estimate ""Fitness for Duty"" and prevent accidents due to alcohol consumption.",https://arxiv.org/abs/2209.01665
ArgLegalSumm: Improving Abstractive Summarization of Legal Documentswith Argument Mining,"MohamedElaraby, DianeLitman",04-sep-22,Computation and Language (cs.CL), A challenging task when generating summaries of legal documents is the ability to address their argumentative nature. We introduce a simple technique to capture the argumentative structure of legal documents by integrating argument role labeling into the summarization process. Experiments with pretrained language models show that our proposed approach improves performance over strong baselines,https://arxiv.org/abs/2209.01657
SCL-RAI: Span-based Contrastive Learning with Retrieval AugmentedInference for Unlabeled Entity Problem in NER,"ShuzhengSi, ShuangZeng, JiaxingLin, BaobaoChang",04-sep-22,Computation and Language (cs.CL)," Named Entity Recognition is the task to locate and classify the entities in the text. However, Unlabeled Entity Problem in NER datasets seriously hinders the improvement of NER performance. This paper proposes SCL-RAI to cope with this problem. Firstly, we decrease the distance of span representations with the same label while increasing it for different ones via span-based contrastive learning, which relieves the ambiguity among entities and improves the robustness of the model over unlabeled entities. Then we propose retrieval augmented inference to mitigate the decision boundary shifting problem. Our method significantly outperforms the previous SOTA method by 4.21% and 8.64% F1-score on two real-world datasets.",https://arxiv.org/abs/2209.01650
Fraud Detection Using Optimized Machine Learning Tools Under ImbalanceClasses,"MaryIsangediok, KelumGajamannage",04-sep-22,Machine Learning (cs.LG)," Fraud detection is a challenging task due to the changing nature of fraud patterns over time and the limited availability of fraud examples to learn such sophisticated patterns. Thus, fraud detection with the aid of smart versions of machine learning (ML) tools is essential to assure safety. Fraud detection is a primary ML classification task; however, the optimum performance of the corresponding ML tool relies on the usage of the best hyperparameter values. Moreover, classification under imbalanced classes is quite challenging as it causes poor performance in minority classes, which most ML classification techniques ignore. Thus, we investigate four state- of-the-art ML techniques, namely, logistic regression, decision trees, random forest, and extreme gradient boost, that are suitable for handling imbalance classes to maximize precision and simultaneously reduce false positives. First, these classifiers are trained on two original benchmark unbalanced fraud detection datasets, namely, phishing website URLs and fraudulent credit card transactions. Then, three synthetically balanced datasets are produced for each original data set by implementing the sampling frameworks, namely, RandomUnderSampler, SMOTE, and SMOTEENN. The optimum hyperparameters for all the 16 experiments are revealed using the method RandomzedSearchCV. The validity of the 16 approaches in the context of fraud detection is compared using two benchmark performance metrics, namely, area under the curve of receiver operating characteristics (AUC ROC) and area under the curve of precision and recall (AUC PR). For both phishing website URLs and credit card fraud transaction datasets, the results indicate that extreme gradient boost trained on the original data shows trustworthy performance in the imbalanced dataset and manages to outperform the other three methods in terms of both AUC ROC and AUC PR.",https://arxiv.org/abs/2209.01646
IoT Book Bot,"SouvikDatta, MangolikKundu, Ratnadeep DasChoudhury, SriramalakshmiP, SreedeviVT",04-sep-22,Human-Computer Interaction (cs.HC)," In order to ease the process of library management many technologies have been adopted but most of them focus on inventory management. There has hardly been any progress of automation in the field of issuing and returning books to the library on time. In colleges and schools, hostellers often forget to timely return the issued books back to the library. To solve the above issue and to ensure timely submission of the issued books, this work develops a Book-Bot which solves these complexities. The bot can commute from point A to point B, scan and verify QR Codes and Barcodes. The bot will have a certain payload capacity for carrying books. The QR code and Barcode scanning will be enabled by a Pi Camera, OpenCV and Raspberry Pi, thus making the exchange of books safe and secure. The odometry maneuvers of the bot will be controlled manually via a Blynk App. This paper focuses on how human intervention can be reduced and automates the issue part of library management system with the help of a bot.",https://arxiv.org/abs/2209.01642
Every picture tells a story: Image-grounded controllable stylisticstory generation,"HolyLovenia, Bryan Wilie, RomainBarraud, SamuelCahyawijaya, Willy Chung, Pascale Fung",04-sep-22,Computation and Language (cs.CL)," Generating a short story out of an image is arduous. Unlike image captioning, story generation from an image poses multiple challenges: preserving the story coherence, appropriately assessing the quality of the story, steering the generated story into a certain style, and addressing the scarcity of image-story pair reference datasets limiting supervision during training. In this work, we introduce Plug-and-Play Story Teller (PPST) and improve image-to-story generation by: 1) alleviating the data scarcity problem by incorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a fluent image-to-text generation with minimal supervision, and 2) enabling a more style-relevant generation by incorporating stylistic adapters to control the story generation. We conduct image-to-story generation experiments with non-styled, romance-styled, and action-styled PPST approaches and compare our generated stories with those of previous work over three aspects, i.e., story coherence, image-story relevance, and style fitness, using both automatic and human evaluation. The results show that PPST improves story coherence and has better image-story relevance, but has yet to be adequately stylistic.",https://arxiv.org/abs/2209.01641
Joint Linear and Nonlinear Computation across Functions for EfficientPrivacy-Preserving Neural Network Inference,"QiaoZhang, TaoXiang, Chunsheng Xin, Biwen Chen, Hongyi Wu",04-sep-22,Cryptography and Security (cs.CR)," While it is encouraging to witness the recent development in privacy-preserving Machine Learning as a Service (MLaaS), there still exists a significant performance gap for its deployment in real-world applications. We observe the state-of-the-art frameworks follow a compute-and-share principle for every function output where the summing in linear functions, which is the last of two steps for function output, involves all rotations (which is the most expensive HE operation), and the multiplexing in nonlinear functions, which is also the last of two steps for function output, introduces noticeable communication rounds. Therefore, we challenge the conventional compute-and-share logic and introduce the first joint linear and nonlinear computation across functions that features by 1) the PHE triplet for computing the nonlinear function, with which the multiplexing is eliminated; 2) the matrix encoding to calculate the linear function, with which all rotations for summing is removed; and 3) the network adaptation to reassemble the model structure, with which the joint computation module is utilized as much as possible. The boosted efficiency is verified by the numerical complexity, and the experiments demonstrate up to 13x speedup for various functions used in the state-of-the-art models and up to 5x speedup over mainstream neural networks.",https://arxiv.org/abs/2209.01638
Towards Adaptive Storage Views in Virtual Memory,"FelixSchuhknecht, JustusHenneberg",04-sep-22,Databases (cs.DB)," Traditionally, DBMSs separate their storage layer from their indexing layer. While the storage layer physically materializes the database and provides low-level access methods to it, the indexing layer on top enables a faster locating of searched-for entries. While this clearly separates concerns, it also adds a level of indirection to the already complex execution path. In this work, we propose an alternative design: Instead of conservatively separating both layers, we naturally fuse them by integrating an adaptive coarse-granular indexing scheme directly into the storage layer. We do so by utilizing tools of the virtual memory management subsystem provided by the OS: On the lowest level, we materialize the database content in form of physical main memory. On top of that, we allow the creation of arbitrarily many virtual memory storage views that map to subsets of the database having certain properties of interest. This creation happens fully adaptively as a side-product of query processing. To speed up query answering, we route each query automatically to the most fitting virtual view(s). By this, we naturally index the storage layer in its core and gradually improve the provided scan performance.",https://arxiv.org/abs/2209.01637
Single-source Domain Expansion Network for Cross-Scene HyperspectralImage Classification,"YuxiangZhang, WeiLi, WeidongSun, RanTao, QianDu",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Currently, cross-scene hyperspectral image (HSI) classification has drawn increasing attention. It is necessary to train a model only on source domain (SD) and directly transferring the model to target domain (TD), when TD needs to be processed in real time and cannot be reused for training. Based on the idea of domain generalization, a Single-source Domain Expansion Network (SDEnet) is developed to ensure the reliability and effectiveness of domain extension. The method uses generative adversarial learning to train in SD and test in TD. A generator including semantic encoder and morph encoder is designed to generate the extended domain (ED) based on encoder-randomization-decoder architecture, where spatial and spectral randomization are specifically used to generate variable spatial and spectral information, and the morphological knowledge is implicitly applied as domain invariant information during domain expansion. Furthermore, the supervised contrastive learning is employed in the discriminator to learn class-wise domain invariant representation, which drives intra-class samples of SD and ED. Meanwhile, adversarial training is designed to optimize the generator to drive intra-class samples of SD and ED to be separated. Extensive experiments on two public HSI datasets and one additional multispectral image (MSI) dataset demonstrate the superiority of the proposed method when compared with state-of-the-art techniques.",https://arxiv.org/abs/2209.01635
Older adults acceptance of SARs: The link between anticipated andactual interaction,"MayaKrakovski, OdedZafrani, GalitNimrod, Yael Edan",04-sep-22,Robotics (cs.RO)," This study demonstrates how anticipated and actual interactions shape the QE of SARs among older adults. The study consisted of two parts: an online survey to explore the anticipated interaction through video viewing of a SAR and an acceptance study where older adults interacted with the robot. Both parts of this study were completed with the assistance of Gymmy, a robotic system that our lab developed for training older adults in physical and cognitive activities. Both study parts exhibited similar user responses, indicating that users acceptance of SARs can be predicted by their anticipated interaction. Index Terms: Aging, human-robot interaction, older adults, quality evaluation, socially assistive robots, technology acceptance, technophobia, trust, user experience.",https://arxiv.org/abs/2209.01634
"IEEE Trust, Acceptance and Social Cues in Human-Robot Interaction --SCRITA 2022 Workshop","AlessandraRossi, PatrickHolthaus, SÃ¬lviaMoros, GabriellaLakatos","22 Aug 2022 (v1(https://arxiv.org/abs/2208.11090v1)), lastrevised 28 Aug 2022 (this version, v2)",Robotics (cs.RO)," The Trust, Acceptance and Social Cues in Human-Robot Interaction - SCRITA is the 5th edition of a series of workshops held in conjunction with the IEEE RO-MAN conference. This workshop focuses on addressing the challenges and development of the dynamics between people and robots in order to foster short interactions and long-lasting relationships in different fields, from educational, service, collaborative, companion, care- home and medical robotics. In particular, we aimed in investigating how robots can manipulate (i.e. creating, improving, and recovering) people's ability of accepting and trusting them for a fruitful and successful coexistence between humans and people. While advanced progresses are reached in studying and evaluating the factors affecting acceptance and trust of people in robots in controlled or short-term (repeated interactions) setting, developing service and personal robots, that are accepted and trusted by people where the supervision of operators is not possible, still presents an open challenge for scientists in robotics, AI and HRI fields. In such unstructured static and dynamic human-centred environments scenarios, robots should be able to learn and adapt their behaviours to the situational context, but also to people's prior experiences and learned associations, their expectations, and their and the robot's ability to predict and understand each other's behaviours. Although the previous editions valued the participation of leading researchers in the field and several exceptional invited speakers who tackled down some fundamental points in this research domains, we wish to continue to further explore the role of trust in robotics to present groundbreaking research to effectively design and develop socially acceptable and trustable robots to be deployed ""in the wild"".   Website: [this https URL](https://scrita.herts.ac.uk)",https://arxiv.org/abs/2209.01624
Computing Generalized Convolutions Faster Than Brute Force,"BarÄ±ÅŸ CanEsmer, Ariel Kulik, DÃ¡niel Marx, PhilippSchepper, KarolWÄ™grzycki",04-sep-22,Data Structures and Algorithms (cs.DS)," In this paper, we consider a general notion of convolution. Let $D$ be a finite domain and let $D^n$ be the set of $n$-length vectors (tuples) of $D$. Let $f : D \times D \to D$ be a function and let $\oplus_f$ be a coordinate-wise application of $f$. The $f$-Convolution of two functions $g,h : D^n \to \\{-M,\ldots,M\\}$ is $$(g \otimes_f h)(\textbf{v}) := \sum_{\substack{\textbf{v}_g,\textbf{v}_h \in D^n \text{s.t. } \textbf{v}_g \oplus_f \textbf{v}_h}} g(\textbf{v}_g) \cdot h(\textbf{v}_h) $$ for every $\textbf{v} \in D^n$. This problem generalizes many fundamental convolutions such as Subset Convolution, XOR Product, Covering Product or Packing Product, etc. For arbitrary function $f$ and domain $D$ we can compute $f$-Convolution via brute-force enumeration in $\widetilde{O}(|D|^{2n}\mathrm{polylog}(M))$ time.   Our main result is an improvement over this naive algorithm. We show that $f$-Convolution can be computed exactly in $\widetilde{O}((c \cdot |D|^2)^{n}\mathrm{polylog(M)})$ for constant $c := 5/6$ when $D$ has even cardinality. Our main observation is that a \emph{cyclic partition} of a function $f : D \times D \to D$ can be used to speed up the computation of $f$-Convolution, and we show that an appropriate cyclic partition exists for every $f$.   Furthermore, we demonstrate that a single entry of the $f$-Convolution can be computed more efficiently. In this variant, we are given two functions $g,h : D^n \to \\{-M,\ldots,M\\}$ alongside with a vector $\textbf{v} \in D^n$ and the task of the $f$-Query problem is to compute integer $(g \otimes_f h)(\textbf{v})$. This is a generalization of the well-known Orthogonal Vectors problem. We show that $f$-Query can be computed in $\widetilde{O}(|D|^{\frac{\omega}{2} n}\mathrm{polylog}(M))$ time, where $\omega \in [2,2.373)$ is the exponent of currently fastest matrix multiplication algorithm.",https://arxiv.org/abs/2208.11090
Robotic Exercise Trainer: How Failures and T-HRI Levels Affect UserAcceptance and Trust,"MayaKrakovski, NaamaAharony, Yael Edan",04-sep-22,Robotics (cs.RO)," Physical activity is important for health and wellbeing, but only few fulfill the World Health Organization's criteria for physical activity. The development of a robotic exercise trainer can assist in increasing training accessibility and motivation. The acceptance and trust of users are crucial for the successful implementation of such an assistive robot. This can be affected by the transparency of the robotic system and the robot's performance, specifically, its failures. The study presents an initial investigation into the transparency levels as related to the task, human, robot, and interaction (T-HRI), with robot behavior adjusted accordingly. A failure in robot performance during part of the experiments allowed to analyze the effect of the T-HRI levels as related to failures. Participants who experienced failure in the robot's performance demonstrated a lower level of acceptance and trust than those who did not experience this failure. In addition, there were differences in acceptance measures between T-HRI levels and participant groups, suggesting several directions for future research.",https://arxiv.org/abs/2209.01623
"IEEE Trust, Acceptance and Social Cues in Human-Robot Interaction --SCRITA 2022 Workshop","AlessandraRossi, PatrickHolthaus, SÃ¬lviaMoros, GabriellaLakatos","22 Aug 2022 (v1(https://arxiv.org/abs/2208.11090v1)), lastrevised 28 Aug 2022 (this version, v2)",Robotics (cs.RO)," The Trust, Acceptance and Social Cues in Human-Robot Interaction - SCRITA is the 5th edition of a series of workshops held in conjunction with the IEEE RO-MAN conference. This workshop focuses on addressing the challenges and development of the dynamics between people and robots in order to foster short interactions and long-lasting relationships in different fields, from educational, service, collaborative, companion, care- home and medical robotics. In particular, we aimed in investigating how robots can manipulate (i.e. creating, improving, and recovering) people's ability of accepting and trusting them for a fruitful and successful coexistence between humans and people. While advanced progresses are reached in studying and evaluating the factors affecting acceptance and trust of people in robots in controlled or short-term (repeated interactions) setting, developing service and personal robots, that are accepted and trusted by people where the supervision of operators is not possible, still presents an open challenge for scientists in robotics, AI and HRI fields. In such unstructured static and dynamic human-centred environments scenarios, robots should be able to learn and adapt their behaviours to the situational context, but also to people's prior experiences and learned associations, their expectations, and their and the robot's ability to predict and understand each other's behaviours. Although the previous editions valued the participation of leading researchers in the field and several exceptional invited speakers who tackled down some fundamental points in this research domains, we wish to continue to further explore the role of trust in robotics to present groundbreaking research to effectively design and develop socially acceptable and trustable robots to be deployed ""in the wild"".   Website: [this https URL](https://scrita.herts.ac.uk)",https://arxiv.org/abs/2209.01622
Interactive Question Answering Systems: Literature Review,"Giovanni MariaBiancofiore, YasharDeldjoo, Tommaso DiNoia, Eugenio DiSciascio, FedelucioNarducci",04-sep-22,Computation and Language (cs.CL)," Question answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. This survey offers a detailed overview of the interactive question-answering methods that are prevalent in current literature. It begins by explaining the foundational principles of question-answering systems, hence defining new notations and taxonomies to combine all identified works inside a unified framework. The reviewed published work on interactive question-answering systems is then presented and examined in terms of its proposed methodology, evaluation approaches, and dataset/application domain. We also describe trends surrounding specific tasks and issues raised by the community, so shedding light on the future interests of scholars. Our work is further supported by a GitHub page with a synthesis of all the major topics covered in this literature study. [this https URL](https://sisinflab.github.io/interactive-question-answering- systems-survey/)",https://arxiv.org/abs/2208.11090
MAFormer: A Transformer Network with Multi-scale Attention Fusion forVisual Recognition,"YunhaoWang, HuixinSun, XiaodiWang, BinZhang, ChaoLi, YingXin, BaochangZhang, ErruiDing, ShuminHan",31 Aug 2022,Computer Vision and Pattern Recognition (cs.CV)," Vision Transformer and its variants have demonstrated great potential in various computer vision tasks. But conventional vision transformers often focus on global dependency at a coarse level, which suffer from a learning challenge on global relationships and fine-grained representation at a token level. In this paper, we introduce Multi-scale Attention Fusion into transformer (MAFormer), which explores local aggregation and global feature extraction in a dual-stream framework for visual recognition. We develop a simple but effective module to explore the full potential of transformers for visual representation by learning fine- grained and coarse-grained features at a token level and dynamically fusing them. Our Multi-scale Attention Fusion (MAF) block consists of: i) a local window attention branch that learns short-range interactions within windows, aggregating fine-grained local features; ii) global feature extraction through a novel Global Learning with Down-sampling (GLD) operation to efficiently capture long-range context information within the whole image; iii) a fusion module that self-explores the integration of both features via attention. Our MAFormer achieves state-of-the-art performance on common vision tasks. In particular, MAFormer-L achieves 85.9$\%$ Top-1 accuracy on ImageNet, surpassing CSWin-B and LV-ViT-L by 1.7$\%$ and 0.6$\%$ respectively. On MSCOCO, MAFormer outperforms the prior art CSWin by 1.7$\%$ mAPs on object detection and 1.4$\%$ on instance segmentation with similar- sized parameters, demonstrating the potential to be a general backbone network.",https://arxiv.org/abs/2209.01621
Interpreting systems as solving POMDPs: a step towards a formalunderstanding of agency,"MartinBiehl, NathanielVirgo",04-sep-22,Artificial Intelligence (cs.AI)," Under what circumstances can a system be said to have beliefs and goals, and how do such agency-related features relate to its physical state? Recent work has proposed a notion of interpretation map, a function that maps the state of a system to a probability distribution representing its beliefs about an external world. Such a map is not completely arbitrary, as the beliefs it attributes to the system must evolve over time in a manner that is consistent with Bayes' theorem, and consequently the dynamics of a system constrain its possible interpretations. Here we build on this approach, proposing a notion of interpretation not just in terms of beliefs but in terms of goals and actions. To do this we make use of the existing theory of partially observable Markov processes (POMDPs): we say that a system can be interpreted as a solution to a POMDP if it not only admits an interpretation map describing its beliefs about the hidden state of a POMDP but also takes actions that are optimal according to its belief state. An agent is then a system together with an interpretation of this system as a POMDP solution. Although POMDPs are not the only possible formulation of what it means to have a goal, this nevertheless represents a step towards a more general formal definition of what it means for a system to be an agent.",https://arxiv.org/abs/2209.01620
Flux Linkage Based Evaluation Method for Voltage Inertia and VoltageRecovery Capability Under Large Disturbances,"YinhongLin, Huaichang Ge, Bin Wang, Qinglai Guo, Hongbin Sun",04-sep-22,Systems and Control (eess.SY)," High-voltage direct current (HVDC) transmission applications and the growth of the dynamic load in large-scale receiving-end grids lead to a higher risk of short-term voltage instability. An effective way to address this problem is to improve the system's dynamic voltage support capability by changing the operation status of dynamic var devices, and the dynamic var reserve (DVR) is commonly used. Due to the time delay in synchronous machine excitation systems, the dynamic var reserved at steady state cannot be exploited immediately under large disturbances. In addition, some reactive power is produced immediately through electromagnetic induction. The voltage support effect of the two capabilities is analyzed based on the flux linkage and an approximate simulation of the fault impact. Then two novel indexes for evaluating the voltage inertia and voltage recovery capability are proposed, which are related to the voltage nadir and voltage recovery speed. The indexes' physical meanings are revealed by comparison with the frequency response, and the potential applications in planning and optimal reactive power dispatch (ORPD) are introduced. Numerical simulations based on the IEEE 39-bus system verify that the indexes can quantify the voltage support capabilities, and the minimum voltage support requirements are obtained to maintain systems' security.",https://arxiv.org/abs/2209.01619
ProBoost: a Boosting Method for Probabilistic Classifiers,"FÃ¡bioMendonÃ§a, Sheikh ShanawazMostafa, Fernando Morgado-Dias, Antonio G. Ravelo-GarcÃ­a, MÃ¡rio A. T.Figueiredo",04-sep-22,Machine Learning (cs.LG)," ProBoost, a new boosting algorithm for probabilistic classifiers, is proposed in this work. This algorithm uses the epistemic uncertainty of each training sample to determine the most challenging/uncertain ones; the relevance of these samples is then increased for the next weak learner, producing a sequence that progressively focuses on the samples found to have the highest uncertainty. In the end, the weak learners' outputs are combined into a weighted ensemble of classifiers. Three methods are proposed to manipulate the training set: undersampling, oversampling, and weighting the training samples according to the uncertainty estimated by the weak learners. Furthermore, two approaches are studied regarding the ensemble combination. The weak learner herein considered is a standard convolutional neural network, and the probabilistic models underlying the uncertainty estimation use either variational inference or Monte Carlo dropout. The experimental evaluation carried out on MNIST benchmark datasets shows that ProBoost yields a significant performance improvement. The results are further highlighted by assessing the relative achievable improvement, a metric proposed in this work, which shows that a model with only four weak learners leads to an improvement exceeding 12% in this metric (for either accuracy, sensitivity, or specificity), in comparison to the model learned without ProBoost.",https://arxiv.org/abs/2209.01615
Generalization in Neural Networks: A Broad Survey,ChrisRohlfs,04-sep-22,Machine Learning (cs.LG)," This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain adaptation benchmark datasets available. Recent breakthroughs surveyed in (4) task generalization include few-shot meta-learning approaches and the BERT NLP engine, and recent (5) modality generalization studies are discussed that integrate image and text data and that apply a biologically-inspired network across olfactory, visual, and auditory modalities. Recent (6) scope generalization results are reviewed that embed knowledge graphs into deep NLP approaches. Additionally, concepts from neuroscience are discussed on the modular architecture of brains and the steps by which dopamine-driven conditioning leads to abstract thinking.",https://arxiv.org/abs/2209.01611
Dynamic Regret of Adaptive Gradient Methods for Strongly ConvexProblems,"ParvinNazari, EsmaileKhorram",04-sep-22,Machine Learning (cs.LG)," Adaptive gradient algorithms such as ADAGRAD and its variants have gained popularity in the training of deep neural networks. While many works as for adaptive methods have focused on the static regret as a performance metric to achieve a good regret guarantee, the dynamic regret analyses of these methods remain unclear. As opposed to the static regret, dynamic regret is considered to be a stronger concept of performance measurement in the sense that it explicitly elucidates the non-stationarity of the environment. In this paper, we go through a variant of ADAGRAD (referred to as M-ADAGRAD ) in a strong convex setting via the notion of dynamic regret, which measures the performance of an online learner against a reference (optimal) solution that may change over time. We demonstrate a regret bound in terms of the path-length of the minimizer sequence that essentially reflects the non-stationarity of environments. In addition, we enhance the dynamic regret bound by exploiting the multiple accesses of the gradient to the learner in each round. Empirical results indicate that M-ADAGRAD works also well in practice.",https://arxiv.org/abs/2209.01610
A Case Study on the Classification of Lost Circulation Events DuringDrilling using Machine Learning Techniques on an Imbalanced Large Dataset,"Toluwalase A.Olukoga, Yin Feng","4 Sep 2022 (v1(https://arxiv.org/abs/2209.01607v1)), lastrevised 7 Sep 2022 (this version, v2)",Machine Learning (cs.LG)," This study presents machine learning models that forecast and categorize lost circulation severity preemptively using a large class imbalanced drilling dataset. We demonstrate reproducible core techniques involved in tackling a large drilling engineering challenge utilizing easily interpretable machine learning approaches.   We utilized a 65,000+ records data with class imbalance problem from Azadegan oilfield formations in Iran. Eleven of the dataset's seventeen parameters are chosen to be used in the classification of five lost circulation events. To generate classification models, we used six basic machine learning algorithms and four ensemble learning methods. Linear Discriminant Analysis (LDA), Logistic Regression (LR), Support Vector Machines (SVM), Classification and Regression Trees (CART), k-Nearest Neighbors (KNN), and Gaussian Naive Bayes (GNB) are the six fundamental techniques. We also used bagging and boosting ensemble learning techniques in the investigation of solutions for improved predicting performance. The performance of these algorithms is measured using four metrics: accuracy, precision, recall, and F1-score. The F1-score weighted to represent the data imbalance is chosen as the preferred evaluation criterion.   The CART model was found to be the best in class for identifying drilling fluid circulation loss events with an average weighted F1-score of 0.9904 and standard deviation of 0.0015. Upon application of ensemble learning techniques, a Random Forest ensemble of decision trees showed the best predictive performance. It identified and classified lost circulation events with a perfect weighted F1-score of 1.0. Using Permutation Feature Importance (PFI), the measured depth was found to be the most influential factor in accurately recognizing lost circulation events while drilling.",https://arxiv.org/abs/2209.01608
CloudVision: DNN-based Visual Localization of Autonomous Robots usingPrebuilt LiDAR Point Cloud,"EvgenyYudin, PavelKarpyshev, MikhailKurenkov, AlenaSavinykh, AndreiPotapov, EvgenyKruzhkov, DzmitryTsetserukou",04-sep-22,Robotics (cs.RO)," In this study, we propose a novel visual localization approach to accurately estimate six degrees of freedom (6-DoF) poses of the robot within the 3D LiDAR map based on visual data from an RGB camera. The 3D map is obtained utilizing an advanced LiDAR-based simultaneous localization and mapping (SLAM) algorithm capable of collecting a precise sparse map. The features extracted from the camera images are compared with the points of the 3D map, and then the geometric optimization problem is being solved to achieve precise visual localization. Our approach allows employing a scout robot equipped with an expensive LiDAR only once - for mapping of the environment, and multiple operational robots with only RGB cameras onboard - for performing mission tasks, with the localization accuracy higher than common camera-based solutions. The proposed method was tested on the custom dataset collected in the Skolkovo Institute of Science and Technology (Skoltech). During the process of assessing the localization accuracy, we managed to achieve centimeter-level accuracy; the median translation error was as low as 1.3 cm. The precise positioning achieved with only cameras makes possible the usage of autonomous mobile robots to solve the most complex tasks that require high localization accuracy.",https://arxiv.org/abs/2209.01607
Representative Image Feature Extraction via Contrastive LearningPretraining for Chest X-ray Report Generation,"Yu-JenChen, Wei-Hsiang Shen, Hao-WeiChung, Jing-Hao Chiu, Da-Cheng Juan, Tsung-Ying Ho, Chi-TungCheng, Meng-Lin Li, Tsung-Yi Ho",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Medical report generation is a challenging task since it is time- consuming and requires expertise from experienced radiologists. The goal of medical report generation is to accurately capture and describe the image findings. Previous works pretrain their visual encoding neural networks with large datasets in different domains, which cannot learn general visual representation in the specific medical domain. In this work, we propose a medical report generation framework that uses a contrastive learning approach to pretrain the visual encoder and requires no additional meta information. In addition, we adopt lung segmentation as an augmentation method in the contrastive learning framework. This segmentation guides the network to focus on encoding the visual feature within the lung region. Experimental results show that the proposed framework improves the performance and the quality of the generated medical reports both quantitatively and qualitatively.",https://arxiv.org/abs/2209.01605
DMiner: Dashboard Design Mining and Recommendation,"YannaLin, HaotianLi, AoyuWu, YongWang, HuaminQu",04-sep-22,Human-Computer Interaction (cs.HC)," Dashboards, which comprise multiple views on a single display, help analyze and communicate multiple perspectives of data simultaneously. However, creating effective and elegant dashboards is challenging since it requires careful and logical arrangement and coordination of multiple visualizations. To solve the problem, we propose a data-driven approach for mining design rules from dashboards and automating dashboard organization. Specifically, we focus on two prominent aspects of the organization: arrangement, which describes the position, size, and layout of each view in the display space; and coordination, which indicates the interaction between pairwise views. We build a new dataset containing 854 dashboards crawled online, and develop feature engineering methods for describing the single views and view-wise relationships in terms of data, encoding, layout, and interactions. Further, we identify design rules among those features and develop a recommender for dashboard design. We demonstrate the usefulness of DMiner through an expert study and a user study. The expert study shows that our extracted design rules are reasonable and conform to the design practice of experts. Moreover, a comparative user study shows that our recommender could help automate dashboard organization and reach human-level performance. In summary, our work offers a promising starting point for design mining visualizations to build recommenders.",https://arxiv.org/abs/2209.01604
Data-Assisted Vision-Based Hybrid Control for Robust Stabilizationwith Obstacle Avoidance via Learning of Perception Maps,"Alejandro Murillo-Gonzalez, Jorge I.Poveda",04-sep-22,Robotics (cs.RO)," We study the problem of target stabilization with robust obstacle avoidance in robots and vehicles that have access only to vision-based sensors for the purpose of realtime localization. This problem is particularly challenging due to the topological obstructions induced by the obstacle, which preclude the existence of smooth feedback controllers able to achieve simultaneous stabilization and robust obstacle avoidance. To overcome this issue, we develop a vision-based hybrid controller that switches between two different feedback laws depending on the current position of the vehicle using a hysteresis mechanism and a data-assisted supervisor. The main innovation of the paper is the incorporation of suitable perception maps into the hybrid controller. These maps can be learned from data obtained from cameras in the vehicles and trained via convolutional neural networks (CNN). Under suitable assumptions on this perception map, we establish theoretical guarantees for the trajectories of the vehicle in terms of convergence and obstacle avoidance. Moreover, the proposed vision-based hybrid controller is numerically tested under different scenarios, including noisy data, sensors with failures, and cameras with occlusions.",https://arxiv.org/abs/2209.01599
A Prufer-Sequence Based Representation of Large Graphs for StructuralEncoding of Logic Networks,"ManjariPradhan, Bhargab B.Bhattacharya",04-sep-22,Machine Learning (cs.LG)," The pervasiveness of graphs in today's real life systems is quite evident, where the system either explicitly exists as graph or can be readily modelled as one. Such graphical structure is thus a store house rich information. This has various implication depending on whether we are interested in a node or the graph as a whole. In this paper, we are primarily concerned with the later, that is, the inference that the structure of the graph influences the property of the real life system it represents. A model of such structural influence would be useful in inferencing useful properties of complex and large systems, like VLSI circuits, through its structural property. However, before we can apply some machine learning (ML) based technique to model such relationship, an effective representation of the graph is imperative. In this paper, we propose a graph representation which is lossless, linear-sized in terms of number of vertices and gives a 1-D representation of the graph. Our representation is based on Prufer encoding for trees. Moreover, our method is based on a novel technique, called $\mathcal{GT}$-enhancement whereby we first transform the graph such that it can be represented by a singular tree. The encoding also provides scope to include additional graph property and improve the interpretability of the code.",https://arxiv.org/abs/2209.01597
Incremental maximum likelihood estimation for efficient adaptivefiltering,"ShirinJalali, CarlNuzman, YueSun",04-sep-22,Information Theory (cs.IT)," Adaptive filtering is a well-known problem with a wide range of applications, including echo cancellation. Extensive research during the past few decades has led to the invention of various algorithms. However, the known computationally-efficient solutions show a tradeoff between convergence speed and accuracy. Moreover, running these algorithms involves heuristically setting various parameters that considerably affect their performances. In this paper, we propose a new algorithm which we refer to as online block maximum likelihood (OBML). OBML is a computationally-efficient online learning algorithm that employs maximum likelihood (ML) estimation every $P$ samples. We fully characterize the expected performance of OBML and show that i) OBML is able to asymptotically recover the unknown coefficients and ii) its expected estimation error asymptotically converges to zero as $O({1\over t})$. We also derive an alternative version of OBML, which we refer to as incremental maximum likelihood (IML), which incrementally updates its ML estimate of the coefficients at every sample. Our simulation results verify the analytical conclusions for memoryless inputs, and also show excellent performance of both OBML and IML in an audio echo cancellation application with strongly correlated input signals.",https://arxiv.org/abs/2209.01596
Consistent Teacher Provides Better Supervision in Semi-supervisedObject Detection,"XinjiangWang, XingyiYang, ShilongZhang, Yijiang Li, Litong Feng, Shijie Fang, Chengqi Lyu, Kai Chen, Wayne Zhang",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In this study, we dive deep into the unique challenges in semi- supervised object detection~(SSOD). We observe that current detectors generally suffer from 3 inconsistency problems. 1) Assignment inconsistency, that the conventional assignment policy is sensitive to labeling noise. 2) Subtasks inconsistency, where the classification and regression predictions are misaligned at the same feature point. 3) Temporal inconsistency, that the pseudo bboxes vary dramatically at different training steps. These issues lead to inconsistent optimization objectives of the student network, thus deteriorating performance and slowing down the model convergence. We, therefore, propose a systematic solution, termed Consistent Teacher, to remedy the above-mentioned challenges. First, adaptive anchor assignment substitutes the static IoU-based strategy, which enables the student network to be resistant to noisy psudo bboxes; Then we calibrate the subtask predictions by designing a feature alignment module; Lastly, We adopt a Gaussian Mixture Model (GMM) to dynamically adjust the pseudo-boxes threshold. Consistent Teacher provides a new strong baseline on a large range of SSOD evaluations. It achieves 40.0 mAP with ResNet-50 backbone given only 10% of annotated MS-COCO data, which surpasses previous baselines using pseudo labels by around 4 mAP. When trained on fully annotated MS-COCO with additional unlabeled data, the performance further increases to 49.1 mAP. Our code will be open-sourced soon.",https://arxiv.org/abs/2209.01594
Smoothers Based on Nonoverlapping Domain Decomposition Methods for$H(\mathbf{curl})$ Problems: A Numerical Study,Duk-SoonOh,04-sep-22,Numerical Analysis (math.NA), This paper presents a numerical study on multigrid algorithms of $V$-cycle type for problems posed in the Hilbert space $H(\mathbf{curl})$ in three dimensions. The multigrid methods are designed for discrete problems originated from the discretization using the hexahedral NÃ©dÃ©lec edge element of the lowest-order. Our suggested methods are associated with smoothers constructed by substructuring based on domain decomposition methods of nonoverlapping type. Numerical experiments to demonstrate the robustness and the effectiveness of the suggested algorithms are also provided.,https://arxiv.org/abs/2209.01589
The Approximate Degree of DNF and CNF Formulas,Alexander A.Sherstov,04-sep-22,Computational Complexity (cs.CC)," The approximate degree of a Boolean function $f\colon\\{0,1\\}^n\to\\{0,1\\}$ is the minimum degree of a real polynomial $p$ that approximates $f$ pointwise: $|f(x)-p(x)|\leq1/3$ for all $x\in\\{0,1\\}^n.$ For every $\delta0,$ we construct CNF and DNF formulas of polynomial size with approximate degree $\Omega(n^{1-\delta}),$ essentially matching the trivial upper bound of $n.$ This improves polynomially on previous lower bounds and fully resolves the approximate degree of constant-depth circuits ($\text{AC}^0$), a question that has seen extensive research over the past 10 years. Previously, an $\Omega(n^{1-\delta})$ lower bound was known only for $\text{AC}^0$ circuits of depth that grows with $1/\delta$ (Bun and Thaler, FOCS 2017). Moreover, our CNF and DNF formulas are the simplest possible in that they have constant width. Our result holds even for one-sided approximation, and has the following further consequences.   (i) We essentially settle the communication complexity of $\text{AC}^0$ circuits in the bounded-error quantum model, $k$-party number-on-the- forehead randomized model, and $k$-party number-on-the-forehead nondeterministic model: we prove that for every $\delta0$, these models require $\Omega(n^{1-\delta})$, $\Omega(n/4^kk^2)^{1-\delta}$, and $\Omega(n/4^kk^2)^{1-\delta}$, respectively, bits of communication even for polynomial-size constant-width CNF formulas.   (ii) In particular, we show that the multiparty communication class $\text{coNP}_k$ can be separated essentially optimally from $\text{NP}_k$ and $\text{BPP}_k$ by a particularly simple function, a polynomial-size constant-width CNF.   (iii) We give an essentially tight separation, of $O(1)$ versus $\Omega(n^{1-\delta})$, for the one-sided versus two-sided approximate degree of a function; and $O(1)$ versus $\Omega(n^{1-\delta})$ for the one- sided approximate degree of a function $f$ versus its negation $\neg f$.",https://arxiv.org/abs/2209.01588
PhishClone: Measuring the Efficacy of Cloning Evasion Attacks,"ArthurWong, AlsharifAbuadbba, MahathirAlmashor, SalilKanhere",04-sep-22,Cryptography and Security (cs.CR)," Web-based phishing accounts for over 90% of data breaches, and most web-browsers and security vendors rely on machine-learning (ML) models as mitigation. Despite this, links posted regularly on anti-phishing aggregators such as PhishTank and VirusTotal are shown to easily bypass existing detectors. Prior art suggests that automated website cloning, with light mutations, is gaining traction with attackers. This has limited exposure in current literature and leads to sub-optimal ML-based countermeasures. The work herein conducts the first empirical study that compiles and evaluates a variety of state-of-the-art cloning techniques in wide circulation. We collected 13,394 samples and found 8,566 confirmed phishing pages targeting 4 popular websites using 7 distinct cloning mechanisms. These samples were replicated with malicious code removed within a controlled platform fortified with precautions that prevent accidental access. We then reported our sites to VirusTotal and other platforms, with regular polling of results for 7 days, to ascertain the efficacy of each cloning technique. Results show that no security vendor detected our clones, proving the urgent need for more effective detectors. Finally, we posit 4 recommendations to aid web developers and ML-based defences to alleviate the risks of cloning attacks.",https://arxiv.org/abs/2209.01584
Autonomous Delivery of Multiple Packages Using Single Drone in UrbanAirspace,"SeunghyunLee, BabarShahzaad, BalsamAlkouz, AbdallahLakhdari, AthmanBouguettaya",04-sep-22,Robotics (cs.RO)," Current drone delivery solutions mainly focus on single package delivery using one drone. However, the recent developments in drone technology enable a drone to deliver multiple packages in a single trip. We use the nearest destination first strategy for the faster delivery of packages in a skyway network. This demonstration is a proof-of-concept prototype for the multi-package delivery in urban airspace following a skyway network. We deploy and test this multi-package drone delivery in an indoor testbed environment using a 3D model of Sydney CBD. Demo: [this https URL](https://youtu.be/YTwsIfUvWPc)",https://arxiv.org/abs/2209.01582
Rice Leaf Disease Classification and Detection Using YOLOv5,"Md ErshadulHaque, AshikurRahman, IftekharJunaeid, Samiul UlHoque, ManoranjanPaul",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," A staple food in more than a hundred nations worldwide is rice (Oryza sativa). The cultivation of rice is vital to global economic growth. However, the main issue facing the agricultural industry is rice leaf disease. The quality and quantity of the crops have declined, and this is the main cause. As farmers in any country do not have much knowledge about rice leaf disease, they cannot diagnose rice leaf disease properly. That's why they cannot take proper care of rice leaves. As a result, the production is decreasing. From literature survey, it has seen that YOLOv5 exhibit the better result compare to others deep learning method. As a result of the continual advancement of object detection technology, YOLO family algorithms, which have extraordinarily high precision and better speed have been used in various scene recognition tasks to build rice leaf disease monitoring systems. We have annotate 1500 collected data sets and propose a rice leaf disease classification and detection method based on YOLOv5 deep learning. We then trained and evaluated the YOLOv5 model. The simulation outcomes show improved object detection result for the augmented YOLOv5 network proposed in this article. The required levels of recognition precision, recall, mAP value, and F1 score are 90\%, 67\%, 76\%, and 81\% respectively are considered as performance metrics.",https://arxiv.org/abs/2209.01580
Spatial-Temporal Transformer for Video Snapshot Compressive Imaging,"LishunWang, MiaoCao, YongZhong, XinYuan","4 Sep 2022 (v1(https://arxiv.org/abs/2209.01578v1)), lastrevised 8 Sep 2022 (this version, v2)",Image and Video Processing (eess.IV)," Video snapshot compressive imaging (SCI) captures multiple sequential video frames by a single measurement using the idea of computational imaging. The underlying principle is to modulate high-speed frames through different masks and these modulated frames are summed to a single measurement captured by a low-speed 2D sensor (dubbed optical encoder); following this, algorithms are employed to reconstruct the desired high-speed frames (dubbed software decoder) if needed. In this paper, we consider the reconstruction algorithm in video SCI, i.e., recovering a series of video frames from a compressed measurement. Specifically, we propose a Spatial-Temporal transFormer (STFormer) to exploit the correlation in both spatial and temporal domains. STFormer network is composed of a token generation block, a video reconstruction block, and these two blocks are connected by a series of STFormer blocks. Each STFormer block consists of a spatial self-attention branch, a temporal self-attention branch and the outputs of these two branches are integrated by a fusion network. Extensive results on both simulated and real data demonstrate the state-of-the-art performance of STFormer. The code and models are publicly available at [this https URL](https://github.com/ucaswangls/STFormer.git)",https://arxiv.org/abs/2209.01579
Structure and approximation properties of Laplacian-like matrices,"J.A.Conejero, A.FalcÃ³, M. Mora-JimÃ©nez",04-sep-22,Numerical Analysis (math.NA)," Many of today's problems require techniques that involve the solution of arbitrarily large systems $A\mathbf{x}=\mathbf{b}$. A popular numerical approach is the so-called Greedy Rank-One Update Algorithm, based on a particular tensor decomposition. The numerical experiments support the fact that this algorithm converges especially fast when the matrix of the linear system is Laplacian-Like. These matrices that follow the tensor structure of the Laplacian operator are formed by sums of Kronecker product of matrices following a particular pattern. Moreover, this set of matrices is not only a linear subspace it is a a Lie sub-algebra of a matrix Lie Algebra. In this paper, we characterize and give the main properties of this particular class of matrices. Moreover, the above results allow us to propose an algorithm to explicitly compute the orthogonal projection onto this subspace of a given square matrix $A \in \mathbb{R}^{N\times N}.$",https://arxiv.org/abs/2209.01578
On the Usability (In)Security of In-App Browsing Interfaces in MobileApps,"ZichengZhang, Daoyuan Wu, Lixiang Li, Debin Gao",04-sep-22,Cryptography and Security (cs.CR)," Due to the frequent encountering of web URLs in various application scenarios (e.g., chatting and email reading), many mobile apps build their in-app browsing interfaces (IABIs) to provide a seamless user experience. Although this achieves user-friendliness by avoiding the constant switching between the subject app and the system built-in browser apps, we find that IABIs, if not well designed or customized, could result in usability security risks. In this paper, we conduct the first empirical study on the usability (in)security of in-app browsing interfaces in both Android and iOS apps. Specifically, we collect a dataset of 25 high-profile mobile apps from five common application categories that contain IABIs, including Facebook and Gmail, and perform a systematic analysis (not end- user study though) that comprises eight carefully designed security tests and covers the entire course of opening, displaying, and navigating an in- app web page. During this process, we obtain three major security findings: (1) about 30% of the tested apps fail to provide enough URL information for users to make informed decisions on opening an URL; (2) nearly all custom IABIs have various problems in providing sufficient indicators to faithfully display an in-app page to users, whereas ten IABIs that are based on Chrome Custom Tabs and SFSafariViewController are generally secure; and (3) only a few IABIs give warnings to remind users of the risk of inputting passwords during navigating a (potentially phishing) login page. Most developers had acknowledged our findings but their willingness and readiness to fix usability issues are rather low compared to fixing technical vulnerabilities, which is a puzzle in usability security research. Nevertheless, to help mitigate risky IABIs and guide future designs, we propose a set of secure IABI design principles.",https://arxiv.org/abs/2209.01569
None,None,None,None,None,https://arxiv.org/abs/2209.01568
Pseudo-LiDAR for Visual Odometry,"HuiyingDeng, GuangmingWang, ZhihengFeng, ChaokangJiang, Xinrui Wu, Yanzi Miao, Hesheng Wang",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In the existing methods, LiDAR odometry shows superior performance, but visual odometry is still widely used for its price advantage. Conventionally, the task of visual odometry mainly rely on the input of continuous images. However, it is very complicated for the odometry network to learn the epipolar geometry information provided by the images. In this paper, the concept of pseudo-LiDAR is introduced into the odometry to solve this problem. The pseudo-LiDAR point cloud back-projects the depth map generated by the image into the 3D point cloud, which changes the way of image representation. Compared with the stereo images, the pseudo-LiDAR point cloud generated by the stereo matching network can get the explicit 3D coordinates. Since the 6 Degrees of Freedom (DoF) pose transformation occurs in 3D space, the 3D structure information provided by the pseudo-LiDAR point cloud is more direct than the image. Compared with sparse LiDAR, the pseudo- LiDAR has a denser point cloud. In order to make full use of the rich point cloud information provided by the pseudo-LiDAR, a projection-aware dense odometry pipeline is adopted. Most previous LiDAR-based algorithms sampled 8192 points from the point cloud as input to the odometry network. The projection-aware dense odometry pipeline takes all the pseudo-LiDAR point clouds generated from the images except for the error points as the input to the network. While making full use of the 3D geometric information in the images, the semantic information in the images is also used in the odometry task. The fusion of 2D-3D is achieved in an image-only based odometry. Experiments on the KITTI dataset prove the effectiveness of our method. To the best of our knowledge, this is the first visual odometry method using pseudo-LiDAR.",https://arxiv.org/abs/10.1145
Towards Top-Down Deep Code Generation in Limited Scopes,"JianGu, Harald C.Gall",04-sep-22,Software Engineering (cs.SE)," Deep code generation is a topic of deep learning for software engineering (DL4SE), which adopts neural models to generate code for the intended functions. Since end-to-end neural methods lack the awareness of domain knowledge and software hierarchy, the results often require manual correction. To systematically explore the potential improvements of code generation, we let it participate in the whole top-down development from intentions to realizations, which is possible in limited scopes. In the process, it benefits from massive samples, features, and knowledge. As the foundation, we suggest building a taxonomy on code data, namely code taxonomy, leveraging the categorization of code information. Moreover, we introduce a three-layer semantic pyramid (SP) to associate text data and code data. It identifies the information of different abstraction levels, and thus introduces the domain knowledge on development and reveals the hierarchy of software. Furthermore, we propose a semantic pyramid framework (SPF) as the approach, focusing on softwares of high modularity and low complexity. SPF divides the code generation process into stages and reserves spots for potential interactions. Eventually, we conceived application scopes for SPF.",https://arxiv.org/abs/2209.01567
Selective Text Augmentation with Word Roles for Low-Resource TextClassification,"BiyangGuo, SongqiaoHan, HailiangHuang",04-sep-22,Computation and Language (cs.CL)," Data augmentation techniques are widely used in text classification tasks to improve the performance of classifiers, especially in low-resource scenarios. Most previous methods conduct text augmentation without considering the different functionalities of the words in the text, which may generate unsatisfactory samples. Different words may play different roles in text classification, which inspires us to strategically select the proper roles for text augmentation. In this work, we first identify the relationships between the words in a text and the text category from the perspectives of statistical correlation and semantic similarity and then utilize them to divide the words into four roles -- Gold, Venture, Bonus, and Trivial words, which have different functionalities for text classification. Based on these word roles, we present a new augmentation technique called STA (Selective Text Augmentation) where different text- editing operations are selectively applied to words with specific roles. STA can generate diverse and relatively clean samples, while preserving the original core semantics, and is also quite simple to implement. Extensive experiments on 5 benchmark low-resource text classification datasets illustrate that augmented samples produced by STA successfully boost the performance of classification models which significantly outperforms previous non-selective methods, including two large language model-based techniques. Cross-dataset experiments further indicate that STA can help the classifiers generalize better to other datasets than previous methods.",https://arxiv.org/abs/2209.01566
Hierarchical Transformer with Spatio-Temporal Context Aggregation forNext Point-of-Interest Recommendation,"JiayiXie, ZhenzhongChen",04-sep-22,Information Retrieval (cs.IR)," Next point-of-interest (POI) recommendation is a critical task in location-based social networks, yet remains challenging due to a high degree of variation and personalization exhibited in user movements. In this work, we explore the latent hierarchical structure composed of multi-granularity short-term structural patterns in user check-in sequences. We propose a Spatio-Temporal context AggRegated Hierarchical Transformer (STAR-HiT) for next POI recommendation, which employs stacked hierarchical encoders to recursively encode the spatio-temporal context and explicitly locate subsequences of different granularities. More specifically, in each encoder, the global attention layer captures the spatio-temporal context of the sequence, while the local attention layer performed within each subsequence enhances subsequence modeling using the local context. The sequence partition layer infers positions and lengths of subsequences from the global context adaptively, such that semantics in subsequences can be well preserved. Finally, the subsequence aggregation layer fuses representations within each subsequence to form the corresponding subsequence representation, thereby generating a new sequence of higher-level granularity. The stacking of encoders captures the latent hierarchical structure of the check-in sequence, which is used to predict the next visiting POI. Extensive experiments on three public datasets demonstrate that the proposed model achieves superior performance whilst providing explanations for recommendations. Codes are available at [this https URL](https://github.com/JennyXieJiayi/STAR-HiT).",https://arxiv.org/abs/2209.01560
Scalable Adversarial Online Continual Learning,"TanmoyDam, MahardhikaPratama, MD MeftahulFerdaus, SreenathaAnavatti, HusseinAbbas",04-sep-22,Machine Learning (cs.LG)," Adversarial continual learning is effective for continual learning problems because of the presence of feature alignment process generating task-invariant features having low susceptibility to the catastrophic forgetting problem. Nevertheless, the ACL method imposes considerable complexities because it relies on task-specific networks and discriminators. It also goes through an iterative training process which does not fit for online (one-epoch) continual learning problems. This paper proposes a scalable adversarial continual learning (SCALE) method putting forward a parameter generator transforming common features into task-specific features and a single discriminator in the adversarial game to induce common features. The training process is carried out in meta-learning fashions using a new combination of three loss functions. SCALE outperforms prominent baselines with noticeable margins in both accuracy and execution time.",https://arxiv.org/abs/2209.01559
Reinforced Continual Learning for Graphs,"AppanRakaraddi, Siew KeiLam, MahardhikaPratama, Marcus DeCarvalho",04-sep-22,Machine Learning (cs.LG)," Graph Neural Networks (GNNs) have become the backbone for a myriad of tasks pertaining to graphs and similar topological data structures. While many works have been established in domains related to node and graph classification/regression tasks, they mostly deal with a single task. Continual learning on graphs is largely unexplored and existing graph continual learning approaches are limited to the task-incremental learning scenarios. This paper proposes a graph continual learning strategy that combines the architecture-based and memory-based approaches. The structural learning strategy is driven by reinforcement learning, where a controller network is trained in such a way to determine an optimal number of nodes to be added/pruned from the base network when new tasks are observed, thus assuring sufficient network capacities. The parameter learning strategy is underpinned by the concept of Dark Experience replay method to cope with the catastrophic forgetting problem. Our approach is numerically validated with several graph continual learning benchmark problems in both task-incremental learning and class-incremental learning settings. Compared to recently published works, our approach demonstrates improved performance in both the settings. The implementation code can be found at \url{[this https URL](https://github.com/codexhammer/gcl)}.",https://arxiv.org/abs/2209.01558
Latent Preserving Generative Adversarial Network for Imbalanceclassification,"TanmoyDam, MdMeftahulFerdaus, MahardhikaPratama, Sreenatha G.Anavatti, SenthilnathJayavelu, Hussein A.Abbass",04-sep-22,Machine Learning (cs.LG)," Many real-world classification problems have imbalanced frequency of class labels; a well-known issue known as the ""class imbalance"" problem. Classic classification algorithms tend to be biased towards the majority class, leaving the classifier vulnerable to misclassification of the minority class. While the literature is rich with methods to fix this problem, as the dimensionality of the problem increases, many of these methods do not scale-up and the cost of running them become prohibitive. In this paper, we present an end-to-end deep generative classifier. We propose a domain-constraint autoencoder to preserve the latent-space as prior for a generator, which is then used to play an adversarial game with two other deep networks, a discriminator and a classifier. Extensive experiments are carried out on three different multi-class imbalanced problems and a comparison with state-of-the-art methods. Experimental results confirmed the superiority of our method over popular algorithms in handling high- dimensional imbalanced classification problems. Our code is available on [this https URL](https://github.com/TanmDL/SLPPL-GAN).",https://arxiv.org/abs/2209.01556
Learning to Deceive in Multi-Agent Hidden Role Games,"MatthewAitchison, LyndonBenke, PennySweetser",04-sep-22,Multiagent Systems (cs.MA)," Deception is prevalent in human social settings. However, studies into the effect of deception on reinforcement learning algorithms have been limited to simplistic settings, restricting their applicability to complex real-world problems. This paper addresses this by introducing a new mixed competitive-cooperative multi-agent reinforcement learning (MARL) environment inspired by popular role-based deception games such as Werewolf, Avalon, and Among Us. The environment's unique challenge lies in the necessity to cooperate with other agents despite not knowing if they are friend or foe. Furthermore, we introduce a model of deception, which we call Bayesian belief manipulation (BBM) and demonstrate its effectiveness at deceiving other agents in this environment while also increasing the deceiving agent's performance.",https://arxiv.org/abs/2209.01555
The Broken Windows Theory Applies to Technical Debt,"WilliamLevÃ©n, HampusBroman, TereseBesker, RichardTorkar",04-sep-22,Software Engineering (cs.SE)," Context: The term technical debt (TD) describes the aggregation of sub-optimal solutions that serve to impede the evolution and maintenance of a system. Some claim that the broken windows theory (BWT), a concept borrowed from criminology, also applies to software development projects. The theory states that the presence of indications of previous crime (such as a broken window) will increase the likelihood of further criminal activity; TD could be considered the broken windows of software systems. Objective: To empirically investigate the causal relationship between the TD density of a system and the propensity of developers to introduce new TD during the extension of that system. Method: The study used a mixed-methods research strategy consisting of a controlled experiment with an accompanying survey and follow-up interviews. The experiment had a total of 29 developers of varying experience levels completing a system extension tasks in an already existing systems with high or low TD density. The solutions were scanned for TD. Six subjects participated in follow-up interviews, where the results were analyzed using thematic analysis. Result: The analysis revealed significant effects of TD level on the subjects' tendency to re-implement (rather than reuse) functionality, choose non-descriptive variable names, and introduce other code smells, all with at least 95% credible intervals. Additionally, the developers appeared to be, at least partially, aware of when they had introduced TD. Conclusion: Three separate significant results along with a validating qualitative result combine to form substantial evidence of the BWT's applicability to software engineering contexts. Existing TD has a major impact on developers propensity to introduce new TD of various types during development. While mimicry seems to be part of the explanation it can not alone describe the observed effects.",https://arxiv.org/abs/2209.01551
Autonomous Cross Domain Adaptation under Extreme Label Scarcity,"WeiweiWeng, MahardhikaPratama, ChoiruZa'in, Marcus DeCarvalho, RakaraddiAppan, AndriAshfahani, Edward Yapp KienYee",04-sep-22,Machine Learning (cs.LG)," A cross domain multistream classification is a challenging problem calling for fast domain adaptations to handle different but related streams in never-ending and rapidly changing environments. Notwithstanding that existing multistream classifiers assume no labelled samples in the target stream, they still incur expensive labelling cost since they require fully labelled samples of the source stream. This paper aims to attack the problem of extreme label shortage in the cross domain multistream classification problems where only very few labelled samples of the source stream are provided before process runs. Our solution, namely Learning Streaming Process from Partial Ground Truth (LEOPARD), is built upon a flexible deep clustering network where its hidden nodes, layers and clusters are added and removed dynamically in respect to varying data distributions. A deep clustering strategy is underpinned by a simultaneous feature learning and clustering technique leading to clustering-friendly latent spaces. A domain adaptation strategy relies on the adversarial domain adaptation technique where a feature extractor is trained to fool a domain classifier classifying source and target streams. Our numerical study demonstrates the efficacy of LEOPARD where it delivers improved performances compared to prominent algorithms in 15 of 24 cases. Source codes of LEOPARD are shared in \url{[this https URL](https://github.com/wengweng001/LEOPARD.git)} to enable further study.",https://arxiv.org/abs/2209.01549
Conditional Independence Testing via Latent Representation Learning,"BaoDuong, ThinNguyen",04-sep-22,Machine Learning (cs.LG)," Detecting conditional independencies plays a key role in several statistical and machine learning tasks, especially in causal discovery algorithms. In this study, we introduce LCIT (Latent representation based Conditional Independence Test)-a novel non-parametric method for conditional independence testing based on representation learning. Our main contribution involves proposing a generative framework in which to test for the independence between X and Y given Z, we first learn to infer the latent representations of target variables X and Y that contain no information about the conditioning variable Z. The latent variables are then investigated for any significant remaining dependencies, which can be performed using the conventional partial correlation test. The empirical evaluations show that LCIT outperforms several state-of-the-art baselines consistently under different evaluation metrics, and is able to adapt really well to both non-linear and high-dimensional settings on a diverse collection of synthetic and real data sets.",https://arxiv.org/abs/2209.01548
Scenario-Based Test Reduction and Prioritization for Multi-ModuleAutonomous Driving Systems,"YaoDeng, XiZheng, MengshiZhang, Guannan Lou, Tianyi Zhang",04-sep-22,Software Engineering (cs.SE)," When developing autonomous driving systems (ADS), developers often need to replay previously collected driving recordings to check the correctness of newly introduced changes to the system. However, simply replaying the entire recording is not necessary given the high redundancy of driving scenes in a recording (e.g., keeping the same lane for 10 minutes on a highway). In this paper, we propose a novel test reduction and prioritization approach for multi-module ADS. First, our approach automatically encodes frames in a driving recording to feature vectors based on a driving scene schema. Then, the given recording is sliced into segments based on the similarity of consecutive vectors. Lengthy segments are truncated to reduce the length of a recording and redundant segments with the same vector are removed. The remaining segments are prioritized based on both the coverage and the rarity of driving scenes. We implemented this approach on an industry level, multi-module ADS called Apollo and evaluated it on three road maps in various regression settings. The results show that our approach significantly reduced the original recordings by over 34% while keeping comparable test effectiveness, identifying almost all injected faults. Furthermore, our test prioritization method achieves about 22% to 39% and 41% to 53% improvements over three baselines in terms of both the average percentage of faults detected (APFD) and TOP-K.",https://arxiv.org/abs/2209.01547
Algorithm for Finding an Exact Maximum Distance in E2 with Oexp(N)Complexity: Analysis and Experimental Results,VaclavSkala,04-sep-22,Computational Geometry (cs.CG)," This paper describes a novel and fast, simple and robust algorithm with O(N) expected complexity which enables to decrease run time needed to find the maximum distance of two points in E2. It can be easily modified for the E3 case in general. The proposed algorithm has been evaluated experimentally on larger different datasets in order to verify it and prove expected properties of it. Experiments proved the advantages of the proposed algorithm over the standard algorithms based on the Brute force, convex hull or convex hull diameters approaches. The proposed algorithm gives a significant speed-up to applications, when medium and large data sets are processed. It is over 10 000 times faster than the standard Brute force algorithm for 10 mil. points randomly distributed points in E2 and over 4 times faster than convex hull diameter computation. The speed-up of the proposed algorithm grows with the number of points processed.",https://arxiv.org/abs/2209.01546
Fast O_{expected}(N) Algorithm for Finding Exact Maximum Distance inE^2 Instead of O(N^2) or O(N lg N),VaclavSkala,7 Aug 2022,Computational Geometry (cs.CG)," This paper describes novel and fast, simple and robust algorithm with O(N) expected complexity which enables to decrease run-time needed to find an exact maximum distance of two points in E2. The proposed algorithm has been evaluated experimentally on larger different datasets. The proposed algorithm gives a significant speed-up to applications, when medium and large data sets are processed. It is over 10 000 times faster than the standard algorithm for 10^6 points randomly distributed points in E2. Experiments proved the advantages of the proposed algorithm over the standard algorithm and convex hull diameters approaches.",https://arxiv.org/abs/2209.01543
Recurrent Bilinear Optimization for Binary Neural Networks,"ShengXu, YanjingLi, TianchengWang, TeliMa, BaochangZhang, PengGao, YuQiao, JinhuLv, GuodongGuo",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Binary Neural Networks (BNNs) show great promise for real-world embedded devices. As one of the critical steps to achieve a powerful BNN, the scale factor calculation plays an essential role in reducing the performance gap to their real-valued counterparts. However, existing BNNs neglect the intrinsic bilinear relationship of real-valued weights and scale factors, resulting in a sub-optimal model caused by an insufficient training process. To address this issue, Recurrent Bilinear Optimization is proposed to improve the learning process of BNNs (RBONNs) by associating the intrinsic bilinear variables in the back propagation process. Our work is the first attempt to optimize BNNs from the bilinear perspective. Specifically, we employ a recurrent optimization and Density-ReLU to sequentially backtrack the sparse real-valued weight filters, which will be sufficiently trained and reach their performance limits based on a controllable learning process. We obtain robust RBONNs, which show impressive performance over state-of-the-art BNNs on various models and datasets. Particularly, on the task of object detection, RBONNs have great generalization performance. Our code is open-sourced on [this https URL](https://github.com/SteveTsui/RBONN) .",https://arxiv.org/abs/2208.04730
InviCloak: An End-to-End Approach to Privacy and Performance in WebContent Distribution,"ShihanLin, RuiXin, AayushGoel, XiaoweiYang","4 Sep 2022 (v1(https://arxiv.org/abs/2209.01541v1)), lastrevised 7 Sep 2022 (this version, v2)",Cryptography and Security (cs.CR)," In today's web ecosystem, a website that uses a Content Delivery Network (CDN) shares its Transport Layer Security (TLS) private key or session key with the CDN. In this paper, we present the design and implementation of InviCloak, a system that protects the confidentiality and integrity of a user and a website's private communications without changing TLS or upgrading a CDN. InviCloak builds a lightweight but secure and practical key distribution mechanism using the existing DNS infrastructure to distribute a new public key associated with a website's domain name. A web client and a website can use the new key pair to build an encryption channel inside TLS. InviCloak accommodates the current web ecosystem. A website can deploy InviCloak unilaterally without a client's involvement to prevent a passive attacker inside a CDN from eavesdropping on their communications. If a client also installs InviCloak's browser extension, the client and the website can achieve end-to-end confidential and untampered communications in the presence of an active attacker inside a CDN. Our evaluation shows that InviCloak increases the median page load times (PLTs) of realistic web pages from 2.0s to 2.1s, which is smaller than the median PLTs (2.7s) of a state-of-the-art TEE-based solution.",https://arxiv.org/abs/2209.01542
An Empirical Study of End-to-End Video-Language Transformers withMasked Visual Modeling,"Tsu-JuiFu, LinjieLi, ZheGan, KevinLin, WilliamYang Wang, Lijuan Wang, Zicheng Liu",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Masked visual modeling (MVM) has been recently proven effective for visual pre-training. While similar reconstructive objectives on video inputs (e.g., masked frame modeling) have been explored in video-language (VidL) pre-training, the pre-extracted video features in previous studies cannot be refined through MVM during pre-training, and thus leading to unsatisfactory downstream performance. In this work, we systematically examine the potential of MVM in the context of VidL learning. Specifically, we base our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), which mitigates the disconnection between fixed video representations and MVM training. In total, eight different reconstructive targets of MVM are explored, from low-level pixel values and oriented gradients to high-level depth maps, optical flow, discrete visual tokens and latent visual features. We conduct comprehensive experiments and provide insights on the factors leading to effective MVM training. Empirically, we show VIOLET pre-trained with MVM objective achieves notable improvements on 13 VidL benchmarks, ranging from video question answering, video captioning, to text-to-video retrieval.",https://arxiv.org/abs/2209.01541
VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling,"Tsu-JuiFu, LinjieLi, ZheGan, KevinLin, WilliamYang Wang, Lijuan Wang, Zicheng Liu","24 Nov 2021 (v1(https://arxiv.org/abs/2111.12681v1)), lastrevised 16 Apr 2022 (this version, v2)",Computer Vision and Pattern Recognition (cs.CV)," A great challenge in video-language (VidL) modeling lies in the disconnection between fixed video representations extracted from image/video understanding models and downstream VidL data. Recent studies try to mitigate this disconnection via end-to-end training. To make it computationally feasible, prior works tend to ""imagify"" video inputs, i.e., a handful of sparsely sampled frames are fed into a 2D CNN, followed by a simple mean-pooling or concatenation to obtain the overall video representations. Although achieving promising results, such simple approaches may lose temporal information that is essential for performing downstream VidL tasks. In this work, we present VIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video transformer to explicitly model the temporal dynamics of video inputs. Further, unlike previous studies that found pre-training tasks on video inputs (e.g., masked frame modeling) not very effective, we design a new pre-training task, Masked Visual-token Modeling (MVM), for better video modeling. Specifically, the original video frame patches are ""tokenized"" into discrete visual tokens, and the goal is to recover the original visual tokens based on the masked patches. Comprehensive analysis demonstrates the effectiveness of both explicit temporal modeling via video transformer and MVM. As a result, VIOLET achieves new state-of-the-art performance on 5 video question answering tasks and 4 text-to-video retrieval tasks.",https://arxiv.org/abs/2209.01540
Cross-Network Social User Embedding with Hybrid Differential PrivacyGuarantees,"JiaqianRen, LeiJiang, HaoPeng, Lingjuan Lyu, Zhiwei Liu, ChaochaoChen, JiaWu, XuBai, Philip S.Yu",04-sep-22,Social and Information Networks (cs.SI)," Integrating multiple online social networks (OSNs) has important implications for many downstream social mining tasks, such as user preference modelling, recommendation, and link prediction. However, it is unfortunately accompanied by growing privacy concerns about leaking sensitive user information. How to fully utilize the data from different online social networks while preserving user privacy remains largely unsolved. To this end, we propose a Cross-network Social User Embedding framework, namely DP-CroSUE, to learn the comprehensive representations of users in a privacy-preserving way. We jointly consider information from partially aligned social networks with differential privacy guarantees. In particular, for each heterogeneous social network, we first introduce a hybrid differential privacy notion to capture the variation of privacy expectations for heterogeneous data types. Next, to find user linkages across social networks, we make unsupervised user embedding-based alignment in which the user embeddings are achieved by the heterogeneous network embedding technology. To further enhance user embeddings, a novel cross- network GCN embedding model is designed to transfer knowledge across networks through those aligned users. Extensive experiments on three real- world datasets demonstrate that our approach makes a significant improvement on user interest prediction tasks as well as defending user attribute inference attacks from embedding.",https://arxiv.org/abs/2111.12681
Data Provenance via Differential Auditing,"Xin Mu, Ming Pang, Feida Zhu",04-sep-22,Cryptography and Security (cs.CR)," Auditing Data Provenance (ADP), i.e., auditing if a certain piece of data has been used to train a machine learning model, is an important problem in data provenance. The feasibility of the task has been demonstrated by existing auditing techniques, e.g., shadow auditing methods, under certain conditions such as the availability of label information and the knowledge of training protocols for the target model. Unfortunately, both of these conditions are often unavailable in real applications. In this paper, we introduce Data Provenance via Differential Auditing (DPDA), a practical framework for auditing data provenance with a different approach based on statistically significant differentials, i.e., after carefully designed transformation, perturbed input data from the target model's training set would result in much more drastic changes in the output than those from the model's non-training set. This framework allows auditors to distinguish training data from non-training ones without the need of training any shadow models with the help of labeled output data. Furthermore, we propose two effective auditing function implementations, an additive one and a multiplicative one. We report evaluations on real-world data sets demonstrating the effectiveness of our proposed auditing technique.",https://arxiv.org/abs/2209.01539
Interpretable Fake News Detection with Topic and Deep VariationalModels,"MarjanHosseini, Alireza JavadianSabet, Suining He, DerekAguiar",04-sep-22,Computation and Language (cs.CL)," The growing societal dependence on social media and user generated content for news and information has increased the influence of unreliable sources and fake content, which muddles public discourse and lessens trust in the media. Validating the credibility of such information is a difficult task that is susceptible to confirmation bias, leading to the development of algorithmic techniques to distinguish between fake and real news. However, most existing methods are challenging to interpret, making it difficult to establish trust in predictions, and make assumptions that are unrealistic in many real-world scenarios, e.g., the availability of audiovisual features or provenance. In this work, we focus on fake news detection of textual content using interpretable features and methods. In particular, we have developed a deep probabilistic model that integrates a dense representation of textual news using a variational autoencoder and bi-directional Long Short-Term Memory (LSTM) networks with semantic topic-related features inferred from a Bayesian admixture model. Extensive experimental studies with 3 real-world datasets demonstrate that our model achieves comparable performance to state-of-the-art competing models while facilitating model interpretability from the learned topics. Finally, we have conducted model ablation studies to justify the effectiveness and accuracy of integrating neural embeddings and topic features both quantitatively by evaluating performance and qualitatively through separability in lower dimensional embeddings.",https://arxiv.org/abs/2209.01538
A Scalable Data-Driven Technique for Joint Evacuation Routing andScheduling Problems,"Kazi AshikIslam, DaQi Chen, MadhavMarathe, HenningMortveit, SamarthSwarup, AnilVullikanti",04-sep-22,Artificial Intelligence (cs.AI)," Evacuation planning is a crucial part of disaster management where the goal is to relocate people to safety and minimize casualties. Every evacuation plan has two essential components: routing and scheduling. However, joint optimization of these two components with objectives such as minimizing average evacuation time or evacuation completion time, is a computationally hard problem. To approach it, we present MIP-LNS, a scalable optimization method that combines heuristic search with mathematical optimization and can optimize a variety of objective functions. We use real- world road network and population data from Harris County in Houston, Texas, and apply MIP-LNS to find evacuation routes and schedule for the area. We show that, within a given time limit, our proposed method finds better solutions than existing methods in terms of average evacuation time, evacuation completion time and optimality guarantee of the solutions. We perform agent-based simulations of evacuation in our study area to demonstrate the efficacy and robustness of our solution. We show that our prescribed evacuation plan remains effective even if the evacuees deviate from the suggested schedule upto a certain extent. We also examine how evacuation plans are affected by road failures. Our results show that MIP- LNS can use information regarding estimated deadline of roads to come up with better evacuation plans in terms evacuating more people successfully and conveniently.",https://arxiv.org/abs/2209.01536
Multi-modal Masked Autoencoders Learn Compositional HistopathologicalRepresentations,"Wisdom OluchiIkezogwo, Mehmet SayginSeyfioglu, LindaShapiro",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Self-supervised learning (SSL) enables learning useful inductive biases through utilizing pretext tasks that require no labels. The unlabeled nature of SSL makes it especially important for whole slide histopathological images (WSIs), where patch-level human annotation is difficult. Masked Autoencoders (MAE) is a recent SSL method suitable for digital pathology as it does not require negative sampling and requires little to no data augmentations. However, the domain shift between natural images and digital pathology images requires further research in designing MAE for patch-level WSIs. In this paper, we investigate several design choices for MAE in histopathology. Furthermore, we introduce a multi-modal MAE (MMAE) that leverages the specific compositionality of Hematoxylin & Eosin (H&E) stained WSIs. We performed our experiments on the public patch- level dataset NCT-CRC-HE-100K. The results show that the MMAE architecture outperforms supervised baselines and other state-of-the-art SSL techniques for an eight-class tissue phenotyping task, utilizing only 100 labeled samples for fine-tuning. Our code is available at [this https URL](https://github.com/wisdomikezogwo/MMAE_Pathology)",https://arxiv.org/abs/2209.01535
Informative Language Representation Learning for MassivelyMultilingual Neural Machine Translation,"RenrenJin, DeyiXiong",04-sep-22,Computation and Language (cs.CL)," In a multilingual neural machine translation model that fully shares parameters across all languages, an artificial language token is usually used to guide translation into the desired target language. However, recent studies show that prepending language tokens sometimes fails to navigate the multilingual neural machine translation models into right translation directions, especially on zero-shot translation. To mitigate this issue, we propose two methods, language embedding embodiment and language-aware multi-head attention, to learn informative language representations to channel translation into right directions. The former embodies language embeddings into different critical switching points along the information flow from the source to the target, aiming at amplifying translation direction guiding signals. The latter exploits a matrix, instead of a vector, to represent a language in the continuous space. The matrix is chunked into multiple heads so as to learn language representations in multiple subspaces. Experiment results on two datasets for massively multilingual neural machine translation demonstrate that language-aware multi-head attention benefits both supervised and zero-shot translation and significantly alleviates the off-target translation issue. Further linguistic typology prediction experiments show that matrix-based language representations learned by our methods are capable of capturing rich linguistic typology features.",https://arxiv.org/abs/2209.01534
Hybrid mixed discontinuous Galerkin finite element method forincompressible wormhole propagation problem,"JiansongZhang, YunYu, JiangZhu, YueYu, RongQin",04-sep-22,Numerical Analysis (math.NA)," Wormhole propagation plays a very important role in the product enhancement of oil and gas reservoir. A new combined hybrid mixed finite element method is proposed to solve incompressible wormhole propagation problem with discontinuous Galerkin finite element procedure, in which, the new hybrid mixed finite element algorithm is established for pressure equation, while the discontinuous Galerkin finite element method is considered for concentration equation, and then the porosity function is computed straightly by the approximate value of the concentration. This new combined method can keep local mass balance, meantime it also keeps the boundedness of the porosity. The convergence of the proposed method is analyzed and the optimal error estimate is derived. Finally, numerical examples are presented to verify the validity of the algorithm and the correctness of the theoretical results.",https://arxiv.org/abs/2209.01530
Data-Driven Deep Supervision for Skin Lesion Classification,"SurajMishra, Yizhe Zhang, Li Zhang, TianyuZhang, X.Sharon Hu, Danny Z.Chen",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Automatic classification of pigmented, non-pigmented, and depigmented non-melanocytic skin lesions have garnered lots of attention in recent years. However, imaging variations in skin texture, lesion shape, depigmentation contrast, lighting condition, etc. hinder robust feature extraction, affecting classification accuracy. In this paper, we propose a new deep neural network that exploits input data for robust feature extraction. Specifically, we analyze the convolutional network's behavior (field-of-view) to find the location of deep supervision for improved feature extraction. To achieve this, first, we perform activation mapping to generate an object mask, highlighting the input regions most critical for classification output generation. Then the network layer whose layer-wise effective receptive field matches the approximated object shape in the object mask is selected as our focus for deep supervision. Utilizing different types of convolutional feature extractors and classifiers on three melanoma detection datasets and two vitiligo detection datasets, we verify the effectiveness of our new method.",https://arxiv.org/abs/2209.01528
Hybrid mixed discontinuous Galerkin finite element method forincompressible miscible displacement problem,"JiansongZhang, YunYu, JiangZhu, RongQin, YueYu, MaoshengJiang",04-sep-22,Numerical Analysis (math.NA)," A new hybrid mixed discontinuous Galerkin finite element (HMDGFE) method is constructed for incompressible miscible displacement problem. In this method, the hybrid mixed finite element (HMFE) procedure is considered to solve pressure and velocity equations, and a new hybrid mixed discontinuous Galerkin procedure is constructed to solve the concentration equation with upwind technique. Compared with other traditional discontinuous Galerkin methods, the new method can reach global systems with less unknowns and sparser stencils. The consistency and conservation of the method are analyzed, the stability and optimal error estimates are also derived by the new technique.",https://arxiv.org/abs/2209.01527
Disentangled Graph Contrastive Learning for Review-basedRecommendation,"YuyangRen, HaonanZhang, QiLi, LuoyiFu, JiaxinDing, XindeCao, XinbingWang, ChenghuZhou",04-sep-22,Information Retrieval (cs.IR)," User review data is helpful in alleviating the data sparsity problem in many recommender systems. In review-based recommendation methods, review data is considered as auxiliary information that can improve the quality of learned user/item or interaction representations for the user rating prediction task. However, these methods usually model user-item interactions in a holistic manner and neglect the entanglement of the latent factors behind them, e.g., price, quality, or appearance, resulting in suboptimal representations and reducing interpretability. In this paper, we propose a Disentangled Graph Contrastive Learning framework for Review-based recommendation (DGCLR), to separately model the user-item interactions based on different latent factors through the textual review data. To this end, we first model the distributions of interactions over latent factors from both semantic information in review data and structural information in user-item graph data, forming several factor graphs. Then a factorized message passing mechanism is designed to learn disentangled user/item representations on the factor graphs, which enable us to further characterize the interactions and adaptively combine the predicted ratings from multiple factors via a devised attention mechanism. Finally, we set two factor-wise contrastive learning objectives to alleviate the sparsity issue and model the user/item and interaction features pertinent to each factor more accurately. Empirical results over five benchmark datasets validate the superiority of DGCLR over the state-of-the-art methods. Further analysis is offered to interpret the learned intent factors and rating prediction in DGCLR.",https://arxiv.org/abs/2209.01526
Symplectically Integrated Symbolic Regression of Hamiltonian DynamicalSystems,"Daniel M.DiPietro, Bo Zhu",04-sep-22,Machine Learning (cs.LG)," Here we present Symplectically Integrated Symbolic Regression (SISR), a novel technique for learning physical governing equations from data. SISR employs a deep symbolic regression approach, using a multi-layer LSTM-RNN with mutation to probabilistically sample Hamiltonian symbolic expressions. Using symplectic neural networks, we develop a model-agnostic approach for extracting meaningful physical priors from the data that can be imposed on-the-fly into the RNN output, limiting its search space. Hamiltonians generated by the RNN are optimized and assessed using a fourth- order symplectic integration scheme; prediction performance is used to train the LSTM-RNN to generate increasingly better functions via a risk-seeking policy gradients approach. Employing these techniques, we extract correct governing equations from oscillator, pendulum, two-body, and three-body gravitational systems with noisy and extremely small datasets.",https://arxiv.org/abs/2209.01524
Quantitative Stopword Generation for Sentiment Analysis via Recursiveand Iterative Deletion,Daniel M.DiPietro,04-sep-22,Computation and Language (cs.CL)," Stopwords carry little semantic information and are often removed from text data to reduce dataset size and improve machine learning model performance. Consequently, researchers have sought to develop techniques for generating effective stopword sets. Previous approaches have ranged from qualitative techniques relying upon linguistic experts, to statistical approaches that extract word importance using correlations or frequency- dependent metrics computed on a corpus. We present a novel quantitative approach that employs iterative and recursive feature deletion algorithms to see which words can be deleted from a pre-trained transformer's vocabulary with the least degradation to its performance, specifically for the task of sentiment analysis. Empirically, stopword lists generated via this approach drastically reduce dataset size while negligibly impacting model performance, in one such example shrinking the corpus by 28.4% while improving the accuracy of a trained logistic regression model by 0.25%. In another instance, the corpus was shrunk by 63.7% with a 2.8% decrease in accuracy. These promising results indicate that our approach can generate highly effective stopword sets for specific NLP tasks.",https://arxiv.org/abs/2209.01521
An Empirical Study of Automation in Software Security Patch Management,"NesaraDissanayake, AsangiJayatilaka, MansoorehZahedi, Muhammad AliBabar",04-sep-22,Software Engineering (cs.SE)," Several studies have shown that automated support for different activities of the security patch management process has great potential for reducing delays in installing security patches. However, it is also important to understand how automation is used in practice, its limitations in meeting real-world needs and what practitioners really need, an area that has not been empirically investigated in the existing software engineering literature. This paper reports an empirical study aimed at investigating different aspects of automation for security patch management using semi- structured interviews with 17 practitioners from three different organisations in the healthcare domain. The findings are focused on the role of automation in security patch management for providing insights into the as-is state of automation in practice, the limitations of current automation, how automation support can be enhanced to effectively meet practitioners' needs, and the role of the human in an automated process. Based on the findings, we have derived a set of recommendations for directing future efforts aimed at developing automated support for security patch management.",https://arxiv.org/abs/2209.01519
Joint Prediction of Meningioma Grade and Brain Invasion via Task-AwareContrastive Learning,"TianlingLiu, WennanLiu, LequanYu, LiangWan, TongHan, LeiZhu",04-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Preoperative and noninvasive prediction of the meningioma grade is important in clinical practice, as it directly influences the clinical decision making. What's more, brain invasion in meningioma (i.e., the presence of tumor tissue within the adjacent brain tissue) is an independent criterion for the grading of meningioma and influences the treatment strategy. Although efforts have been reported to address these two tasks, most of them rely on hand-crafted features and there is no attempt to exploit the two prediction tasks simultaneously. In this paper, we propose a novel task-aware contrastive learning algorithm to jointly predict meningioma grade and brain invasion from multi-modal MRIs. Based on the basic multi-task learning framework, our key idea is to adopt contrastive learning strategy to disentangle the image features into task-specific features and task-common features, and explicitly leverage their inherent connections to improve feature representation for the two prediction tasks. In this retrospective study, an MRI dataset was collected, for which 800 patients (containing 148 high-grade, 62 invasion) were diagnosed with meningioma by pathological analysis. Experimental results show that the proposed algorithm outperforms alternative multi-task learning methods, achieving AUCs of 0:8870 and 0:9787 for the prediction of meningioma grade and brain invasion, respectively. The code is available at [this https URL](https://github.com/IsDling/predictTCL).",https://arxiv.org/abs/2209.01518
Do Large Language Models know what humans know?,"SeanTrott, CameronJones, TylerChang, JamesMichaelov, BenjaminBergen",04-sep-22,Computation and Language (cs.CL)," Humans can attribute mental states to others, a capacity known as Theory of Mind. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language develop evidence of Theory of Mind. In a pre-registered analysis, we present a linguistic version of the False Belief Task, widely used to assess Theory of Mind, to both human participants and a state-of-the-art Large Language Model, GPT-3. Both are sensitive to others' beliefs, but the language model does not perform as well as the humans, nor does it explain the full extent of their behavior, despite being exposed to more language than a human would in a lifetime. This suggests that while language exposure may in part explain how humans develop Theory of Mind, other mechanisms are also responsible.",https://arxiv.org/abs/2209.01517
A Novel Nearest Neighbors Algorithm Based on Power Muirhead Mean,"KouroshShahnazari, Seyed MoeinAyyoubzadeh",04-sep-22,Machine Learning (cs.LG), This study aimed to propose a novel classifier based on K-Nearest Neighbors which calculates the local means of every class using the Power Muirhead Mean operator. We have called our new method Power Muirhead Mean K-Nearest Neighbors (PMM-KNN) classifier. The PMM-KNN classifier has several parameters which can be determined and fine-tuned for each problem that is countered as an advantage compared to other Nearest Neighbors methods. We used five well-known datasets to assess PMM-KNN performance. The research results demonstrate that the PMM-KNN has outperformed some of the other classification methods.,https://arxiv.org/abs/2209.01515
An interpretative and adaptive MPC for nonlinear systems,LiangWu,04-sep-22,Systems and Control (eess.SY)," Model predictive control (MPC) for nonlinear systems suffers a trade-off between the model accuracy and real-time computational burden. One widely used approximation method is the successive linearization MPC (SL- MPC) with EKF method, in which the EKF algorithm is to handle unmeasured disturbances and unavailable full states information. Inspired by this, an interpretative and adaptive MPC (IA-MPC) method, is presented in this paper. In our IA-MPC method, a linear state-space model is firstly obtained by performing the linearization of a first-principle-based model at the initial point, and then this linear state-space model is transformed into an equivalent ARX model. This interpretative ARX model is then updated online by the EKF algorithm, which is modified as a decoupled one without matrix- inverse operator. The corresponding ARX-based MPC problem are solved by our previous construction-free, matrix-free and library-free CDAL-ARX algorithm. This simple library-free C-code implementation would significantly reduce the difficulty in deploying nonlinear MPC on embedded platforms. The performance of the IA-MPC method is tested against the nonlinear MPC with EKF and SL-MPC with EKF method in four typical nonlinear benchmark examples, which show the effectiveness of our IA-MPC method.",https://arxiv.org/abs/2209.01514
Transfer Learning of an Ensemble of DNNs for SSVEP BCI Spellerswithout User-Specific Training,"Osman BerkeGuney, HuseyinOzkan",03-sep-22,Machine Learning (cs.LG)," Objective: Steady-state visually evoked potentials (SSVEPs), measured with EEG (electroencephalogram), yield decent information transfer rates (ITR) in brain-computer interface (BCI) spellers. However, the current high performing SSVEP BCI spellers in the literature require an initial lengthy and tiring user-specific training for each new user for system adaptation, including data collection with EEG experiments, algorithm training and calibration (all are before the actual use of the system). This impedes the widespread use of BCIs. To ensure practicality, we propose a highly novel target identification method based on an ensemble of deep neural networks (DNNs), which does not require any sort of user-specific training. Method: We exploit already-existing literature datasets from participants of previously conducted EEG experiments to train a global target identifier DNN first, which is then fine-tuned to each participant. We transfer this ensemble of fine-tuned DNNs to the new user instance, determine the k most representative DNNs according to the participants' statistical similarities to the new user, and predict the target character through a weighted combination of the ensemble predictions. Results: On two large-scale benchmark and BETA datasets, our method achieves impressive 155.51 bits/min and 114.64 bits/min ITRs. Code is available for reproducibility: [this https URL](https://github.com/osmanberke/Ensemble-of- DNNs) Conclusion: The proposed method significantly outperforms all the state-of-the-art alternatives for all stimulation durations in [0.2-1.0] seconds on both datasets. Significance: Our Ensemble-DNN method has the potential to promote the practical widespread deployment of BCI spellers in daily lives as we provide the highest performance while enabling the immediate system use without any user-specific training.",https://arxiv.org/abs/2209.01513
Optimal Mitigation of SIR Epidemics Under Model Uncertainty,"BaikeShe, ShreyasSundaram, Philip E.ParÃ©",03-sep-22,Systems and Control (eess.SY)," We study the impact of model parameter uncertainty on optimally mitigating the spread of epidemics. We capture the epidemic spreading process using a susceptible-infected-removed (SIR) epidemic model and consider testing for isolation as the control strategy. We use a testing strategy to remove (isolate) a portion of the infected population. Our goal is to maintain the daily infected population below a certain level, while minimizing the total number of tests. Distinct from existing works on leveraging control strategies in epidemic spreading, we propose a testing strategy by overestimating the seriousness of the epidemic and study the feasibility of the system under the impact of model parameter uncertainty. Compared to the optimal testing strategy, we establish that the proposed strategy under model parameter uncertainty will flatten the curve effectively but require more tests and a longer time period.",https://arxiv.org/abs/2209.01511
Low-Power Hardware-Based Deep-Learning Diagnostics Support Case Study,"KhushalSethi, VivekParmar, Manan Suri",03-sep-22,Machine Learning (cs.LG)," Deep learning research has generated widespread interest leading to emergence of a large variety of technological innovations and applications. As significant proportion of deep learning research focuses on vision based applications, there exists a potential for using some of these techniques to enable low-power portable health-care diagnostic support solutions. In this paper, we propose an embedded-hardware-based implementation of microscopy diagnostic support system for PoC case study on: (a) Malaria in thick blood smears, (b) Tuberculosis in sputum samples, and (c) Intestinal parasite infection in stool samples. We use a Squeeze-Net based model to reduce the network size and computation time. We also utilize the Trained Quantization technique to further reduce memory footprint of the learned models. This enables microscopy-based detection of pathogens that classifies with laboratory expert level accuracy as a standalone embedded hardware platform. The proposed implementation is 6x more power-efficient compared to conventional CPU-based implementation and has an inference time of $\sim$ 3 ms/sample.",https://arxiv.org/abs/2209.01508
Neural Networks for Chess,DominikKlein,03-sep-22,Machine Learning (cs.LG)," AlphaZero, Leela Chess Zero and Stockfish NNUE revolutionized Computer Chess. This book gives a complete introduction into the technical inner workings of such engines. The book is split into four main chapters -- excluding chapter 1 (introduction) and chapter 6 (conclusion): Chapter 2 introduces neural networks and covers all the basic building blocks that are used to build deep networks such as those used by AlphaZero. Contents include the perceptron, back-propagation and gradient descent, classification, regression, multilayer perceptron, vectorization techniques, convolutional networks, squeeze and excitation networks, fully connected networks, batch normalization and rectified linear units, residual layers, overfitting and underfitting. Chapter 3 introduces classical search techniques used for chess engines as well as those used by AlphaZero. Contents include minimax, alpha-beta search, and Monte Carlo tree search. Chapter 4 shows how modern chess engines are designed. Aside from the ground-breaking AlphaGo, AlphaGo Zero and AlphaZero we cover Leela Chess Zero, Fat Fritz, Fat Fritz 2 and Efficiently Updatable Neural Networks (NNUE) as well as Maia. Chapter 5 is about implementing a miniaturized AlphaZero. Hexapawn, a minimalistic version of chess, is used as an example for that. Hexapawn is solved by minimax search and training positions for supervised learning are generated. Then as a comparison, an AlphaZero-like training loop is implemented where training is done via self-play combined with reinforcement learning. Finally, AlphaZero-like training and supervised training are compared.",https://arxiv.org/abs/2209.01507
Locally-verifiable sufficient conditions for exactness of thehierarchical B-spline discrete de Rham complex in $\mathbb{R}^n$,"KendrickShepherd, DeepeshToshniwal",03-sep-22,Numerical Analysis (math.NA)," Given a domain $\Omega \subset \mathbb{R}^n$, the de Rham complex of differential forms arises naturally in the study of problems in electromagnetism and fluid mechanics defined on $\Omega$, and its discretization helps build stable numerical methods for such problems. For constructing such stable methods, one critical requirement is ensuring that the discrete subcomplex is cohomologically equivalent to the continuous complex. When $\Omega$ is a hypercube, we thus require that the discrete subcomplex be exact. Focusing on such $\Omega$, we theoretically analyze the discrete de Rham complex built from hierarchical B-spline differential forms, i.e., the discrete differential forms are smooth splines and support adaptive refinements - these properties are key to enabling accurate and efficient numerical simulations. We provide locally-verifiable sufficient conditions that ensure that the discrete spline complex is exact. Numerical tests are presented to support the theoretical results, and the examples discussed include complexes that satisfy our prescribed conditions as well as those that violate them.",https://arxiv.org/abs/2209.01506
Meta-Learning with Less Forgetting on Large-Scale Non-Stationary TaskDistributions,"ZhenyiWang, LiShen, LeFang, QiulingSuo, DonglinZhan, TiehangDuan, Mingchen Gao",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The paradigm of machine intelligence moves from purely supervised learning to a more practical scenario when many loosely related unlabeled data are available and labeled data is scarce. Most existing algorithms assume that the underlying task distribution is stationary. Here we consider a more realistic and challenging setting in that task distributions evolve over time. We name this problem as Semi-supervised meta-learning with Evolving Task diStributions, abbreviated as SETS. Two key challenges arise in this more realistic setting: (i) how to use unlabeled data in the presence of a large amount of unlabeled out-of-distribution (OOD) data; and (ii) how to prevent catastrophic forgetting on previously learned task distributions due to the task distribution shift. We propose an OOD Robust and knowleDge presErved semi-supeRvised meta-learning approach (ORDER), to tackle these two major challenges. Specifically, our ORDER introduces a novel mutual information regularization to robustify the model with unlabeled OOD data and adopts an optimal transport regularization to remember previously learned knowledge in feature space. In addition, we test our method on a very challenging dataset: SETS on large-scale non-stationary semi-supervised task distributions consisting of (at least) 72K tasks. With extensive experiments, we demonstrate the proposed ORDER alleviates forgetting on evolving task distributions and is more robust to OOD data than related strong baselines.",https://arxiv.org/abs/2209.01504
Sion: Elastic Serverless Cloud Storage,"JingyuanZhang, AoWang, Xiaolong Ma, BenjaminCarver, Nicholas JohnNewman, Ali Anwar, LukasRupprecht, DimitriosSkourtis, VasilyTarasov, Feng Yan, YueCheng",03-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," Cloud object storage such as AWS S3 is cost-effective and highly elastic but relatively slow, while high-performance cloud storage such as AWS ElastiCache is expensive and provides limited elasticity. We present a new cloud storage service called ServerlessMemory, which stores data using the memory of serverless functions. ServerlessMemory employs a time-window- based data placement strategy to effectively segregate old and new data and provides high elasticity, performance, and a pay-per-access cost model with extremely low cost for a new memory-based storage.   We then design and implement SION (Serverless I/O Nirvana), a persistent and elastic cloud storage system, which seamlessly couples the function-based ServerlessMemory layer with a persistent, inexpensive cloud object store layer. SION enables durability despite function failures using a fast parallel recovery scheme built on the auto-scaling functionality of a FaaS (Function-as-a-Service) platform. We evaluate SION extensively using three real-world applications and results show that SION achieves 27.77% and 97.30% tenant-side cost reduction compared to InfiniCache (a serverless- function-based object cache) and AWS ElastiCache respectively, while offering pay-per-access with competitive performance.",https://arxiv.org/abs/2209.01501
Learning Differential Operators for Interpretable Time Series Modeling,"YingtaoLuo, ChangXu, YangLiu, WeiqingLiu, ShunZheng, JiangBian",03-sep-22,Machine Learning (cs.LG)," Modeling sequential patterns from data is at the core of various time series forecasting tasks. Deep learning models have greatly outperformed many traditional models, but these black-box models generally lack explainability in prediction and decision making. To reveal the underlying trend with understandable mathematical expressions, scientists and economists tend to use partial differential equations (PDEs) to explain the highly nonlinear dynamics of sequential patterns. However, it usually requires domain expert knowledge and a series of simplified assumptions, which is not always practical and can deviate from the ever-changing world. Is it possible to learn the differential relations from data dynamically to explain the time-evolving dynamics? In this work, we propose an learning framework that can automatically obtain interpretable PDE models from sequential data. Particularly, this framework is comprised of learnable differential blocks, named $P$-blocks, which is proved to be able to approximate any time-evolving complex continuous functions in theory. Moreover, to capture the dynamics shift, this framework introduces a meta- learning controller to dynamically optimize the hyper-parameters of a hybrid PDE model. Extensive experiments on times series forecasting of financial, engineering, and health data show that our model can provide valuable interpretability and achieve comparable performance to state-of-the-art models. From empirical studies, we find that learning a few differential operators may capture the major trend of sequential dynamics without massive computational complexity.",https://arxiv.org/abs/2209.01496
Model-Free Deep Reinforcement Learning in Software-Defined Networks,"LukeBorchjes, ClementNyirenda, LouiseLeenen",03-sep-22,Artificial Intelligence (cs.AI)," This paper compares two deep reinforcement learning approaches for cyber security in software defined networking. Neural Episodic Control to Deep Q-Network has been implemented and compared with that of Double Deep Q-Networks. The two algorithms are implemented in a format similar to that of a zero-sum game. A two-tailed T-test analysis is done on the two game results containing the amount of turns taken for the defender to win. Another comparison is done on the game scores of the agents in the respective games. The analysis is done to determine which algorithm is the best in game performer and whether there is a significant difference between them, demonstrating if one would have greater preference over the other. It was found that there is no significant statistical difference between the two approaches.",https://arxiv.org/abs/2209.01491
Differentially-private Distributed Algorithms for Aggregative Gameswith Guaranteed Convergence,"YongqiangWang, AngeliaNedich",03-sep-22,Computer Science and Game Theory (cs.GT)," The distributed computation of a Nash equilibrium in aggregative games is gaining increased traction in recent years. Of particular interest is the mediator-free scenario where individual players only access or observe the decisions of their neighbors due to practical constraints. Given the competitive rivalry among participating players, protecting the privacy of individual players becomes imperative when sensitive information is involved. We propose a fully distributed equilibrium-computation approach for aggregative games that can achieve both rigorous differential privacy and guaranteed computation accuracy of the Nash equilibrium. This is in sharp contrast to existing differential-privacy solutions for aggregative games that have to either sacrifice the accuracy of equilibrium computation to gain rigorous privacy guarantees, or allow the cumulative privacy budget to grow unbounded, hence losing privacy guarantees, as iteration proceeds. Our approach uses independent noises across players, thus making it effective even when adversaries have access to all shared messages as well as the underlying algorithm structure. The encryption-free nature of the proposed approach, also ensures efficiency in computation and communication. The approach is also applicable in stochastic aggregative games, able to ensure both rigorous differential privacy and guaranteed computation accuracy of the Nash equilibrium when individual players only have stochastic estimates of their pseudo-gradient mappings. Numerical comparisons with existing counterparts confirm the effectiveness of the proposed approach.",https://arxiv.org/abs/2209.01490
A Hybrid Tracking Control Strategy for an Unmanned Underwater VehicleAided with Bioinspired Neural Dynamics,"Zhe Xu, Tao Yan, Simon X.Yang, S.AndrewGadsden",03-sep-22,Robotics (cs.RO)," Tracking control has been a vital research topic in robotics. This paper presents a novel hybrid control strategy for an unmanned underwater vehicle (UUV) based on a bioinspired neural dynamics model. An enhanced backstepping kinematic control strategy is first developed to avoid sharp velocity jumps and provides smooth velocity commands relative to conventional methods. Then, a novel sliding mode control is proposed, which is capable of providing smooth and continuous torque commands free from chattering. In comparative studies, the proposed combined hybrid control strategy has ensured control signals smoothness, which is critical in real world applications, especially for an unmanned underwater vehicle that needs to operate in complex underwater environments.",https://arxiv.org/abs/2209.01486
A Novel Knowledge-Based Genetic Algorithm for Robot Path Planning inComplex Environments,"YanrongHu, Simon X.Yang",03-sep-22,Robotics (cs.RO)," In this paper, a novel knowledge-based genetic algorithm for path planning of a mobile robot in unstructured complex environments is proposed, where five problem-specific operators are developed for efficient robot path planning. The proposed genetic algorithm incorporates the domain knowledge of robot path planning into its specialized operators, some of which also combine a local search technique. A unique and simple representation of the robot path is proposed and a simple but effective path evaluation method is developed, where the collisions can be accurately detected and the quality of a robot path is well reflected. The proposed algorithm is capable of finding a near-optimal robot path in both static and dynamic complex environments. The effectiveness and efficiency of the proposed algorithm are demonstrated by simulation studies. The irreplaceable role of the specialized genetic operators in the proposed genetic algorithm for solving the robot path planning problem is demonstrated through a comparison study.",https://arxiv.org/abs/2209.01484
Equivariant Self-Supervision for Musical Tempo Estimation,ElioQuinton,03-sep-22,Sound (cs.SD)," Self-supervised methods have emerged as a promising avenue for representation learning in the recent years since they alleviate the need for labeled datasets, which are scarce and expensive to acquire. Contrastive methods are a popular choice for self-supervision in the audio domain, and typically provide a learning signal by forcing the model to be invariant to some transformations of the input. These methods, however, require measures such as negative sampling or some form of regularisation to be taken to prevent the model from collapsing on trivial solutions. In this work, instead of invariance, we propose to use equivariance as a self-supervision signal to learn audio tempo representations from unlabelled data. We derive a simple loss function that prevents the network from collapsing on a trivial solution during training, without requiring any form of regularisation or negative sampling. Our experiments show that it is possible to learn meaningful representations for tempo estimation by solely relying on equivariant self-supervision, achieving performance comparable with supervised methods on several benchmarks. As an added benefit, our method only requires moderate compute resources and therefore remains accessible to a wide research community.",https://arxiv.org/abs/2209.01482
Learning the Dynamics of Particle-based Systems with Lagrangian GraphNeural Networks,"RavinderBhattoo, Sayan Ranu, N. M. AnoopKrishnan",03-sep-22,Machine Learning (cs.LG)," Physical systems are commonly represented as a combination of particles, the individual dynamics of which govern the system dynamics. However, traditional approaches require the knowledge of several abstract quantities such as the energy or force to infer the dynamics of these particles. Here, we present a framework, namely, Lagrangian graph neural network (LGnn), that provides a strong inductive bias to learn the Lagrangian of a particle-based system directly from the trajectory. We test our approach on challenging systems with constraints and drag -- LGnn outperforms baselines such as feed-forward Lagrangian neural network (Lnn) with improved performance. We also show the zero-shot generalizability of the system by simulating systems two orders of magnitude larger than the trained one and also hybrid systems that are unseen by the model, a unique feature. The graph architecture of LGnn significantly simplifies the learning in comparison to Lnn with ~25 times better performance on ~20 times smaller amounts of data. Finally, we show the interpretability of LGnn, which directly provides physical insights on drag and constraint forces learned by the model. LGnn can thus provide a fillip toward understanding the dynamics of physical systems purely from observable quantities.",https://arxiv.org/abs/2209.01478
Model-based Analysis and Specification of Functional Requirements andTests for Complex Automotive Systems,"CarstenWiecher, ConstantinMandel, MatthiasGÃ¼nther, JannikFischbach, JoelGreenyer, MatthiasGreinert, CarstenWolff, RomanDumitrescu, DanielMendez, AlbertAlbers",03-sep-22,Software Engineering (cs.SE)," The specification of requirements and tests are crucial activities in automotive development projects. However, due to the increasing complexity of automotive systems, practitioners fail to specify requirements and tests for distributed and evolving systems with complex interactions when following traditional development processes. To address this research gap, we propose a technique that starts with the early identification of validation concerns from a stakeholder perspective, which we use to systematically design tests that drive a scenario-based modeling and automated analysis of system requirements. We discover that Natural Language Processing (NLP) techniques are suitable to automate the test-case design and hence enable the application of our technique to real-world stakeholder requirements. To ensure complete and consistent requirements and test specifications in a form that is required in automotive development projects, we develop a Model-Based Systems Engineering (MBSE) methodology. This methodology supports system architects and test designers in the collaborative application of our technique and in maintaining a central system model, in order to automatically derive the required specifications. We evaluate our methodology by applying it at KOSTAL (Tier1 supplier) and within student projects as part of the masters program Embedded Systems Engineering. Our study corroborates that our methodology is applicable and improves existing requirements and test specification processes by supporting the integrated and stakeholder-focused modeling of product and validation systems, where the early definition of stakeholder and validation concerns fosters a problem-oriented, iterative and test-driven requirements modeling.",https://arxiv.org/abs/2209.01476
Neural Sign Reenactor: Deep Photorealistic Sign Language Retargeting,"Christina O.Tze, Panagiotis P.Filntisis, Athanasia-LidaDimou, AnastasiosRoussos, PetrosMaragos",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," In this paper, we introduce a neural rendering pipeline for transferring the facial expressions, head pose and body movements of one person in a source video to another in a target video. We apply our method to the challenging case of Sign Language videos: given a source video of a sign language user, we can faithfully transfer the performed manual (e.g. handshape, palm orientation, movement, location) and non-manual (e.g. eye gaze, facial expressions, head movements) signs to a target video in a photo-realistic manner. To effectively capture the aforementioned cues, which are crucial for sign language communication, we build upon an effective combination of the most robust and reliable deep learning methods for body, hand and face tracking that have been introduced lately. Using a 3D-aware representation, the estimated motions of the body parts are combined and retargeted to the target signer. They are then given as conditional input to our Video Rendering Network, which generates temporally consistent and photo-realistic videos. We conduct detailed qualitative and quantitative evaluations and comparisons, which demonstrate the effectiveness of our approach and its advantages over existing approaches. Our method yields promising results of unprecedented realism and can be used for Sign Language Anonymization. In addition, it can be readily applicable to reenactment of other types of full body activities (dancing, acting performance, exercising, etc.), as well as to the synthesis module of Sign Language Production systems.",https://arxiv.org/abs/2209.01473
Machine learning for dynamically predicting the onset of renalreplacement therapy in chronic kidney disease patients using claims data,"Daniel Lopez-Martinez, ChristinaChen, Ming-Jun Chen",03-sep-22,Machine Learning (cs.LG)," Chronic kidney disease (CKD) represents a slowly progressive disorder that can eventually require renal replacement therapy (RRT) including dialysis or renal transplantation. Early identification of patients who will require RRT (as much as 1 year in advance) improves patient outcomes, for example by allowing higher-quality vascular access for dialysis. Therefore, early recognition of the need for RRT by care teams is key to successfully managing the disease. Unfortunately, there is currently no commonly used predictive tool for RRT initiation. In this work, we present a machine learning model that dynamically identifies CKD patients at risk of requiring RRT up to one year in advance using only claims data. To evaluate the model, we studied approximately 3 million Medicare beneficiaries for which we made over 8 million predictions. We showed that the model can identify at risk patients with over 90% sensitivity and specificity. Although additional work is required before this approach is ready for clinical use, this study provides a basis for a screening tool to identify patients at risk within a time window that enables early proactive interventions intended to improve RRT outcomes.",https://arxiv.org/abs/2209.01470
Randomized Privacy Budget Differential Privacy,MeisamMohammady,03-sep-22,Cryptography and Security (cs.CR)," While pursuing better utility by discovering knowledge from the data, individual's privacy may be compromised during an analysis. To that end, differential privacy has been widely recognized as the state-of-the-art privacy notion. By requiring the presence of any individual's data in the input to only marginally affect the distribution over the output, differential privacy provides strong protection against adversaries in possession of arbitrary background. However, the privacy constraints (e.g., the degree of randomization) imposed by differential privacy may render the released data less useful for analysis, the fundamental trade-off between privacy and utility (i.e., analysis accuracy) has attracted significant attention in various settings. In this report we present DP mechanisms with randomized parameters, i.e., randomized privacy budget, and formally analyze its privacy and utility and demonstrate that randomizing privacy budget in DP mechanisms will boost the accuracy in a humongous scale.",https://arxiv.org/abs/2209.01469
R$^2$DP: A Universal and Automated Approach to Optimizing theRandomization Mechanisms of Differential Privacy for Utility Metrics with NoKnown Optimal Distributions,"MeisamMohammady, Shangyu Xie, Yuan Hong, MengyuanZhang, Lingyu Wang, MakanPourzandi, MouradDebbabi","20 Sep 2020 (v1(https://arxiv.org/abs/2009.09451v1)), lastrevised 24 Sep 2020 (this version, v2)",Cryptography and Security (cs.CR)," Differential privacy (DP) has emerged as a de facto standard privacy notion for a wide range of applications. Since the meaning of data utility in different applications may vastly differ, a key challenge is to find the optimal randomization mechanism, i.e., the distribution and its parameters, for a given utility metric. Existing works have identified the optimal distributions in some special cases, while leaving all other utility metrics (e.g., usefulness and graph distance) as open problems. Since existing works mostly rely on manual analysis to examine the search space of all distributions, it would be an expensive process to repeat such efforts for each utility metric. To address such deficiency, we propose a novel approach that can automatically optimize different utility metrics found in diverse applications under a common framework. Our key idea that, by regarding the variance of the injected noise itself as a random variable, a two-fold distribution may approximately cover the search space of all distributions. Therefore, we can automatically find distributions in this search space to optimize different utility metrics in a similar manner, simply by optimizing the parameters of the two-fold distribution. Specifically, we define a universal framework, namely, randomizing the randomization mechanism of differential privacy (R$^2$DP), and we formally analyze its privacy and utility. Our experiments show that R$^2$DP can provide better results than the baseline distribution (Laplace) for several utility metrics with no known optimal distributions, whereas our results asymptotically approach to the optimality for utility metrics having known optimal distributions. As a side benefit, the added degree of freedom introduced by the two-fold distribution allows R$^2$DP to accommodate the preferences of both data owners and recipients.",https://arxiv.org/abs/2209.01468
Age-Dependent Differential Privacy,"MengZhang, ErminWei, RandallBerry, JianweiHuang",03-sep-22,Cryptography and Security (cs.CR)," The proliferation of real-time applications has motivated extensive research on analyzing and optimizing data freshness in the context of \textit{age of information}. However, classical frameworks of privacy (e.g., differential privacy (DP)) have overlooked the impact of data freshness on privacy guarantees, which may lead to unnecessary accuracy loss when trying to achieve meaningful privacy guarantees in time-varying databases. In this work, we introduce \textit{age-dependent DP}, taking into account the underlying stochastic nature of a time-varying database. In this new framework, we establish a connection between classical DP and age- dependent DP, based on which we characterize the impact of data staleness and temporal correlation on privacy guarantees. Our characterization demonstrates that \textit{aging}, i.e., using stale data inputs and/or postponing the release of outputs, can be a new strategy to protect data privacy in addition to noise injection in the traditional DP framework. Furthermore, to generalize our results to a multi-query scenario, we present a sequential composition result for age-dependent DP under any publishing and aging policies. We then characterize the optimal tradeoffs between privacy risk and utility and show how this can be achieved. Finally, case studies show that to achieve a target of an arbitrarily small privacy risk in a single-query case, combing aging and noise injection only leads to a bounded accuracy loss, whereas using noise injection only (as in the benchmark case of DP) will lead to an unbounded accuracy loss.",https://arxiv.org/abs/2009.09451
A Markov Process Theory for Network Growth Processes of DAG-basedBlockchain Systems,"Xing-ShuoSong, Quan-Lin Li, Yan-XiaChang, ChiZhan",03-sep-22,Performance (cs.PF)," Note that the serial structure of blockchain has a number of essential pitfalls, and, thus, a data network structure and its DAG-based blockchain are introduced to resolve the blockchain pitfalls. From such a network perspective, analysis of the DAG-based blockchain systems becomes interesting but difficult and challenging. So, the simulation models are adopted widely. In this paper, we first describe a simple Markov model for the DAG-based blockchain with IOTA Tangle by means of two layers of tips and internal tips' impatient connection behavior. Then we set up a continuous- time Markov process to analyze the DAG-based blockchain system and show that this Markov process is a level-dependent quasi-birth-and-death (QBD) process. Based on this, we prove that the QBD process must be irreducible and positive recurrent. Furthermore, once the stationary probability vector of the QBD process is given, we provide performance analysis of the DAG- based blockchain system. Nextly, we propose a new effective method for computing the average sojourn time of any arriving internal tip at this system by means of the first passage times and the PH distributions. Finally, we use numerical examples to check the validity of our theoretical results and indicate how some key system parameters influence the performance measures of this system. Therefore, we hope that the methodology and results developed in this paper shed light on the DAG-based blockchain systems such that a series of promising research can be developed potentially.",https://arxiv.org/abs/2209.01466
Phishing URL Detection: A Network-based Approach Robust to Evasion,"TaeriKim, NoseongPark, JiwonHong, Sang-Wook Kim",03-sep-22,Cryptography and Security (cs.CR)," Many cyberattacks start with disseminating phishing URLs. When clicking these phishing URLs, the victim's private information is leaked to the attacker. There have been proposed several machine learning methods to detect phishing URLs. However, it still remains under-explored to detect phishing URLs with evasion, i.e., phishing URLs that pretend to be benign by manipulating patterns. In many cases, the attacker i) reuses prepared phishing web pages because making a completely brand-new set costs non- trivial expenses, ii) prefers hosting companies that do not require private information and are cheaper than others, iii) prefers shared hosting for cost efficiency, and iv) sometimes uses benign domains, IP addresses, and URL string patterns to evade existing detection methods. Inspired by those behavioral characteristics, we present a network-based inference method to accurately detect phishing URLs camouflaged with legitimate patterns, i.e., robust to evasion. In the network approach, a phishing URL will be still identified as phishy even after evasion unless a majority of its neighbors in the network are evaded at the same time. Our method consistently shows better detection performance throughout various experimental tests than state-of-the-art methods, e.g., F-1 of 0.89 for our method vs. 0.84 for the best feature-based method.",https://arxiv.org/abs/2209.01458
A comprehensive survey on recent deep learning-based methods appliedto surgical data,"MansoorAli, RafaelMartinez GarciaPena, Gilberto OchoaRuiz, Sharib Ali",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Minimally invasive surgery is highly operator dependant with lengthy procedural times causing fatigue and risk to patients. In order to mitigate these risks, real-time systems can help assist surgeons to navigate and track tools, by providing clear understanding of scene and avoid miscalculations during operation. While several efforts have been made in this direction, a lack of diverse datasets, as well as very dynamic scenes and its variability in each patient entails major hurdle in accomplishing robust systems. In this work, we present a systematic review of recent machine learning-based approaches including surgical tool localisation, segmentation, tracking and 3D scene perception. Furthermore, we present current gaps and directions of these invented methods and provide rational behind clinical integration of these approaches.",https://arxiv.org/abs/2209.01454
Reinforcement Learning with Prior Policy Guidance for Motion Planningof Dual-Arm Free-Floating Space Robot,"YuxueCao, ShengjieWang, XiangZheng, WenkeMa, XinruXie, LeiLiu",03-sep-22,Robotics (cs.RO)," Reinforcement learning methods as a promising technique have achieved superior results in the motion planning of free-floating space robots. However, due to the increase in planning dimension and the intensification of system dynamics coupling, the motion planning of dual-arm free-floating space robots remains an open challenge. In particular, the current study cannot handle the task of capturing a non-cooperative object due to the lack of the pose constraint of the end-effectors. To address the problem, we propose a novel algorithm, EfficientLPT, to facilitate RL-based methods to improve planning accuracy efficiently. Our core contributions are constructing a mixed policy with prior knowledge guidance and introducing infinite norm to build a more reasonable reward function. Furthermore, our method successfully captures a rotating object with different spinning speeds.",https://arxiv.org/abs/2209.01435
STAD: Self-Training with Ambiguous Data for Low-Resource RelationExtraction,"JunjieYu, XingWang, JiangjiangZhao, ChunjieYang, Wenliang Chen","3 Sep 2022 (v1(https://arxiv.org/abs/2209.01431v1)), lastrevised 7 Sep 2022 (this version, v2)",Computation and Language (cs.CL)," We present a simple yet effective self-training approach, named as STAD, for low-resource relation extraction. The approach first classifies the auto-annotated instances into two groups: confident instances and uncertain instances, according to the probabilities predicted by a teacher model. In contrast to most previous studies, which mainly only use the confident instances for self-training, we make use of the uncertain instances. To this end, we propose a method to identify ambiguous but useful instances from the uncertain instances and then divide the relations into candidate-label set and negative-label set for each ambiguous instance. Next, we propose a set-negative training method on the negative-label sets for the ambiguous instances and a positive training method for the confident instances. Finally, a joint-training method is proposed to build the final relation extraction system on all data. Experimental results on two widely used datasets SemEval2010 Task-8 and Re-TACRED with low-resource settings demonstrate that this new self-training approach indeed achieves significant and consistent improvements when comparing to several competitive self- training systems. Code is publicly available at [this https URL](https://github.com/jjyunlp/STAD)",https://arxiv.org/abs/2209.01434
Online Evasive Strategy for Aerial Survey using Sierpinski curve,"AshayWakode, Arpita Sinha",03-sep-22,Robotics (cs.RO)," This paper deals with the aerial survey of a closed region using the Space-Filling curve, particularly Sierpinski curve. The specified region is triangulated, and the Sierpinski curve is used to explore each smaller triangular region. The entire region may have one or more obstacles. An algorithm is presented which suggests evasive manoeuvre (detour) if an obstacle is detected. The algorithm is online; that is, it does not require prior knowledge of the location of obstacles and can be applied while the robotic system is traversing the designated path. The fractal nature of the Sierpinski curve and simple geometric observations were used to formulate and validate the algorithm. The non-uniform coverage and multiple obstacle problems are also dealt with towards the end.",https://arxiv.org/abs/2209.01431
Dynamic Spatio-Temporal Specialization Learning for Fine-GrainedAction Recognition,"TianjiaoLi, Lin GengFoo, QiuhongKe, HosseinRahmani, Anran Wang, Jinghua Wang, Jun Liu",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The goal of fine-grained action recognition is to successfully discriminate between action categories with subtle differences. To tackle this, we derive inspiration from the human visual system which contains specialized regions in the brain that are dedicated towards handling specific tasks. We design a novel Dynamic Spatio-Temporal Specialization (DSTS) module, which consists of specialized neurons that are only activated for a subset of samples that are highly similar. During training, the loss forces the specialized neurons to learn discriminative fine-grained differences to distinguish between these similar samples, improving fine- grained recognition. Moreover, a spatio-temporal specialization method further optimizes the architectures of the specialized neurons to capture either more spatial or temporal fine-grained information, to better tackle the large range of spatio-temporal variations in the videos. Lastly, we design an Upstream-Downstream Learning algorithm to optimize our model's dynamic decisions during training, improving the performance of our DSTS module. We obtain state-of-the-art performance on two widely-used fine- grained action recognition datasets.",https://arxiv.org/abs/2209.01426
Deep Live Video Ad Placement on the 5G Edge,MohammadHosseini,03-sep-22,Multimedia (cs.MM)," The video broadcasting industry has been growing significantly in the recent years, specially on delivering personalized contents to the end users. While video broadcasting has continued to grow beyond TV, video adverting has become a key marketing tool to deliver targeted messages directly to the audience. However, unfortunately for broadband TV, a key problem is that the TV commercials target the broad audience, therefore lacking user-specific and personalized ad contents.   In this paper, we propose a deep edge-cloud ad-placement system, and briefly describe our methodologies and the architecture of our designed ad placement system for delivering both the Video on Demand (VoD) and live broadcast TV contents over MMT streaming protocol. The aim of our paper is to showcase how to enable targeted, personalized, and user-specific advertising services deployed on the future 5G MEC platforms, which in turn can have high potentials to increase ad revenues for the mobile operator industry.",https://arxiv.org/abs/2209.01425
Homogenization of discrete diffusion models by asymptotic expansion,"JanEliÃ¡Å¡, Hao Yin, GianlucaCusatis",03-sep-22,Numerical Analysis (math.NA)," Diffusion behaviors of heterogeneous materials are of paramount importance in many engineering problems. Numerical models that take into account the internal structure of such materials are robust but computationally very expensive. This burden can be partially decreased by using discrete models, however even then the practical application is limited to relatively small material volumes.   This paper formulates a homogenization scheme for discrete diffusion models. Asymptotic expansion homogenization is applied to distinguish between (i) the continuous macroscale description approximated by the standard finite element method and (ii) the fully resolved discrete mesoscale description in a local representative volume element (RVE) of material. Both transient and steady-state variants with nonlinear constitutive relations are discussed. In all the cases, the resulting discrete RVE problem becomes a simple linear steady-state problem that can be easily pre-computed. The scale separation provides a significant reduction of computational time allowing the solution of practical problems with a negligible error introduced mainly by the finite element discretization at the macroscale.",https://arxiv.org/abs/2209.01421
Homogenization of discrete mesoscale model of concrete for coupledmass transport and mechanics by asymptotic expansion,"JanEliÃ¡Å¡, GianlucaCusatis",03-sep-22,"Computational Engineering, Finance, and Science (cs.CE)"," Mass transport phenomenon in concrete structures is strongly coupled with their mechanical behavior. The first coupling fabric is the Biot's theory according to which fluid pressure interacts with solid stress state and volumetric deformation rate of the solid induces changes in fluid pressure. Another coupling mechanism emerges with cracks which serve as channels for the fluid to flow through them and provide volume for fluid storage. Especially the second coupling mechanism presents a challenge for numerical modeling as it requires detailed knowledge about cracking process. Discrete mesoscale mechanical models coupled with mass transport offer simple and robust way to solve the problem. On the other hand, however, they are computationally demanding. In order to reduce this computational burden, the present paper applies the asymptotic expansion homogenization technique to the coupled problem to deliver (i) continuous and homogeneous description of the macroscopic problem which can be easily solved by the finite element method, (ii) discrete and heterogeneous mesoscale problem in the periodic setup attached to each integration point of the macroscale along with (iii) equations providing communication between these two scales. The transient terms appear at the macroscale only, as well as the Biot's coupling terms. The coupling through cracking is treated at the mesoscale by changing conductivity of the conduit elements according to the mechanical solution, otherwise the two mesoscale steady state problems are decoupled and can be therefore solved in a sequence. This paper presents verification studies showing performance of the homogenized solution.",https://arxiv.org/abs/2209.01420
Learning to Predict Requires Integrated Information,"CarlottaaLanger, Nihat Ay",03-sep-22,Information Theory (cs.IT)," Embodied agents are regularly faced with the challenge to learn new tasks. In order to do that they need to be able to predict their next sensory state by forming an internal world model. We theorize that agents require a high value of information integration to update that world model in light of new information. This can be seen in the context of Integrated Information Theory, which provides a quantitative approach to consciousness and can be applied to neural networks. We use the sensorimotor loop to model the interactions among the agent's brain, body and environment. Thereby we can calculate various information theoretic measures that quantify different information flows in the system, one of which corresponds to Integrated Information. Additionally we are able to measure the interaction among the body and the environment, which leads to the concept of Morphological Computation. Previous research reveals an antagonistic relationship between Integrated Information and Morphological Computation. A morphology adapted well to a task can reduce the necessity for Integrated Information significantly. This creates the problem that embodied intelligence is correlated with reduced conscious experience. Here we propose a solution to this problem, namely that the agents need Integrated Information to learn. We support our hypothesis with results from a simple experimental setup in which the agents learn by using the em-algorithm.",https://arxiv.org/abs/2209.01419
Suppressing Noise from Built Environment Datasets to ReduceCommunication Rounds for Convergence of Federated Learning,"RahulMishra, Hari PrabhatGupta, TanimaDutta, SajalK. Das",03-sep-22,Machine Learning (cs.LG)," Smart sensing provides an easier and convenient data-driven mechanism for monitoring and control in the built environment. Data generated in the built environment are privacy sensitive and limited. Federated learning is an emerging paradigm that provides privacy-preserving collaboration among multiple participants for model training without sharing private and limited data. The noisy labels in the datasets of the participants degrade the performance and increase the number of communication rounds for convergence of federated learning. Such large communication rounds require more time and energy to train the model. In this paper, we propose a federated learning approach to suppress the unequal distribution of the noisy labels in the dataset of each participant. The approach first estimates the noise ratio of the dataset for each participant and normalizes the noise ratio using the server dataset. The proposed approach can handle bias in the server dataset and minimizes its impact on the participants' dataset. Next, we calculate the optimal weighted contributions of the participants using the normalized noise ratio and influence of each participant. We further derive the expression to estimate the number of communication rounds required for the convergence of the proposed approach. Finally, experimental results demonstrate the effectiveness of the proposed approach over existing techniques in terms of the communication rounds and achieved performance in the built environment.",https://arxiv.org/abs/2209.01418
MMKGR: Multi-hop Multi-modal Knowledge Graph Reasoning,"ShangfeiZheng, Weiqing Wang, Jianfeng Qu, Hongzhi Yin, Wei Chen, Lei Zhao",03-sep-22,Artificial Intelligence (cs.AI)," Multi-modal knowledge graphs (MKGs) include not only the relation triplets, but also related multi-modal auxiliary data (i.e., texts and images), which enhance the diversity of knowledge. However, the natural incompleteness has significantly hindered the applications of MKGs. To tackle the problem, existing studies employ the embedding-based reasoning models to infer the missing knowledge after fusing the multi-modal features. However, the reasoning performance of these methods is limited due to the following problems: (1) ineffective fusion of multi-modal auxiliary features; (2) lack of complex reasoning ability as well as inability to conduct the multi-hop reasoning which is able to infer more missing knowledge. To overcome these problems, we propose a novel model entitled MMKGR (Multi-hop Multi-modal Knowledge Graph Reasoning). Specifically, the model contains the following two components: (1) a unified gate-attention network which is designed to generate effective multi-modal complementary features through sufficient attention interaction and noise reduction; (2) a complementary feature-aware reinforcement learning method which is proposed to predict missing elements by performing the multi-hop reasoning process, based on the features obtained in component (1). The experimental results demonstrate that MMKGR outperforms the state-of-the-art approaches in the MKG reasoning task.",https://arxiv.org/abs/2209.01417
Negative Selection Approach to support Formal Verification andValidation of BlackBox Models' Input Constraints,"Abdul-RaufNuhu, KishorDattaGupta, Wendwosen BelleteBedada, MahmoudNabil, LydiaAsratZeleke, AbdollahHomaifar, EdwardTunstel",03-sep-22,Machine Learning (cs.LG), Generating unsafe sub-requirements from a partitioned input space to support verification-guided test cases for formal verification of black- box models is a challenging problem for researchers. The size of the search space makes exhaustive search computationally impractical. This paper investigates a meta-heuristic approach to search for unsafe candidate sub- requirements in partitioned input space. We present a Negative Selection Algorithm (NSA) for identifying the candidates' unsafe regions within given safety properties. The Meta-heuristic capability of the NSA algorithm made it possible to estimate vast unsafe regions while validating a subset of these regions. We utilize a parallel execution of partitioned input space to produce safe areas. The NSA based on the prior knowledge of the safe regions is used to identify candidate unsafe region areas and the Marabou framework is then used to validate the NSA results. Our preliminary experimentation and evaluation show that the procedure finds candidate unsafe sub- requirements when validated with the Marabou framework with high precision.,https://arxiv.org/abs/2209.01416
Closed-Loop View of the Regulation of AI: Equal Impact across RepeatedInteractions,"QuanZhou, RamenGhosh, RobertShorten, JakubMarecek",03-sep-22,Artificial Intelligence (cs.AI)," There has been much recent interest in the regulation of AI. We argue for a view based on civil-rights legislation, built on the notions of equal treatment and equal impact. In a closed-loop view of the AI system and its users, the equal treatment concerns one pass through the loop. Equal impact, in our view, concerns the long-run average behaviour across repeated interactions. In order to establish the existence of the average and its properties, one needs to study the ergodic properties of the closed-loop and its unique stationary measure.",https://arxiv.org/abs/2209.01411
Towards Accurate Binary Neural Networks via Modeling ContextualDependencies,"XingrunXing, Yangguang Li, Wei Li, WenruiDing, YalongJiang, Yufeng Wang, Jing Shao, Chunlei Liu, Xianglong Liu",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Existing Binary Neural Networks (BNNs) mainly operate on local convolutions with binarization function. However, such simple bit operations lack the ability of modeling contextual dependencies, which is critical for learning discriminative deep representations in vision models. In this work, we tackle this issue by presenting new designs of binary neural modules, which enables BNNs to learn effective contextual dependencies. First, we propose a binary multi-layer perceptron (MLP) block as an alternative to binary convolution blocks to directly model contextual dependencies. Both short-range and long-range feature dependencies are modeled by binary MLPs, where the former provides local inductive bias and the latter breaks limited receptive field in binary convolutions. Second, to improve the robustness of binary models with contextual dependencies, we compute the contextual dynamic embeddings to determine the binarization thresholds in general binary convolutional blocks. Armed with our binary MLP blocks and improved binary convolution, we build the BNNs with explicit Contextual Dependency modeling, termed as BCDNet. On the standard ImageNet-1K classification benchmark, the BCDNet achieves 72.3% Top-1 accuracy and outperforms leading binary methods by a large margin. In particular, the proposed BCDNet exceeds the state-of-the-art ReActNet-A by 2.9% Top-1 accuracy with similar operations. Our code is available at [this https URL](https://github.com/Sense-GVT/BCDN)",https://arxiv.org/abs/2209.01410
Explainability via Short Formulas: the Case of Propositional Logicwith Implementation,"ReijoJaakkola, TomiJanhunen, AnttiKuusisto, Masood FeyzbakhshRankooh, MiikkaVilander",03-sep-22,Artificial Intelligence (cs.AI)," We conceptualize explainability in terms of logic and formula size, giving a number of related definitions of explainability in a very general setting. Our main interest is the so-called special explanation problem which aims to explain the truth value of an input formula in an input model. The explanation is a formula of minimal size that (1) agrees with the input formula on the input model and (2) transmits the involved truth value to the input formula globally, i.e., on every model. As an important example case, we study propositional logic in this setting and show that the special explainability problem is complete for the second level of the polynomial hierarchy. We also provide an implementation of this problem in answer set programming and investigate its capacity in relation to explaining answers to the n-queens and dominating set problems.",https://arxiv.org/abs/2209.01404
Vision Transformers and YoloV5 based Driver Drowsiness DetectionFramework,"Ghanta SaiKrishna, KundrapuSupriya, JaiVardhan, Mallikharjuna RaoK",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Human drivers have distinct driving techniques, knowledge, and sentiments due to unique driving traits. Driver drowsiness has been a serious issue endangering road safety; therefore, it is essential to design an effective drowsiness detection algorithm to bypass road accidents. Miscellaneous research efforts have been approached the problem of detecting anomalous human driver behaviour to examine the frontal face of the driver and automobile dynamics via computer vision techniques. Still, the conventional methods cannot capture complicated driver behaviour features. However, with the origin of deep learning architectures, a substantial amount of research has also been executed to analyze and recognize driver's drowsiness using neural network algorithms. This paper introduces a novel framework based on vision transformers and YoloV5 architectures for driver drowsiness recognition. A custom YoloV5 pre-trained architecture is proposed for face extraction with the aim of extracting Region of Interest (ROI). Owing to the limitations of previous architectures, this paper introduces vision transformers for binary image classification which is trained and validated on a public dataset UTA-RLDD. The model had achieved 96.2\% and 97.4\% as it's training and validation accuracies respectively. For the further evaluation, proposed framework is tested on a custom dataset of 39 participants in various light circumstances and achieved 95.5\% accuracy. The conducted experimentations revealed the significant potential of our framework for practical applications in smart transportation systems.",https://arxiv.org/abs/2209.01403
Optimizing Partial Area Under the Top-k Curve: Theory and Practice,"ZitaiWang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, QingmingHuang",03-sep-22,Machine Learning (cs.LG)," Top-k error has become a popular metric for large-scale classification benchmarks due to the inevitable semantic ambiguity among classes. Existing literature on top-k optimization generally focuses on the optimization method of the top-k objective, while ignoring the limitations of the metric itself. In this paper, we point out that the top-k objective lacks enough discrimination such that the induced predictions may give a totally irrelevant label a top rank. To fix this issue, we develop a novel metric named partial Area Under the top-k Curve (AUTKC). Theoretical analysis shows that AUTKC has a better discrimination ability, and its Bayes optimal score function could give a correct top-K ranking with respect to the conditional probability. This shows that AUTKC does not allow irrelevant labels to appear in the top list. Furthermore, we present an empirical surrogate risk minimization framework to optimize the proposed metric. Theoretically, we present (1) a sufficient condition for Fisher consistency of the Bayes optimal score function; (2) a generalization upper bound which is insensitive to the number of classes under a simple hyperparameter setting. Finally, the experimental results on four benchmark datasets validate the effectiveness of our proposed framework.",https://arxiv.org/abs/2209.01401
Disconnected Emerging Knowledge Graph Oriented Inductive LinkPrediction,YufengZhang (1)Weiqing Wang(2) HongzhiYin (3)Pengpeng Zhao(1) Wei Chen(1) Lei Zhao(1) ((1) Soochow University (2) Monash University ,03-sep-22,Machine Learning (cs.LG)," Inductive link prediction (ILP) is to predict links for unseen entities in emerging knowledge graphs (KGs), considering the evolving nature of KGs. A more challenging scenario is that emerging KGs consist of only unseen entities, called as disconnected emerging KGs (DEKGs). Existing studies for DEKGs only focus on predicting enclosing links, i.e., predicting links inside the emerging KG. The bridging links, which carry the evolutionary information from the original KG to DEKG, have not been investigated by previous work so far. To fill in the gap, we propose a novel model entitled DEKG-ILP (Disconnected Emerging Knowledge Graph Oriented Inductive Link Prediction) that consists of the following two components. (1) The module CLRM (Contrastive Learning-based Relation-specific Feature Modeling) is developed to extract global relation-based semantic features that are shared between original KGs and DEKGs with a novel sampling strategy. (2) The module GSM (GNN-based Subgraph Modeling) is proposed to extract the local subgraph topological information around each link in KGs. The extensive experiments conducted on several benchmark datasets demonstrate that DEKG-ILP has obvious performance improvements compared with state-of-the-art methods for both enclosing and bridging link prediction. The source code is available online.",https://arxiv.org/abs/2209.01398
Hypergraph convolutional neural network-based clustering technique,"Loc H.Tran, NguyenTrinh, LinhH. Tran",03-sep-22,Machine Learning (cs.LG)," This paper constitutes the novel hypergraph convolutional neural networkbased clustering technique. This technique is employed to solve the clustering problem for the Citeseer dataset and the Cora dataset. Each dataset contains the feature matrix and the incidence matrix of the hypergraph (i.e., constructed from the feature matrix). This novel clustering method utilizes both matrices. Initially, the hypergraph auto- encoders are employed to transform both the incidence matrix and the feature matrix from high dimensional space to low dimensional space. In the end, we apply the k-means clustering technique to the transformed matrix. The hypergraph convolutional neural network (CNN)-based clustering technique presented a better result on performance during experiments than those of the other classical clustering techniques.",https://arxiv.org/abs/2209.01397
How to Prompt? Opportunities and Challenges of Zero- and Few-ShotLearning for Human-AI Interaction in Creative Applications of GenerativeModels,"HaiDang, LukasMecke, FlorianLehmann, SvenGoller, DanielBuschek",03-sep-22,Human-Computer Interaction (cs.HC)," Deep generative models have the potential to fundamentally change the way we create high-fidelity digital content but are often hard to control. Prompting a generative model is a promising recent development that in principle enables end-users to creatively leverage zero-shot and few-shot learning to assign new tasks to an AI ad-hoc, simply by writing them down. However, for the majority of end-users writing effective prompts is currently largely a trial and error process. To address this, we discuss the key opportunities and challenges for interactive creative applications that use prompting as a new paradigm for Human-AI interaction. Based on our analysis, we propose four design goals for user interfaces that support prompting. We illustrate these with concrete UI design sketches, focusing on the use case of creative writing. The research community in HCI and AI can take these as starting points to develop adequate user interfaces for models capable of zero- and few-shot learning.",https://arxiv.org/abs/2209.01391
Differential Privacy on Dynamic Data,"YuanQiu, KeYi","3 Sep 2022 (v1(https://arxiv.org/abs/2209.01387v1)), lastrevised 8 Sep 2022 (this version, v2)",Cryptography and Security (cs.CR)," A fundamental problem in differential privacy is to release a privatized data structure over a dataset that can be used to answer a class of linear queries with small errors. This problem has been well studied in the static case. In this paper, we consider the dynamic setting where items may be inserted into or deleted from the dataset over time, and we need to continually release data structures so that queries can be answered at any time instance. We present black-box constructions of such dynamic differentially private mechanisms from static ones with only a polylogarithmic degradation in the utility. For the fully-dynamic case, this is the first such result. For the insertion-only case, similar constructions are known, but we improve them for sparse update streams.",https://arxiv.org/abs/2209.01390
SaleNet: A low-power end-to-end CNN accelerator for sustainedattention level evaluation using EEG,"ChaoZhang, Zijian Tang, Taoming Guo, Jiaxin Lei, Jiaxin Xiao, Anhe Wang, Shuo Bai, Milin Zhang",03-sep-22,Hardware Architecture (cs.AR)," This paper proposes SaleNet - an end-to-end convolutional neural network (CNN) for sustained attention level evaluation using prefrontal electroencephalogram (EEG). A bias-driven pruning method is proposed together with group convolution, global average pooling (GAP), near-zero pruning, weight clustering and quantization for the model compression, achieving a total compression ratio of 183.11x. The compressed SaleNet obtains a state-of-the-art subject-independent sustained attention level classification accuracy of 84.2% on the recorded 6-subject EEG database in this work. The SaleNet is implemented on a Artix-7 FPGA with a competitive power consumption of 0.11 W and an energy-efficiency of 8.19 GOps/W.",https://arxiv.org/abs/2209.01387
Training Strategies for Improved Lip-reading,"PingchuanMa, YujiangWang, StavrosPetridis, Jie Shen, Maja Pantic",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Several training strategies and temporal models have been recently proposed for isolated word lip-reading in a series of independent works. However, the potential of combining the best strategies and investigating the impact of each of them has not been explored. In this paper, we systematically investigate the performance of state-of-the-art data augmentation approaches, temporal models and other training strategies, like self-distillation and using word boundary indicators. Our results show that Time Masking (TM) is the most important augmentation followed by mixup and Densely-Connected Temporal Convolutional Networks (DC-TCN) are the best temporal model for lip-reading of isolated words. Using self-distillation and word boundary indicators is also beneficial but to a lesser extent. A combination of all the above methods results in a classification accuracy of 93.4%, which is an absolute improvement of 4.6% over the current state-of- the-art performance on the LRW dataset. The performance can be further improved to 94.1% by pre-training on additional datasets. An error analysis of the various training strategies reveals that the performance improves by increasing the classification accuracy of hard-to-recognise words.",https://arxiv.org/abs/2209.01386
Modeling Opinion Dynamics: Ranking Algorithms on HeterogeneousPopulations,Ivan V.Kozitsin,03-sep-22,Social and Information Networks (cs.SI)," The heterogeneity of the influence processes is an important feature of social systems: how we perceive social influence and how we influence other individuals is heavily influenced by our opinion and non- opinion attributes. The latter include demographic, cultural, and structural (how we are embedded in social networks) characteristics. Furthermore, the results of the influence processes may also depend on how similar the interacting individuals are in terms of their features. This paper addresses this issue and elaborates on an agent-based model that is sensitive to the individual characteristics, both opinion and non-opinion ones. The model is fortified with a ranking algorithm that mimics the ranking algorithms widely adopted in real-world online social networks. For the resulting model, I elaborate a mean-field approximation that describes the behavior of the model at the macroscopic level via an autonomous system of ordinary differential equations. The properties of this system are thoroughly studied.",https://arxiv.org/abs/2209.01383
Tree-Based Learning in RNNs for Power Consumption Forecasting,"RobertoBaviera, PietroManzoni",03-sep-22,Machine Learning (cs.LG)," A Recurrent Neural Network that operates on several time lags, called an RNN(p), is the natural generalization of an Autoregressive ARX(p) model. It is a powerful forecasting tool when different time scales can influence a given phenomenon, as it happens in the energy sector where hourly, daily, weekly and yearly interactions coexist. The cost-effective BPTT is the industry standard as learning algorithm for RNNs. We prove that, when training RNN(p) models, other learning algorithms turn out to be much more efficient in terms of both time and space complexity. We also introduce a new learning algorithm, the Tree Recombined Recurrent Learning, that leverages on a tree representation of the unrolled network and appears to be even more effective. We present an application of RNN(p) models for power consumption forecasting on the hourly scale: experimental results demonstrate the efficiency of the proposed algorithm and the excellent predictive accuracy achieved by the selected model both in point and in probabilistic forecasting of the energy consumption.",https://arxiv.org/abs/2209.01382
A CNC approach for Directional Total Variation,"GabrieleScrivanti, EmilieChouzenoux, Jean-ChristophePesquet",03-sep-22,Numerical Analysis (math.NA)," The core of many approaches for the resolution of variational inverse problems arising in signal and image processing consists of promoting the sought solution to have a sparse representation in a well- suited space. A crucial task in this context is the choice of a good sparsity prior that can ensure a good trade-off between the quality of the solution and the resulting computational cost. The recently introduced Convex-Non-Convex (CNC) strategy appears as a great compromise, as it combines the high qualitative performance of non-convex sparsity-promoting functions with the convenience of dealing with convex optimization problems. This work proposes a new variational formulation to implement CNC approach in the context of image denoising. By suitably exploiting duality properties, our formulation allows to encompass sophisticated directional total variation (DTV) priors. We additionally propose an efficient optimisation strategy for the resulting convex minimisation problem. We illustrate on numerical examples the good performance of the resulting CNC- DTV method, when compared to the standard convex total variation denoiser.",https://arxiv.org/abs/2209.01378
A Variational Approach for Joint Image Recovery and FeaturesExtraction Based on Spatially Varying Generalised Gaussian Models,"EmilieChouzenoux, Marie-CarolineCorbineau, Jean-ChristophePesquet, GabrieleScrivanti",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," The joint problem of reconstruction / feature extraction is a challenging task in image processing. It consists in performing, in a joint manner, the restoration of an image and the extraction of its features. In this work, we firstly propose a novel nonsmooth and nonconvex variational formulation of the problem. For this purpose, we introduce a versatile generalised Gaussian prior whose parameters, including its exponent, are space-variant. Secondly, we design an alternating proximal-based optimisation algorithm that efficiently exploits the structure of the proposed nonconvex objective function. We also analyze the convergence of this algorithm. As shown in numerical experiments conducted on joint segmentation/deblurring tasks, the proposed method provides high-quality results.",https://arxiv.org/abs/2209.01376
Identify The Beehive Sound Using Deep Learning,"Shah Jafor SadeekQuaderi, Sadia AfrinLabonno, SadiaMostafa, ShamimAkhter",03-sep-22,Sound (cs.SD)," Flowers play an essential role in removing the duller from the environment. The life cycle of the flowering plants involves pollination, fertilization, flowering, seed-formation, dispersion, and germination. Honeybees pollinate approximately 75% of all flowering plants. Environmental pollution, climate change, natural landscape demolition, and so on, threaten the natural habitats, thus continuously reducing the number of honeybees. As a result, several researchers are attempting to resolve this issue. Applying acoustic classification to recordings of beehive sounds may be a way of detecting changes within them. In this research, we use deep learning techniques, namely Sequential Neural Network, Convolutional Neural Network, and Recurrent Neural Network, on the recorded sounds to classify bee sounds from the nonbeehive noises. In addition, we perform a comparative study among some popular non-deep learning techniques, namely Support Vector Machine, Decision Tree, Random Forest, and NaÃ¯ve Bayes, with the deep learning techniques. The techniques are also verified on the combined recorded sounds (25-75% noises).",https://arxiv.org/abs/2209.01375
TogetherNet: Bridging Image Restoration and Object Detection Togethervia Dynamic Enhancement Learning,"YongzhenWang, XuefengYan, KaiwenZhang, LinaGong, HaoranXie, Fu LeeWang, Mingqiang Wei",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Adverse weather conditions such as haze, rain, and snow often impair the quality of captured images, causing detection networks trained on normal images to generalize poorly in these scenarios. In this paper, we raise an intriguing question - if the combination of image restoration and object detection, can boost the performance of cutting-edge detectors in adverse weather conditions. To answer it, we propose an effective yet unified detection paradigm that bridges these two subtasks together via dynamic enhancement learning to discern objects in adverse weather conditions, called TogetherNet. Different from existing efforts that intuitively apply image dehazing/deraining as a pre-processing step, TogetherNet considers a multi-task joint learning problem. Following the joint learning scheme, clean features produced by the restoration network can be shared to learn better object detection in the detection network, thus helping TogetherNet enhance the detection capacity in adverse weather conditions. Besides the joint learning architecture, we design a new Dynamic Transformer Feature Enhancement module to improve the feature extraction and representation capabilities of TogetherNet. Extensive experiments on both synthetic and real-world datasets demonstrate that our TogetherNet outperforms the state-of-the-art detection approaches by a large margin both quantitatively and qualitatively. Source code is available at [this https URL](https://github.com/yz-wang/TogetherNet).",https://arxiv.org/abs/2209.01374
CrossDial: An Entertaining Dialogue Dataset of Chinese Crosstalk,"BaizhouHuang, Shikang Du, Xiaojun Wan",03-sep-22,Computation and Language (cs.CL)," Crosstalk is a traditional Chinese theatrical performance art. It is commonly performed by two performers in the form of a dialogue. With the typical features of dialogues, crosstalks are also designed to be hilarious for the purpose of amusing the audience. In this study, we introduce CrossDial, the first open-source dataset containing most classic Chinese crosstalks crawled from the Web. Moreover, we define two new tasks, provide two benchmarks, and investigate the ability of current dialogue generation models in the field of crosstalk generation. The experiment results and case studies demonstrate that crosstalk generation is challenging for straightforward methods and remains an interesting topic for future works.",https://arxiv.org/abs/2209.01373
Ridgeline: A 2D Roofline Model for Distributed Systems,"FabioChecconi, Jesmin JahanTithi, FabrizioPetrini",03-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," In this short paper, we introduce the Ridgeline model, an extension of the Roofline model [4] for distributed systems. The Roofline model targets shared memory systems, bounding the performance of a kernel based on its operational intensity, and the peak compute throughput and memory bandwidth of the execution system. In a distributed setting, with multiple communicating compute entities, the network must be taken into account to model the system behavior accurately. The Ridgeline aggregates information on compute, memory, and network limits in one 2D plot to show, in an intuitive way, which of the resources is the expected bottleneck. We show the applicability of the Ridgeline in a case study based on a data- parallel Multi-Layer Perceptron (MLP) instance.",https://arxiv.org/abs/2209.01370
Deceiving Audio Design in Augmented Environments : A Systematic Reviewof Audio Effects in Augmented Reality,"EsmÃ©e Henrieke Anne deHaas, Lik-Hang Lee",03-sep-22,Human-Computer Interaction (cs.HC)," Recently, a lot of works show promising directions for audio design in augmented reality (AR). These works are mainly focused on how to improve user experience and make AR more realistic. But even though these improvements seem promising, these new possibilities could also be used as an input for manipulative design. This survey aims to analyze all recent discoveries in audio development regarding AR and argue what kind of ""manipulative"" effect this could have on the user. It can be concluded that even though there are many works explaining the effects of audio design in AR, very few works point out the risk of harm or manipulation toward the user. Future works could contain more awareness of this problem or maybe even",https://arxiv.org/abs/2209.01368
Sharp bounds on the price of bandit feedback for several models ofmistake-bounded online learning,"RaymondFeng, JesseGeneson, Andrew Lee, EspenSlettnes",03-sep-22,Machine Learning (cs.LG)," We determine sharp bounds on the price of bandit feedback for several variants of the mistake-bound model. The first part of the paper presents bounds on the $r$-input weak reinforcement model and the $r$-input delayed, ambiguous reinforcement model. In both models, the adversary gives $r$ inputs in each round and only indicates a correct answer if all $r$ guesses are correct. The only difference between the two models is that in the delayed, ambiguous model, the learner must answer each input before receiving the next input of the round, while the learner receives all $r$ inputs at once in the weak reinforcement model. In the second part of the paper, we introduce models for online learning with permutation patterns, in which a learner attempts to learn a permutation from a set of permutations by guessing statistics related to sub-permutations. For these permutation models, we prove sharp bounds on the price of bandit feedback.",https://arxiv.org/abs/2209.01367
Data Augmentation for Deep Receivers,"TomerRaviv, NirShlezinger",03-sep-22,Information Theory (cs.IT)," Deep neural networks (DNNs) allow digital receivers to learn to operate in complex environments. To do so, DNNs should preferably be trained using large labeled data sets with a similar statistical relationship as the one under which they are to infer. For DNN-aided receivers, obtaining labeled data conventionally involves pilot signalling at the cost of reduced spectral efficiency, typically resulting in access to limited data sets. In this paper, we study how one can enrich a small set of labeled pilots data into a larger data set for training deep receivers. Motivated by the widespread use of data augmentation techniques for enriching visual and text data, we propose dedicated augmentation schemes that exploits the characteristics of digital communication data. We identify the key considerations in data augmentations for deep receivers as the need for domain orientation, class (constellation) diversity, and low complexity. Following these guidelines, we devise three complementing augmentations that exploit the geometric properties of digital constellations. Our combined augmentation approach builds on the merits of these different augmentations to synthesize reliable data from a momentary channel distribution, to be used for training deep receivers. Furthermore, we exploit previous channel realizations to increase the reliability of the augmented samples.",https://arxiv.org/abs/2209.01366
Error-rate in Viterbi decoding of a duobinary signal in presence ofnoise and distortions: theory and simulation,"HenriMertens, Marc VanDroogenbroeck",03-sep-22,Information Theory (cs.IT)," The Viterbi algorithm, presented in 1967, allows a maximum likelihood decoding of partial response codes. This study focuses on the duobinary code which is the first member of this family and has been specified for the digital part of television systems recommended by International Organizations. Up to now the error-rate, which is the main criterion of the performance, has been evaluated by simulation. Although there exist theoretical bounds, these bounds are not satisfactory for a channel such as broadcasting (by terrestrial transmitters, cable networks or satellite) which is strongly impaired by noise, and linear and non-linear distortions. Analytical methods, verified by simulation, are presented here in order to evaluate the theoretical and exact values of the error-rate, in the form of series of numerical integrations, for a transmission in baseband or in radio-frequency with quadriphase modulation (or AM/VSB for cable networks) and coherent demodulation, in presence of noise and several distortions. This methodology can be later extended to other partial response codes, to convolutional codes and their concatenations.",https://arxiv.org/abs/2209.01362
DualCam: A Novel Benchmark Dataset for Fine-grained Real-time TrafficLight Detection,"HarinduJayarathne, TharinduSamarakoon, HasaraKoralege, AsithaDivisekara, RangaRodrigo, PeshalaJayasekara",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Traffic light detection is essential for self-driving cars to navigate safely in urban areas. Publicly available traffic light datasets are inadequate for the development of algorithms for detecting distant traffic lights that provide important navigation information. We introduce a novel benchmark traffic light dataset captured using a synchronized pair of narrow-angle and wide-angle cameras covering urban and semi-urban roads. We provide 1032 images for training and 813 synchronized image pairs for testing. Additionally, we provide synchronized video pairs for qualitative analysis. The dataset includes images of resolution 1920$\times$1080 covering 10 different classes. Furthermore, we propose a post-processing algorithm for combining outputs from the two cameras. Results show that our technique can strike a balance between speed and accuracy, compared to the conventional approach of using a single camera frame.",https://arxiv.org/abs/2209.01360
Semantic Segmentation in Learned Compressed Domain,"JinmingLiu, HemingSun, JiroKatto",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Most machine vision tasks (e.g., semantic segmentation) are based on images encoded and decoded by image compression algorithms (e.g., JPEG). However, these decoded images in the pixel domain introduce distortion, and they are optimized for human perception, making the performance of machine vision tasks suboptimal. In this paper, we propose a method based on the compressed domain to improve segmentation tasks. i) A dynamic and a static channel selection method are proposed to reduce the redundancy of compressed representations that are obtained by encoding. ii) Two different transform modules are explored and analyzed to help the compressed representation be transformed as the features in the segmentation network. The experimental results show that we can save up to 15.8\% bitrates compared with a state- of-the-art compressed domain-based work while saving up to about 83.6\% bitrates and 44.8\% inference time compared with the pixel domain-based method.",https://arxiv.org/abs/2209.01357
A repeated unknown game: Decentralized task offloading in vehicularfog computing,"ByungjinCho, YuXiao",03-sep-22,Multiagent Systems (cs.MA)," Offloading computation to nearby edge/fog computing nodes, including the ones carried by moving vehicles, e.g., vehicular fog nodes (VFN), has proved to be a promising approach for enabling low-latency and compute-intensive mobility applications, such as cooperative and autonomous driving. This work considers vehicular fog computing scenarios where the clients of computation offloading services try to minimize their own costs while deciding which VFNs to offload their tasks. We focus on decentralized multi-agent decision-making in a repeated unknown game where each agent, e.g., service client, can observe only its own action and realized cost. In other words, each agent is unaware of the game composition or even the existence of opponents. We apply a completely uncoupled learning rule to generalize the decentralized decision-making algorithm presented in \cite{Cho2021} for the multi-agent case. The multi-agent solution proposed in this work can capture the unknown offloading cost variations susceptive to resource congestion under an adversarial framework where each agent may take implicit cost estimation and suitable resource choice adapting to the dynamics associated with volatile supply and demand. According to the evaluation via simulation, this work reveals that such individual perturbations for robustness to uncertainty and adaptation to dynamicity ensure a certain level of optimality in terms of social welfare, e.g., converging the actual sequence of play with unknown and asymmetric attributes and lowering the correspondent cost in social welfare due to the self-interested behaviors of agents.",https://arxiv.org/abs/2209.01355
Improving Compositional Generalization in Math Word Problem Solving,"YunshiLan, LeiWang, JingJiang, Ee-Peng Lim",03-sep-22,Computation and Language (cs.CL)," Compositional generalization refers to a model's capability to generalize to newly composed input data based on the data components observed during training. It has triggered a series of compositional generalization analysis on different tasks as generalization is an important aspect of language and problem solving skills. However, the similar discussion on math word problems (MWPs) is limited. In this manuscript, we study compositional generalization in MWP solving. Specifically, we first introduce a data splitting method to create compositional splits from existing MWP datasets. Meanwhile, we synthesize data to isolate the effect of compositions. To improve the compositional generalization in MWP solving, we propose an iterative data augmentation method that includes diverse compositional variation into training data and could collaborate with MWP methods. During the evaluation, we examine a set of methods and find all of them encounter severe performance loss on the evaluated datasets. We also find our data augmentation method could significantly improve the compositional generalization of general MWP methods. Code is available at [this https URL](https://github.com/demoleiwang/CGMWP).",https://arxiv.org/abs/2209.01353
Semi-supervised Training for Knowledge Base Graph Self-attentionNetworks on Link Prediction,"ShuanglongYao, DechangPi, JunfuChen, YufeiLiu, ZhiyuanWu",03-sep-22,Artificial Intelligence (cs.AI)," The task of link prediction aims to solve the problem of incomplete knowledge caused by the difficulty of collecting facts from the real world. GCNs-based models are widely applied to solve link prediction problems due to their sophistication, but GCNs-based models are suffering from two problems in the structure and training process. 1) The transformation methods of GCN layers become increasingly complex in GCN- based knowledge representation models; 2) Due to the incompleteness of the knowledge graph collection process, there are many uncollected true facts in the labeled negative samples. Therefore, this paper investigates the characteristic of the information aggregation coefficient (self-attention) of adjacent nodes and redesigns the self-attention mechanism of the GAT structure. Meanwhile, inspired by human thinking habits, we designed a semi- supervised self-training method over pre-trained models. Experimental results on the benchmark datasets FB15k-237 and WN18RR show that our proposed self-attention mechanism and semi-supervised self-training method can effectively improve the performance of the link prediction task. If you look at FB15k-237, for example, the proposed method improves Hits@1 by about 30%.",https://arxiv.org/abs/2209.01352
Towards the Age of Intelligent Vehicular Networks for Connected andAutonomous Vehicles in 6G,"Van-LinhNguyen, Ren-HungHwang, Po-Ching Lin, AbhishekVyas, Van-TaoNguyen",03-sep-22,Networking and Internet Architecture (cs.NI)," Twenty-two years after the advent of the first-generation vehicular network, i.e., dedicated short-range communications (DSRC) standard/IEEE 802.11p, the vehicular technology market has become very competitive with a new player, Cellular Vehicle-to-Everything (C-V2X). Currently, C-V2X technology likely dominates the race because of the big advantages of comprehensive coverage and high throughput/reliability. Meanwhile, DSRC-based technologies are struggling to survive and rebound with many hopes betting on the success of the second-generation standard, IEEE P802.11bd. While the standards battle to attract automotive makers and dominate the commercial market landing, the research community has started thinking about the shape of the next-generation vehicular networks. This article details the state-of-the-art progress of vehicular networks, particularly the cellular V2X-related technologies in specific use cases, compared to the features of the current generation. Through the typical examples, we also highlight why 5G is inadequate to provide the best connectivity for vehicular applications, and then 6G technologies can fill up the vacancy.",https://arxiv.org/abs/2209.01350
How to cut a discrete cake fairly,AyumiIgarashi,03-sep-22,Computer Science and Game Theory (cs.GT)," Cake-cutting is a fundamental model of dividing a heterogeneous resource, such as land, broadcast time, and advertisement space. In this study, we consider the problem of dividing a discrete cake fairly in which the indivisible goods are aligned on a path and agents are interested in receiving a connected subset of items. We prove that a connected division of indivisible items satisfying a discrete counterpart of envy-freeness, called envy-freeness up to one good (EF1), always exists for any number of agents n with monotone valuations. Our result settles an open question raised by BilÃ² et al. (2019), who proved that an EF1 connected division always exists for the number of agents at most 4. Moreover, the proof can be extended to show the following secretive (1) and extra (2) versions: (1) for n agents with monotone valuations, the path can be divided into n connected bundles such that an EF1 assignment of the remaining bundles can be made to the other agents for any selection made by the secretive agent; (2) for n+1 agents with monotone valuations, the path can be divided into n connected bundles such that when any extra agent leaves, an EF1 assignment of the bundles can be made to the remaining agents.",https://arxiv.org/abs/2209.01349
Explanation Guided Contrastive Learning for Sequential Recommendation,"LeiWang, Ee-PengLim, ZhiweiLiu, TianxiangZhao",03-sep-22,Information Retrieval (cs.IR)," Recently, contrastive learning has been applied to the sequential recommendation task to address data sparsity caused by users with few item interactions and items with few user adoptions. Nevertheless, the existing contrastive learning-based methods fail to ensure that the positive (or negative) sequence obtained by some random augmentation (or sequence sampling) on a given anchor user sequence remains to be semantically similar (or different). When the positive and negative sequences turn out to be false positive and false negative respectively, it may lead to degraded recommendation performance. In this work, we address the above problem by proposing Explanation Guided Augmentations (EGA) and Explanation Guided Contrastive Learning for Sequential Recommendation (EC4SRec) model framework. The key idea behind EGA is to utilize explanation method(s) to determine items' importance in a user sequence and derive the positive and negative sequences accordingly. EC4SRec then combines both self-supervised and supervised contrastive learning over the positive and negative sequences generated by EGA operations to improve sequence representation learning for more accurate recommendation results. Extensive experiments on four real- world benchmark datasets demonstrate that EC4SRec outperforms the state-of- the-art sequential recommendation methods and two recent contrastive learning-based sequential recommendation methods, CL4SRec and DuoRec. Our experiments also show that EC4SRec can be easily adapted for different sequence encoder backbones (e.g., GRU4Rec and Caser), and improve their recommendation performance.",https://arxiv.org/abs/2209.01348
HammingMesh: A Network Topology for Large-Scale Deep Learning,"TorstenHoefler, TommasoBonato, Daniele DeSensi, Salvatore DiGirolamo, Shigang Li, MarcoHeddes, JonBelk, DeepakGoel, MiguelCastro, Steve Scott",03-sep-22,"Distributed, Parallel, and Cluster Computing (cs.DC)"," Numerous microarchitectural optimizations unlocked tremendous processing power for deep neural networks that in turn fueled the AI revolution. With the exhaustion of such optimizations, the growth of modern AI is now gated by the performance of training systems, especially their data movement. Instead of focusing on single accelerators, we investigate data-movement characteristics of large-scale training at full system scale. Based on our workload analysis, we design HammingMesh, a novel network topology that provides high bandwidth at low cost with high job scheduling flexibility. Specifically, HammingMesh can support full bandwidth and isolation to deep learning training jobs with two dimensions of parallelism. Furthermore, it also supports high global bandwidth for generic traffic. Thus, HammingMesh will power future large-scale deep learning systems with extreme bandwidth requirements.",https://arxiv.org/abs/2209.01347
Federated XGBoost on Sample-Wise Non-IID Data,"KatelinhJones, YuyaJeremy Ong, Yi Zhou, NathalieBaracaldo",03-sep-22,Machine Learning (cs.LG)," Federated Learning (FL) is a paradigm for jointly training machine learning algorithms in a decentralized manner which allows for parties to communicate with an aggregator to create and train a model, without exposing the underlying raw data distribution of the local parties involved in the training process. Most research in FL has been focused on Neural Network- based approaches, however Tree-Based methods, such as XGBoost, have been underexplored in Federated Learning due to the challenges in overcoming the iterative and additive characteristics of the algorithm. Decision tree-based models, in particular XGBoost, can handle non-IID data, which is significant for algorithms used in Federated Learning frameworks since the underlying characteristics of the data are decentralized and have risks of being non- IID by nature. In this paper, we focus on investigating the effects of how Federated XGBoost is impacted by non-IID distributions by performing experiments on various sample size-based data skew scenarios and how these models perform under various non-IID scenarios. We conduct a set of extensive experiments across multiple different datasets and different data skew partitions. Our experimental results demonstrate that despite the various partition ratios, the performance of the models stayed consistent and performed close to or equally well against models that were trained in a centralized manner.",https://arxiv.org/abs/2209.01346
DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network forText-to-Image Generation,"MengqiHuang, Zhendong Mao, Penghui Wang, Quan Wang, YongdongZhang",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Text-to-image generation aims at generating realistic images which are semantically consistent with the given text. Previous works mainly adopt the multi-stage architecture by stacking generator-discriminator pairs to engage multiple adversarial training, where the text semantics used to provide generation guidance remain static across all stages. This work argues that text features at each stage should be adaptively re-composed conditioned on the status of the historical stage (i.e., historical stage's text and image features) to provide diversified and accurate semantic guidance during the coarse-to-fine generation process. We thereby propose a novel Dynamical Semantic Evolution GAN (DSE-GAN) to re-compose each stage's text features under a novel single adversarial multi-stage architecture. Specifically, we design (1) Dynamic Semantic Evolution (DSE) module, which first aggregates historical image features to summarize the generative feedback, and then dynamically selects words required to be re-composed at each stage as well as re-composed them by dynamically enhancing or suppressing different granularity subspace's semantics. (2) Single Adversarial Multi-stage Architecture (SAMA), which extends the previous structure by eliminating complicated multiple adversarial training requirements and therefore allows more stages of text-image interactions, and finally facilitates the DSE module. We conduct comprehensive experiments and show that DSE-GAN achieves 7.48\% and 37.8\% relative FID improvement on two widely used benchmarks, i.e., CUB-200 and MSCOCO, respectively.",https://arxiv.org/abs/2209.01340
FedAR+: A Federated Learning Approach to Appliance Recognition withMislabeled Data in Residential Buildings,"AshishGupta, HariPrabhatGupta, Sajal K. Das",03-sep-22,Machine Learning (cs.LG)," With the enhancement of people's living standards and rapid growth of communication technologies, residential environments are becoming smart and well-connected, increasing overall energy consumption substantially. As household appliances are the primary energy consumers, their recognition becomes crucial to avoid unattended usage, thereby conserving energy and making smart environments more sustainable. An appliance recognition model is traditionally trained at a central server (service provider) by collecting electricity consumption data, recorded via smart plugs, from the clients (consumers), causing a privacy breach. Besides that, the data are susceptible to noisy labels that may appear when an appliance gets connected to a non-designated smart plug. While addressing these issues jointly, we propose a novel federated learning approach to appliance recognition, called FedAR+, enabling decentralized model training across clients in a privacy preserving way even with mislabeled training data. FedAR+ introduces an adaptive noise handling method, essentially a joint loss function incorporating weights and label distribution, to empower the appliance recognition model against noisy labels. By deploying smart plugs in an apartment complex, we collect a labeled dataset that, along with two existing datasets, are utilized to evaluate the performance of FedAR+. Experimental results show that our approach can effectively handle up to $30\%$ concentration of noisy labels while outperforming the prior solutions by a large margin on accuracy.",https://arxiv.org/abs/2209.01339
Graph Fourier transforms on directed product graphs,"ChengCheng, YangChen, Jeon YuLee, QiyuSun","3 Sep 2022 (v1(https://arxiv.org/abs/2209.01336v1)), lastrevised 7 Sep 2022 (this version, v2)",Information Theory (cs.IT)," Graph Fourier transform (GFT) is one of the fundamental tools in graph signal processing to decompose graph signals into different frequency components and to represent graph signals with strong correlation by different modes of variation effectively. The GFT on undirected graphs has been well studied and several approaches have been proposed to define GFTs on directed graphs. In this paper, based on the singular value decompositions of some graph Laplacians, we propose two GFTs on the Cartesian product graph of two directed graphs. We show that the proposed GFTs could represent spatial-temporal data sets on directed networks with strong correlation efficiently, and in the undirected graph setting they are essentially the joint GFT in the literature. In this paper, we also consider the bandlimiting procedure in the spectral domain of the proposed GFTs, and demonstrate its performance to denoise the temperature data set in the region of Brest (France) on January 2014.",https://arxiv.org/abs/2209.01338
Multilingual ColBERT-X,"DawnLawrie, Eugene Yang, Douglas W.Oard, JamesMayfield",03-sep-22,Information Retrieval (cs.IR)," ColBERT-X is a dense retrieval model for Cross Language Information Retrieval (CLIR). In CLIR, documents are written in one natural language, while the queries are expressed in another. A related task is multilingual IR (MLIR) where the system creates a single ranked list of documents written in many languages. Given that ColBERT-X relies on a pretrained multilingual neural language model to rank documents, a multilingual training procedure can enable a version of ColBERT-X well- suited for MLIR. This paper describes that training procedure. An important factor for good MLIR ranking is fine-tuning XLM-R using mixed-language batches, where the same query is matched with documents in different languages in the same batch. Neural machine translations of MS MARCO passages are used to fine-tune the model.",https://arxiv.org/abs/2209.01336
Noise-Robust Bidirectional Learning with Dynamic Sample Reweighting,"Chen-ChenZong, Zheng-Tao Cao, Hong-Tao Guo, YunDu, Ming-KunXie, Shao-YuanLi, Sheng-JunHuang",03-sep-22,Machine Learning (cs.LG)," Deep neural networks trained with standard cross-entropy loss are more prone to memorize noisy labels, which degrades their performance. Negative learning using complementary labels is more robust when noisy labels intervene but with an extremely slow model convergence speed. In this paper, we first introduce a bidirectional learning scheme, where positive learning ensures convergence speed while negative learning robustly copes with label noise. Further, a dynamic sample reweighting strategy is proposed to globally weaken the effect of noise-labeled samples by exploiting the excellent discriminatory ability of negative learning on the sample probability distribution. In addition, we combine self-distillation to further improve the model performance. The code is available at \url{[this https URL](https://github.com/chenchenzong/BLDR)}.",https://arxiv.org/abs/2209.01335
LDP-FPMiner: FP-Tree Based Frequent Itemset Mining with LocalDifferential Privacy,"ZhiliChen, JialiWang",03-sep-22,Databases (cs.DB)," Data aggregation in the setting of local differential privacy (LDP) guarantees strong privacy by providing plausible deniability of sensitive data. Existing works on this issue mostly focused on discovering heavy hitters, leaving the task of frequent itemset mining (FIM) as an open problem. To the best of our knowledge, the-state-of-the-art LDP solution to FIM is the SVSM protocol proposed recently. The SVSM protocol is mainly based on the padding and sampling based frequency oracle (PSFO) protocol, and regarded an itemset as an independent item without considering the frequency consistency among itemsets.   In this paper, we propose a novel LDP approach to FIM called LDP-FPMiner based on frequent pattern tree (FP-tree). Our proposal exploits frequency consistency among itemsets by constructing and optimizing a noisy FP-tree with LDP. Specifically, it works as follows. First, the most frequent items are identified, and the item domain is cut down accordingly. Second, the maximum level of the FP-tree is estimated. Third, a noisy FP-tree is constructed and optimized by using itemset frequency consistency, and then mined to obtain the k most frequent itemsets. Experimental results show that the LDP-FPMiner significantly improves over the state-of-the-art approach, SVSM, especially in the case of a high privacy level.",https://arxiv.org/abs/2209.01334
Class-Specific Channel Attention for Few-Shot Learning,"Ying-YuChen, Jun-WeiHsieh, Ming-Ching Chang",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Few-Shot Learning (FSL) has attracted growing attention in computer vision due to its capability in model training without the need for excessive data. FSL is challenging because the training and testing categories (the base vs. novel sets) can be largely diversified. Conventional transfer-based solutions that aim to transfer knowledge learned from large labeled training sets to target testing sets are limited, as critical adverse impacts of the shift in task distribution are not adequately addressed. In this paper, we extend the solution of transfer- based methods by incorporating the concept of metric-learning and channel attention. To better exploit the feature representations extracted by the feature backbone, we propose Class-Specific Channel Attention (CSCA) module, which learns to highlight the discriminative channels in each class by assigning each class one CSCA weight vector. Unlike general attention modules designed to learn global-class features, the CSCA module aims to learn local and class-specific features with very effective computation. We evaluated the performance of the CSCA module on standard benchmarks including miniImagenet, Tiered-ImageNet, CIFAR-FS, and CUB-200-2011. Experiments are performed in inductive and in/cross-domain settings. We achieve new state-of-the-art results.",https://arxiv.org/abs/2209.01333
Power Allocation for Space-Terrestrial Cooperation Systems withStatistical CSI,"Trinh VanChien, EvaLagunas, Tiep M.Hoang, SymeonChatzinotas, BjÃ¶rnOttersten, Lajos Hanzo",03-sep-22,Information Theory (cs.IT)," This paper studies an integrated network design that boosts system capacity through cooperation between wireless access points (APs) and a satellite. By coherently combing the signals received by the central processing unit from the users through the space and terrestrial links, we mathematically derive an achievable throughput expression for the uplink (UL) data transmission over spatially correlated Rician channels. A closed- form expression is obtained when maximum ratio combining is employed to detect the desired signals. We formulate the max-min fairness and total transmit power optimization problems relying on the channel statistics to perform power allocation. The solution of each optimization problem is derived in form of a low-complexity iterative design, in which each data power variable is updated based on a closed-form expression. The mathematical analysis is validated with numerical results showing the added benefits of considering a satellite link in terms of improving the ergodic data throughput.",https://arxiv.org/abs/2209.01332
Semi-Supervised Semantic Segmentation with Cross Teacher Training,"HuiXiao, LiDong, KangkangSong, HaoXu, ShuiboFu, DiqunYan, ChengbinPeng",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Convolutional neural networks can achieve remarkable performance in semantic segmentation tasks. However, such neural network approaches heavily rely on costly pixel-level annotation. Semi-supervised learning is a promising resolution to tackle this issue, but its performance still far falls behind the fully supervised counterpart. This work proposes a cross- teacher training framework with three modules that significantly improves traditional semi-supervised learning approaches. The core is a cross-teacher module, which could simultaneously reduce the coupling among peer networks and the error accumulation between teacher and student networks. In addition, we propose two complementary contrastive learning modules. The high-level module can transfer high-quality knowledge from labeled data to unlabeled ones and promote separation between classes in feature space. The low-level module can encourage low-quality features learning from the high- quality features among peer networks. In experiments, the cross-teacher module significantly improves the performance of traditional student-teacher approaches, and our framework outperforms stateof-the-art methods on benchmark datasets. Our source code of CTT will be released.",https://arxiv.org/abs/2209.01329
Continual Learning for Steganalysis,"ZihaoYin, RuohanMeng, ZhiliZhou",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," To detect the existing steganographic algorithms, recent steganalysis methods usually train a Convolutional Neural Network (CNN) model on the dataset consisting of corresponding paired cover/stego-images. However, it is inefficient and impractical for those steganalysis tools to completely retrain the CNN model to make it effective against both the existing steganographic algorithms and a new emerging steganographic algorithm. Thus, existing steganalysis models usually lack dynamic extensibility for new steganographic algorithms, which limits their application in real-world scenarios. To address this issue, we propose an accurate parameter importance estimation (APIE) based-continual learning scheme for steganalysis. In this scheme, when a steganalysis model is trained on the new image dataset generated by the new steganographic algorithm, its network parameters are effectively and efficiently updated with sufficient consideration of their importance evaluated in the previous training process. This approach can guide the steganalysis model to learn the patterns of the new steganographic algorithm without significantly degrading the detectability against the previous steganographic algorithms. Experimental results demonstrate the proposed scheme has promising extensibility for new emerging steganographic algorithms.",https://arxiv.org/abs/2209.01327
Classifying Spatial Trajectories,"Hasan Pourmahmood-Aghababa, Jeff M.Phillips",03-sep-22,Computational Geometry (cs.CG)," We provide the first comprehensive study on how to classify trajectories using only their spatial representations, measured on 5 real- world data sets. Our comparison considers 20 distinct classifiers arising either as a KNN classifier of a popular distance, or as a more general type of classifier using a vectorized representation of each trajectory. We additionally develop new methods for how to vectorize trajectories via a data-driven method to select the associated landmarks, and these methods prove among the most effective in our study. These vectorized approaches are simple and efficient to use, and also provide state-of-the-art accuracy on an established transportation mode classification task. In all, this study sets the standard for how to classify trajectories, including introducing new simple techniques to achieve these results, and sets a rigorous standard for the inevitable future study on this topic.",https://arxiv.org/abs/2209.01326
Deep Stable Representation Learning on Electronic Health Records,"YingtaoLuo, ZhaochengLiu, QiangLiu",03-sep-22,Machine Learning (cs.LG)," Deep learning models have achieved promising disease prediction performance of the Electronic Health Records (EHR) of patients. However, most models developed under the I.I.D. hypothesis fail to consider the agnostic distribution shifts, diminishing the generalization ability of deep learning models to Out-Of-Distribution (OOD) data. In this setting, spurious statistical correlations that may change in different environments will be exploited, which can cause sub-optimal performances of deep learning models. The unstable correlation between procedures and diagnoses existed in the training distribution can cause spurious correlation between historical EHR and future diagnosis. To address this problem, we propose to use a causal representation learning method called Causal Healthcare Embedding (CHE). CHE aims at eliminating the spurious statistical relationship by removing the dependencies between diagnoses and procedures. We introduce the Hilbert- Schmidt Independence Criterion (HSIC) to measure the degree of independence between the embedded diagnosis and procedure features. Based on causal view analyses, we perform the sample weighting technique to get rid of such spurious relationship for the stable learning of EHR across different environments. Moreover, our proposed CHE method can be used as a flexible plug-and-play module that can enhance existing deep learning models on EHR. Extensive experiments on two public datasets and five state-of-the-art baselines unequivocally show that CHE can improve the prediction accuracy of deep learning models on out-of-distribution data by a large margin. In addition, the interpretability study shows that CHE could successfully leverage causal structures to reflect a more reasonable contribution of historical records for predictions.",https://arxiv.org/abs/2209.01322
Synthesizing Photorealistic Virtual Humans Through Cross-modalDisentanglement,"SiddarthRavichandran, OndÅ™ejTexler, DimitarDinev, HyunJae Kang",03-sep-22,Computer Vision and Pattern Recognition (cs.CV)," Over the last few decades, many aspects of human life have been enhanced with virtual domains, from the advent of digital assistants such as Amazon's Alexa and Apple's Siri to the latest metaverse efforts of the rebranded Meta. These trends underscore the importance of generating photorealistic visual depictions of humans. This has led to the rapid growth of so-called deepfake and talking head generation methods in recent years. Despite their impressive results and popularity, they usually lack certain qualitative aspects such as texture quality, lips synchronization, or resolution, and practical aspects such as the ability to run in real-time. To allow for virtual human avatars to be used in practical scenarios, we propose an end-to-end framework for synthesizing high-quality virtual human faces capable of speech with a special emphasis on performance. We introduce a novel network utilizing visemes as an intermediate audio representation and a novel data augmentation strategy employing a hierarchical image synthesis approach that allows disentanglement of the different modalities used to control the global head motion. Our method runs in real-time, and is able to deliver superior results compared to the current state-of-the-art.",https://arxiv.org/abs/2209.01321
Kinova Gemini: Interactive Robot Grasping with Visual Reasoning andConversational AI,"HanxiaoChen, JiankunWang, MaxQ.-H. Meng",03-sep-22,Robotics (cs.RO)," To facilitate recent advances in robotics and AI for delicate collaboration between humans and machines, we propose the Kinova Gemini, an original robotic system that integrates conversational AI dialogue and visual reasoning to make the Kinova Gen3 lite robot help people retrieve objects or complete perception-based pick-and-place tasks. When a person walks up to Kinova Gen3 lite, our Kinova Gemini is able to fulfill the user's requests in three different applications: (1) It can start a natural dialogue with people to interact and assist humans to retrieve objects and hand them to the user one by one. (2) It detects diverse objects with YOLO v3 and recognize color attributes of the item to ask people if they want to grasp it via the dialogue or enable the user to choose which specific one is required. (3) It applies YOLO v3 to recognize multiple objects and let you choose two items for perception-based pick-and-place tasks such as ""Put the banana into the bowl"" with visual reasoning and conversational interaction.",https://arxiv.org/abs/2209.01320
Illegal But Not Malware: An Underground Economy App Detection SystemBased on Usage Scenario,"ZhuoChen, JieLiu, YuboHu, LeiWu, YajinZhou, XianhaoLiao, KeWang",03-sep-22,Cryptography and Security (cs.CR)," This paper focuses on mobile apps serving the underground economy by providing illegal services in the mobile system (e.g., gambling, porn, scam). These apps are named as underground economy apps, or UEware for short. As most UEware do not have malicious payloads, traditional malware detection approaches are ineffective to perform the detection. To address this problem, we propose a novel approach to effectively and efficiently detect UEware by considering the transition orders of the user interfaces (UIs), which determine the usage scenarios of these apps. Based on the proposed approach, we design a system named DeUEDroid to detect the UEware via scene graph. To evaluate DeUEDroid, we collect 26, 591 apps to evaluate DeUEDroid and build up the first large-scale ground-truth UEware dataset (1, 720 underground economy apps and 831 legitimate apps). The evaluation result shows that DeUEDroid can construct scene graph accurately, and achieve the accuracy scores of 77.70% on the five-classification task (i.e., gambling game, porn, financial scam, miscellaneous, and legitimate apps), reaching obvious improvements over the SOTA approaches. Running further on 24, 017 apps, DeUEDroid performs well in the real-world scenario to mitigate the threat. Specifically, by using DeUEDroid, we found that UEware are prevalent, i.e., 61% apps in the wild and 21% apps in the app stores are UEware (with over 72% accuracy after the manual investigation). We will release our dataset and system to engage the community after been accepted.",https://arxiv.org/abs/2209.01319
The Folded Pneumatic Artificial Muscle (foldPAM): TowardsProgrammability and Control via End Geometry,"SichengWang, EugenioFriasMiranda, Laura H.Blumenschein",03-sep-22,Robotics (cs.RO)," Soft pneumatic actuators have seen applications in many soft robotic systems, and their pressure-driven nature presents unique challenges and opportunities for controlling their motion. In this work, we present a new concept: designing and controlling pneumatic actuators via end geometry. We demonstrate a novel actuator class, named the folded Pneumatic Artificial Muscle (foldPAM), which features a thin-filmed air pouch that is symmetrically folded on each side. Varying the folded portion of the actuator changes the end constraints and, hence, the force-strain relationships. We investigated this change experimentally by measuring the force-strain relationship of individual foldPAM units with various lengths and amounts of folding. In addition to static-geometry units, an actuated foldPAM device was designed to produce continuous, on-demand adjustment of the end geometry, enabling closed-loop position control while maintaining constant pressure. Experiments with the device indicate that geometry control allows access to different areas on the force-strain plane and that closed-loop geometry control can achieve errors within 0.5% of the actuation range.",https://arxiv.org/abs/2209.01317
